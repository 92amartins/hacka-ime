Author,Text,Affiliation
Fábio Kon,Universidade de São Paulo - Departamento de Ciência da Computação,"The Future Internet will integrate large-scale systems constructed from the composition of thousands of distributed services, while interacting directly with the physical world via sensors and actuators, which compose the Internet of Things. This Future Internet will enable the realization of the Smart Cities vision, in which the urban infrastructure will be used to its fullest extent to offer a better quality of life for its citizens. Key to the efficient and effective realization of Smart Cities is the scientific and technological research covering the multiple layers that make up the Internet. This paper discusses the research challenges and initiatives related to Future Internet and Smart Cities in the scope of the InterSCity project. The challenges and initiatives are organized in three fronts: (1) Networking and High-Performance Distributed Computing; (2) Software Engineering for the Future Internet; and (3) Analysis and Mathematical Modeling for the Future Internet and Smart Cities. InterSCity aims at developing an integrated open-source platform containing all the major building blocks for the development of robust, integrated, sophisticated applications for the smart cities of the future. © 2016 IEEE. Big Data; Future Internet; Internet of Things; Machine Learning; Mathematical Modeling; Smart Cities Big data; Distributed computer systems; Internet of things; Large scale systems; Learning systems; Mathematical models; Open source software; Software engineering; Distributed service; Future internet; High-performance distributed computing; Open source platforms; Research challenges; Sensors and actuators; Technological researches; Urban infrastructure; Smart city;Smart City technologies emerge as a potential solution to tackle common problems in large urban centers by using city resources efficiently and providing quality services for citizens. Despite the various advances in middleware technologies to support future smart cities, there are no universally accepted platforms yet. Most of the existing solutions do not provide the required flexibility to be shared across cities. Moreover, the extensive use and development of non-open-source software leads to interoperability issues and limits the collaboration among R&D groups. In this paper, we explore the use of a microservices architecture to address key practical challenges in smart city platforms. We present InterSCity, a microservice-based open source smart city platform that aims at supporting collaborative, novel smart city research, development, and deployment initiatives. We discuss how the microservice approach enables a flexible, extensible, and loosely coupled architecture and present experimental results demonstrating the scalability of the proposed platform. © 2017 by SCITEPRESS Science and Technology Publications, Lda. All Rights Reserved. Microservices; Open source software; Scalability; Smart cities; Software platform Middleware; Open source software; Open systems; Scalability; Software engineering; Loosely coupled architectures; Microservices; Middleware technology; Open source platforms; Open sources; Quality services; Software platforms; Urban centers; Smart city;To benefit from the performance gains and cost savings enabled by elasticity in cloud IaaS environments, effective automated mechanisms for scaling are essential. This automation requires monitoring system status and defining criteria to trigger allocation and deallocation of resources. While these criteria are usually based upon resource utilization, this may be inadequate due to the impossibility of identifying the actual amount of required resources. Consequently, most systems update resource allocation in small increments, e.g., one VM at a time, which may negatively affect performance and cost. In this paper, we propose a novel approach in which the system monitors workload, instead of utilization, and, by means of a scalability model, it makes predictions of resource demand and updates the allocated resources accordingly. We provide an implementation of this approach and describe experimental results that show its effectiveness. © 2016 IEEE.  Resource allocation; Dynamic resource allocations; Monitoring system; Performance forecasting; Performance Gain; Resource demands; Resource utilizations; Scalability modeling; System monitors; Monitoring;Apache Hadoop has evolved significantly over the last years, with more than 60 releases bringing new features. By implementing the MapReduce programming paradigm and leveraging HDFS, its distributed file system, Hadoop has become a reliable and fault tolerant middleware for parallel and distributed computing over large datasets. Nevertheless, Hadoop may struggle under certain workloads, resulting in poor performance and high energy consumption. Users increasingly demand that high performance computing solutions address sustainability and limit energy consumption. In this paper, we introduce HDFSH, a hybrid storage mechanism for HDFS, which uses a combination of Hard Disks and Solid-State Disks to achieve higher performance while saving power in Hadoop computations. HDFSH brings to the middleware the best from HDs (affordable cost per GB and high storage capacity) and SSDs (high throughput and low energy consumption) in a configurable fashion, using dedicated storage zones for each storage device type. We implemented our mechanism as a block placement policy for HDFS, and assessed it over six recent releases of Hadoop with different architectural properties. Results indicate that our approach increases overall job performance while decreasing the energy consumption under most hybrid configurations evaluated. Our results also showed that, in many cases, storing only part of the data in SSDs results in significant energy savings and execution speedups. Energy consumption; Hadoop; HDFS; Hybrid storage Computer software; Digital storage; Energy conservation; Energy utilization; File organization; Hard disk storage; Middleware; Virtual storage; Architectural properties; Distributed file systems; Hadoop; HDFS; High energy consumption; High performance computing; Hybrid storages; Parallel and distributed computing; Distributed computer systems;In ever-changing business environments, organizations continuously refine their processes to benefit from and meet the constraints of new technology, new business rules, and new market requirements. Workflow management systems (WFMSs) support organizations in evolving their processes by providing them with technological mechanisms to design, enact, and monitor workflows. However, workflows repositories often grow and start to encompass a variety of interdependent workflows. Without appropriate tool support, keeping track of such interdependencies and staying aware of the impact of a change in a workflow schema becomes hard. Workflow designers are often blindsided by changes that end up inducing side- and ripple-effects. This poses threats to the reliability of the workflows and ultimately hampers the evolvability of the workflow repository as a whole. In this paper, the authors introduce a change impact analysis approach based on metrics and visualizations to support the evolution of workflow repositories. They implemented the approach and later integrated it as a module in the HP Operations Orchestration (HP OO) WFMS. The authors conducted an exploratory study in which they thoroughly analyzed the workflow repositories of 8 HP OO customers. They characterized the customer repositories from a change impact perspective and compared them against each other. The authors were able to spot the workflows with high change impact among thousands of workflows in each repository. They also found that while the out-of-the-box repository included in HP OO had 10 workflows with high change impact, customer repositories included 11 (+10%) to 35 (+250%) workflows with this same characteristic. This result indicates the extent to which customers should put additional effort in evolving their repositories. The authors' approach contributes to the body of knowledge on static workflow evolution and complements existing dynamic workflow evolution approaches. Their techniques also aim to help organizations build more flexible and reliable workflow repositories. © 2016, IGI Global. Change Impact Analysis; Dependency Management; Metrics; Visualizations; Workflow Evolution Societies and institutions; Visualization; Work simplification; Body of knowledge; Change impact analysis; Changing business environment; Exploratory studies; Market requirements; Metrics; Workflow Evolution; Workflow management systems; Sales;Data centers are notorious energy consumers. In fact, studies have shown that for every $1 spent on hardware in the datacenter, $0.50 is spent on powering this hardware over its lifetime. Data centers host real or virtual (i.e., cloud) clusters that often execute large compute jobs using MapReduce, of which Hadoop is a popular implementation. Like other successful open source projects, Hadoop has been maintained and evolved over time with new resource management features being added over time in an effort to improve performance, raising questions as to whether such architectural evolution has achieved its goal, and if so, at what cost. In this work we apply Green Mining to find out that later versions of Hadoop - who exhibit more dynamic resource control - can suffer from serious energy consumption performance regressions. © 2015 IEEE.  Energy utilization; Architectural evolution; Data centers; Dynamic resource control; Energy consumer; Green mining; Improve performance; Open source projects; Resource management; Hardware;The integration between Agile Methods and User-Centered Design (UCD) has been addressed by several authors in recent years. Nevertheless, a gap remains regarding a systematically consolidated description of agile usability practices for the final stages of UCD. Our aim is to describe agile usability practices based on the literature in the form of patterns, focusing on the UCD final stages, namely “Create Design Solutions” and “Evaluate Designs”. A literature review was conducted to identify patterns of use of agile usability practices. The major results of the study presented here are the selection and classification of the usability practices for the UCD final stages within the agile community and their structured presentation in the form of patterns (Name, Context, Problem, Solution, and Examples). Presenting agile usability practices as patterns can increase their applicability; it facilitates the visualization of the similarities between the communities of UCD and Agile Methods and also presents the ideas more clearly to other communities that can benefit from using these patterns in their specific development contexts. © Springer International Publishing Switzerland 2016. Agile UCD; Agile usability; Agile UX; Best practices; Patterns Design; Human computer interaction; Usability engineering; Agile UCD; Agile usability; Agile UX; Best practices; Patterns; User centered design;Recent research in Cloud Computing and Peer-to-Peer systems for Video-on-Demand (VoD) has focused on multimedia information retrieval, using cloud nodes as video streaming servers and peers as a way to distribute and share the video segments. A key challenge faced by these systems is providing an efficient way to retrieve the information segments descriptor, composed of its metadata and video segments, distributed among the cloud nodes and the Peer-to-Peer (P2P) network. In this paper, we propose a novel Cloud Computing and P2P hybrid architecture for multimedia information retrieval on VoD services that supports random seeking while providing scalability and efficiency. The architecture comprises Cloud and P2P layers. The Cloud layer is responsible for video segment metadata retrieval, using ontologies to improve the relevance of the retrieved information, and for distributing the metadata structures among cloud nodes. The P2P layer is responsible for finding peers that have the physical location of a segment. In this layer, we use trackers, which manage and collect the segments shared among other peers. We also use two Distributed Hash Tables, one to find these trackers and the other to store the information collected in case the tracker leaves the network and another peer needs to replace it. Unlike previous work, our architecture separates cloud nodes and peers responsibilities to manage the video metadata and its segments, respectively. Also, we show via simulations, the possibility of converting any peer to act as a tracker, while maintaining system scalability and performance, avoiding using centralized and powerful servers. © 2014, Springer-Verlag Wien. Cloud computing; DHT; Peer-to-peer; Video-on-demand Cloud computing; Image segmentation; Information retrieval; Metadata; Multimedia services; Network architecture; Scalability; Search engines; Video on demand; Video streaming; DHT; Distributed Hash Table; Multimedia information retrieval; On demands; Peer to peer; Peer to Peer (P2P) network; Video streaming servers; Video-on-Demand (VoD); Peer to peer networks;Software startup companies develop innovative, software-intensive products within limited time frames and with few resources, searching for sustainable and scalable business models. Software startups are quite distinct from traditional mature software companies, but also from micro-, small-, and medium-sized enterprises, introducing new challenges relevant for software engineering research. This paper's research agenda focuses on software engineering in startups, identifying, in particular, 70+ research questions in the areas of supporting startup engineering activities, startup evolution models and patterns, ecosystems and innovation hubs, human aspects in software startups, applying startup concepts in non-startup environments, and methodologies and theories for startup research. We connect and motivate this research agenda with past studies in software startup research, while pointing out possible future directions. While all authors of this research agenda have their main background in Software Engineering or Computer Science, their interest in software startups broadens the perspective to the challenges, but also to the opportunities that emerge from multi-disciplinary research. Our audience is therefore primarily software engineering researchers, even though we aim at stimulating collaborations and research that crosses disciplinary boundaries. We believe that with this research agenda we cover a wide spectrum of the software startup industry current needs. Research agenda; Software startup; Software-intensive systems Software engineering; Disciplinary boundaries; Engineering activities; Medium sized enterprise; Multi-disciplinary research; Research agenda; Research questions; Software intensive systems; Start-up companies; Engineering research;Software defined Networks (SDNs) have drawn much attention both from academia and industry over the last few years. Despite the fact that underlying ideas already exist through areas such as P2P applications and active networks (e.g. virtual topologies and dynamic changes of the network via software), only now has the technology evolved to a point where it is possible to scale the implementations, which justifies the high interest in SDNs nowadays. In this article, the JISA Editors invite five leading scientists from three continents (Raouf Boutaba, David Hutchison, Raj Jain, Ramachandran Ramjee, and Christian Esteve Rothenberg) to give their opinions about what is really new in SDNs. The interviews cover whether big telecom and data center companies need to consider using SDNs, if the new paradigm is changing the way computer networks are understood and taught, and what are the open issues on the topic. © 2015, Batista et al. Data center; SDN; Virtualization Application programs; Data centers; Dynamic changes; Networking community; P2P applications; SDN; Software-defined networks; Virtual topologies; Virtualizations; Peer to peer networks;Resulting from the technological revolution from the last decades, we observed many software startup ecosystems emerging around the globe. Having tech entrepreneurs as their main agents, some ecosystems exist for more than 50 years, while others are newly born. This difference in terms of evolution and maturity makes the task of comparing different tech hubs a challenge. Moreover, nascent ecosystems need a clear vision of how to develop their community to evolve towards a fruitful and sustainable ecosystem. This paper proposes a maturity model for software startup ecosystems based on a multiple case study of two existing ecosystems. By determining the maturity level for each ecosystem, it is possible not only to compare different realities, but mainly to identify gaps and propose customized practical actions that can lead to real improvements in the existing ecosystems, taking it to the next level of development, promoting innovation. © Springer International Publishing Switzerland 2015. Entrepreneurship; Maturity model; Startup ecosystems Ecology; Process engineering; Entrepreneurship; Main agent; Maturity levels; Maturity model; Multiple-case study; Sustainable ecosystems; Technological revolution; Ecosystems;Cloud computing facilitates dynamic resource provisioning. The automation of resource management, known as elasticity, has been subject to much research. In this context, monitoring of a running service plays a crucial role, and adjustments are made when certain thresholds are crossed. On such occasions, it is common practice to simply add or remove resources. In this paper we investigate how we can predict the performance of a service to dynamically adjust allocated resources based on predictions. In other words, instead of “repairing” because a threshold has been crossed, we attempt to stay ahead and allocate an optimized amount of resources in advance. To do so, we need to have accurate predictive models that are based on workloads. We present our approach, based on the Universal Scalability Law, and discuss initial experiments. © Springer International Publishing Switzerland 2015. Cloud computing; Elasticity; Performance prediction; Scalability modelling Cloud computing; Distributed computer systems; Elasticity; Forecasting; Natural resources management; Resource allocation; Scalability; Dynamic resource provisioning; Performance forecasting; Performance prediction; Predictive models; Resource management; Resources based; Scheduling;Choreographies are a distributed approach for composing web services. Compared to orchestrations, which use a centralized scheme for distributed service management, the interaction among the choreographed services is collaborative with decentralized coordination. Despite the advantages, choreography development, including the testing activities, has not yet evolved suf.ciently to support the complexity of the large distributed systems. This substantially impacts the robustness of the products and overall adoption of choreographies. The goal of the research described in this paper is to support the Test-Driven Development (TDD) of choreographies to facilitate the construction of reliable, decentralized distributed systems. To achieve that, we present Rehearsal, a framework supporting the automated testing of choreographies at development-time. In addition, we present a choreography development methodology that guides the developer on applying TDD using Rehearsal. To assess the framework and the methodology, we conducted an exploratory study with developers, whose result was that Rehearsal was considered very helpful for the application of TDD and that the methodology helped the development of robust choreographies. © 2014 Elsevier Inc. All rights reserved. Automated testing; Test-Driven development; Web service choreographies Social networking (online); Websites; Automated testing; Decentralized coordination; Decentralized distributed systems; Development methodology; Distributed approaches; Distributed service management; Test driven development; Web service choreography; Web services;Context: In recent years, the valuable knowledge that can be retrieved from petabyte scale datasets - known as Big Data - led to the development of solutions to process information based on parallel and distributed computing. Lately, Apache Hadoop has attracted strong attention due to its applicability to Big Data processing. Problem: The support of Hadoop by the research community has provided the development of new features to the framework. Recently, the number of publications in journals and conferences about Hadoop has increased consistently, which makes it difficult for researchers to comprehend the full body of research and areas that require further investigation. Solution: We conducted a systematic literature review to assess research contributions to Apache Hadoop. Our objective was to identify gaps, providing motivation for new research, and outline collaborations to Apache Hadoop and its ecosystem, classifying and quantifying the main topics addressed in the literature. Results: Our analysis led to some relevant conclusions: many interesting solutions developed in the studies were never incorporated into the framework; most publications lack sufficient formal documentation of the experiments conducted by authors, hindering their reproducibility; finally, the systematic review presented in this paper demonstrates that Hadoop has evolved into a solid platform to process large datasets, but we were able to spot promising areas and suggest topics for future research within the framework. © 2014 Elsevier Ltd. Apache Hadoop; HDFS; MapReduce; Survey; Systematic literature review Computer software; Surveying; Apache Hadoop; Formal documentation; HDFS; Map-reduce; Parallel and distributed computing; Process information; Research communities; Systematic literature review; Big data; Surveying;Web service composition is a commonly used solution to build distributed systems on the cloud. Choreographies are one specific kind of service composition in which the responsibilities for the execution of the system are shared by its service components without a central point of coordination. Due to the distributed nature of these systems, a manual approach to resource usage monitoring and allocation to maintain the expected Quality of Service (QoS) is not only inefficient but also does not scale. In this paper, we present an open source choreography enactment middleware that is capable of automatically deploying and executing a composition. Additionally, it also monitors the composition execution to perform automatic resource provisioning and dynamic service reconfiguration based on pre-defined Service Level Agreement (SLA) constraints. To achieve that, it keeps a meta-level representation of the compositions, which contains their specifications, deployment statuses, and QoS attributes. Application developers can write specific rules that take into account these meta-data to reason about the performance of the composition and change its behavior. Our middleware was evaluated on Amazon EC2 and our results demonstrate that, with little effort from the choreography developer or deployer, the middleware is able to maintain the established SLA using both horizontal and vertical scaling when faced with varying levels of load. Additionally, it also reduces operational costs by using as little computational resources as possible. Copyright 2014 ACM. Middleware; QoS; Reection; SOA Middleware; Quality of service; Social networking (online); Application developers; Computational resources; Reection; Service compositions; Service Level Agreements; SOA; Web service choreography; Web service composition; Web services;The integration between agile methods and UCD has been addressed by several authors in recent years. However, a gap remains regarding how the practices have been described, lacking a standard that both designers and agile practitioners can understand and apply. This study aims to propose agile usability patterns based on the literature, with a focus on the User-Centered Design early stages. The goal of the proposed patterns is to facilitate the use of the best agile usability practices by identifying more clearly in which context the pattern can be applied, and what is the problem that each pattern solves, presenting examples. © 2014 Springer International Publishing Switzerland. agile UCD; agile usability; agile UX; best practices; patterns Design; agile UCD; agile usability; agile UX; Best practices; patterns; Usability engineering;In recent years, service-oriented systems are becoming increasingly complex, with growing size and heterogeneity. Developing and deploying such large-scale systems present several challenges, such as reliability, reproducibility, handling failures on infrastructure, scaling deployment time as composition size grows, coordinating deployment among multiple organizations, dependency management, and supporting requirements of adaptable systems. However, many organizations still rely on manual deployment processes, which imposes difficulties in overcoming such challenges. In this paper, we propose a flexible and extensible middleware solution that addresses the challenges present in the large-scale deployment of service compositions. The CHOReOS Enactment Engine is a robust middleware infrastructure to automate the deployment of large-scale service compositions. We describe the middleware architecture and implementation and then present experimental results demonstrating the feasibility of our approach. © 2014 IEEE. automated deployment; cloud computing; service compositions Cloud computing; Complex networks; Engines; automated deployment; Deployment process; Large-scale deployment; Middleware architecture; Middleware infrastructure; Multiple organizations; Service compositions; Service Oriented Systems; Middleware;Agile software development methods have been increasingly adopted worldwide and became one of the mainstream software development approaches. Agile methods have also had an impact on software engineering education with universities adapting their courses to accommodate this new form of software development. Software engineering research has tried to evaluate the impact of agile methods in industrial projects and discover in which situations it is beneficial to apply such methods. However, there are almost no studies focusing on the progress of the agile movement in Brazil. In this paper, we present an overview of the evolution of the agile movement in Brazil, outlining the history of its first advocates in academia and industry. We describe existing educational initiatives, discuss the impact of the agile development on the national research, and present a report on the agile state-of-the-practice in the Brazilian IT industry. © 2013 The Brazilian Computer Society. Agile educational initiatives; Agile software development; Brazilian agile research; Brazilian agile state-of-the-practice; History of computing; Object-oriented programming Agile development; Agile educational initiatives; Agile movement; Agile software development; Brazilian agile state-of-the-practice; History of computing; Industrial projects; Software development approach; Education; Industry; Object oriented programming; Software design; Industrial research;Scalability has been studied in several areas of Computer Science and scalability testing and evaluation of contemporary software systems is an active topic. However, most of the times, these activities are still performed in a predominantly ad hoc fashion. There are a few tools to automate this process, but they present several restrictions about what systems can be tested and how to evaluate scalability. In this paper, we introduce a flexible and extensible framework for automated scalability testing of software offered as a service and propose to evaluate the scalability using hypothesis tests. Additionally, we argue that, instead of stating if a system is scalable or not, we should find out how it could scale better. © 2013 IEEE.  Extensible framework; Hypothesis tests; Software systems; Testing and evaluation; Automation; Scalability; Software as a service (SaaS); Software testing;A service choreography is a distributed service composition in which services interact without a centralized control. Adequate adaptation strategies are required to face complex and ever-changing business processes, given the collaborative nature of choreographies. Choreographies should also be able to adapt to changes in its non-functional requirements, such as response time, and especially for large-scale choreographies, adaptation strategies need to be automated and scale well. However, the body of knowledge regarding choreography adaptation approaches has not yet been consolidated and systematically evaluated. By means of a systematic literature review, in which we examined seven scientific paper sources, we identified and analyzed the state-of-the-art in choreography adaptation. We found 24 relevant primary studies and grouped them into six categories: model-based, measurement-based, multi-agent-based, formal method-based, semantic reasoning-based, and proxy layer-based. We analyzed (i) how each strategy deals with different types of requirements, (ii) what their required degree of human intervention is, (iii) how the different studies considered scalability, (iv) what implementations are currently available, and (v) which choreography languages are employed. From the selected studies, we extracted key examples of choreography adaptation usage and analyzed the terminology they adopted with respect to dynamic adaptation. We found out that more attention has been devoted to functional requirements and automated adaptation; only one work performs scalability evaluation; and most studies present some sort of implementation and use a specific choreography notation. © 2012 Springer-Verlag London. Choreographies adaptation; Choreographies customization; Service choreography; Service composition; Systematic review Choreographies adaptation; Choreographies customization; Service choreographies; Service compositions; Systematic Review; Telecommunication networks; Quality of service;As firms increasingly sanction an open sourcing strategy, the question of which open source project to undertake remains tentative. The lack of established metrics makes it difficult to formulate such strategy. While many projects have been formed and created, only a few managed to remain active. With the majority of these projects failing, firms need a reliable set of criteria to assess what makes a project appealing not only to developers but also to visitors, users and commercial sponsors. In this paper, we develop a theoretical model to explore the contextual and causal factors of project attractiveness in inducing activities such as source code contribution, software maintenance, and usage. We test our model with data derived from more than 4000 projects spanning 4 years. Our main findings include that projects' set of conditions such as license restrictiveness and their available resources provide the context that directly influence the amount of work activities observed in the projects. It was also found that indirect and unintended contributions such as recommending software, despite of being non-technical, cannot be ignored for project activeness, diffusion and sustainability. Finally, our analysis provide evidence that higher attractiveness leads to more code-related activities with the downside of slowing down responsiveness to address projects' tasks, such as the implementation of new features and bug fixes. Our model underscores the significance of the reinforcing effects of attractiveness and work activities in open source projects, giving us the opportunity to discuss strategies to manage common traps such as the liability of newness. We conclude by discussing the applicability of the research model to other user-led initiatives. © 2012 Elsevier B.V. All rights reserved. Attractiveness; Contributions; Contributors; Free software; Open source; Preferential attachment; Software development Attractiveness; Contributions; Contributors; Free software; Open sources; Preferential attachments; Industry; Open systems; Software engineering; Project management;Context: The management of software development productivity is a key issue in software organizations, where the major drivers are lower cost and shorter time-to-market. Agile methods, including Extreme Programming and Scrum, have evolved as ""light"" approaches that simplify the software development process, potentially leading to increased team productivity. However, little empirical research has examined which factors do have an impact on productivity and in what way, when using agile methods. Objective: Our objective is to provide a better understanding of the factors and mediators that impact agile team productivity. Method: We have conducted a multiple-case study for 6 months in three large Brazilian companies that have been using agile methods for over 2 years. We have focused on the main productivity factors perceived by team members through interviews, documentation from retrospectives, and non-participant observation. Results: We developed a novel conceptual framework, using thematic analysis to understand the possible mechanisms behind such productivity factors. Agile team management was found to be the most influential factor in achieving agile team productivity. At the intra-team level, the main productivity factors were team design (structure and work allocation) and member turnover. At the inter-team level, the main productivity factors were how well teams could be effectively coordinated by proper interfaces and other dependencies and avoiding delays in providing promised software to dependent teams. Conclusion: Teams should be aware of the influence and magnitude of turnover, which has been shown negative for agile team productivity. Team design choices remain an important factor impacting team productivity, even more pronounced on agile teams that rely on teamwork and people factors. The intra-team coordination processes must be adjusted to enable productive work by considering priorities and pace between teams. Finally, the revised conceptual framework for agile team productivity supports further tests through confirmatory studies. © 2012 Elsevier B.V. All rights reserved. Agile software development; Industrial case studies; Team management; Team productivity factors; Thematic analysis Agile software development; Industrial case study; Productivity factors; Team management; Thematic analysis; Empowerment of personnel; Productivity; Research; Software design; Factor analysis;[No abstract available]  ;A motivated individual is one of the cornerstones of agile software development. Although motivation has been recognized and studied in the software development field, little research has examined motivation in agile teams. Our study aims to provide a better understanding of what motivates software developers in agile environments. We conducted a systematic review of motivators in the agile context, classifying the results using the MOCC model of software engineers' motivation. Additionally, we performed three case studies in agile companies to both confirm our findings and gather new motivators. Our results suggest that motivation in the agile context is slightly different from the overall view of motivation in software development in general. © 2012 IEEE. Agile software development; human factors; Motivation; Multiple-case study; Systematic literature review Agile companies; Agile contexts; Agile environment; Agile software development; Agile teams; Multiple-case study; Software developer; Software engineers; Systematic literature review; Systematic Review; Human engineering; Software design; Motivation;[No abstract available]  ;The Future Internet environments raise challenging issues for the Service-Oriented Architectures. Due to the scalability and heterogeneity issues new approaches are thought in order to leverage the SOA to support a wider range of services and users. The CHOReOS project is part of the European Community Initiative to sketch technological solutions for the future ultra large systems. In particular, CHOReOS explores the choreography of services paradigm. Within this project, a conceptual architecture combining both the development and runtime environments is realized. This chapter introduces the CHOReOS Integrated Development and Runtime Environment, aka IDRE. © 2012 Springer-Verlag Berlin Heidelberg. Access; Choreography; Cloud & Grid; Discovery; Governance; IDRE; MDA; Middleware; Service; SOA; TDD; V&V Access; Choreography; Discovery; Governance; IDRE; MDA; Service; SOA; TDD; Information services; Middleware; Service oriented architecture (SOA); Software architecture; Internet;The mobile agent paradigm has emerged as a promising alternative to overcome the construction challenges of opportunistic grid environments. This model can be used to implement mechanisms that enable application execution progress even in the presence of failures such as the mechanisms provided by the MAG middleware (Mobile Agents for Grids). MAG includes retrying, replication, and checkpointing as fault tolerance techniques; they operate independently from each other and they are not capable of detecting changes on resource availability. In this paper, we describe a MAG extension that is capable of migrating agents when nodes fail, which optimizes application progress by keeping only the most advanced checkpoint, and also migrates slow replicas. The proposed approach was evaluated via simulations and experiments, which showed significant improvements. Copyright © 2011 John Wiley & Sons, Ltd. Copyright © 2011 John Wiley & Sons, Ltd. adaptive fault tolerance; mobile agent; opportunistic grid adaptive fault tolerance; Agent approach; Agent paradigm; Application execution; Check pointing; Fault tolerance mechanisms; Fault tolerance techniques; Grid environments; Implement mechanisms; Migrating agents; opportunistic grid; Resource availability; Fault tolerance; Middleware; Mobile agents;[No abstract available]  ;Open source communities such as the ones responsible for Linux and Apache became well known for producing, with volunteer labor innovating over the Internet, high-quality software that has been widely adopted by organizations. In the web server market, Apache has dominated in terms of market share for over 15 years, outperforming corporations and research institutions. The resource-based view (RBV) of firms posits that an organization outperforms its competitors because it has valuable, rare, imperfectly imitable, and non-substitutable resources. Accordingly, one concludes that Apache possesses such resources to sustain its competitive advantage. However, one does not know what those resources are. This chapter is an effort to locate them, answering the question: ""What resources enable Apache to outperform its for-profit competitors consistently?"" This research draws on the RBV to develop a series of propositions about Apache's internal resources and organizational capabilities. For each proposition developed, methods for their empirical validation are proposed, and future research directions are provided. © 2012, IGI Global.  ;In recent years, researchers have focused on merging knowledge bases in both pragmatic and theoretical points of view. In this paper, we enumerate a few attempts to deal with inconsistencies while merging knowledge bases. We focus on ontology merging and show that pragmatic and theoretical approaches are not integrated and that both could benefit from a closer relationship. We extended an existing theoretical algorithm for Description Logics and applied it for the ontology merging problem. We describe here an implementation of this algorithm as an open source Protégé plugin.  Description logic; Knowledge basis; Ontology merging; Open sources; Plug-ins; Theoretical algorithms; Theoretical approach; Theoretical points; Algorithms; Data description; Research; Semantic Web; Semantics; Merging;Free/Libre/Open Source Software (FLOSS) communities have produced a large amount of valuable software that is directly or indirectly used daily by any person with access to a computer. The field of Software Engineering studies processes, mechanisms, tools, and frameworks for the development of software artifacts. Historically, however, most of Software Engineering research and education does not benefit from the large and rich source of data and experimental testbeds offered by FLOSS projects and their hundreds of millions of lines of working code. In this paper, we discuss how Software Engineering research and education can greatly benefit from the wealth of information available in the FLOSS ecosystem. We then evaluate how FLOSS has been used, up to now, by papers published in the Brazilian Symposium on Software Engineering. Finally, we present an agenda for the future, proposing concrete ways to exploit the synergies between research and education in Software Engineering and FLOSS projects. © 2011 IEEE.  Experimental testbed; FLOSS projects; Free and open source softwares; Software artifacts; Wealth of information; Engineering education; Engineering research; Software design; Open systems;Agile software development methods have been increasingly adopted worldwide and became one of the mainstream software development approaches. Agile methods have also had an impact on software engineering education with universities adapting their courses to accommodate this new point of view of software development. Software engineering research has tried to evaluate the impact of agile methods in industrial projects and discover in which situations it is beneficial to apply such methods. However, there are still few studies focusing on the progress of the Agile Movement in Brazil.In this paper, we present an overview of the evolution of the Agile Movement in Brazil, outlining the history of its first advocates in academia and industry. We also describe existing educational initiatives and the impact of agile development on the national research and present a report on the agile state-of- the-practice in the Brazilian IT industry. © 2011 IEEE. agile methods; agile movement in Brazil; Agile software development; Brazilian agile research and state-of-the-practice.; educational initiatives Agile methods; Agile movement; Agile software development; Brazilian agile research and state-of-the-practice.; educational initiatives; Curricula; Industrial research; Industry; Software design;Environments with frequent changes in application requirements demand an evolutionary approach for database modeling. The challenge is greater when the database must support multiple applications simultaneously. An existing solution for database evolution is refactoring with a transition period. During this period, both the old and the new database schemas coexist and data is replicated in a synchronous process. This solution brings several difficulties, such as interference with the operation of applications. To minimize these difficulties, in this paper we present an asynchronous approach to keep these schemas updated. This paper presents the design for an experimental assessment of this novel approach for evolutionary database development. © 2011 Springer-Verlag. agile methods; asynchronous data replication; database evolution; performance evaluation Agile methods; Application requirements; Asynchronous data replication; Asynchronous replication; Database development; database evolution; Database modeling; Database schemas; Evolutionary approach; Experimental assessment; Multiple applications; Performance evaluation; Refactorings; Schemas; Synchronous process; Transition period; Internet; Database systems;In this paper we investigate agile team perceptions of factors impacting their productivity. Within this overall goal we also investigate which productivity concept was adopted by the agile teams studied. We here conducted two case studies in the industry and analyzed data from two projects that we followed for six months. From the perspective of agile team members the three most perceived factors impacting on their productivity were appropriate team composition and allocation external dependencies and staff turnover. Teams also mentioned pair programming and collocation as agile practices that impact productivity. As a secondary finding most team members did not share the same understanding of the concept of productivity. While some known factors still impact agile team productivity new factors emerged from the interviews as potential productivity factors impacting agile teams.© 2011 IEEE. Agile methods; Empirical analysis; Productivity factors; Team productivity Agile methods; Agile practices; Agile teams; Empirical analysis; Pair-programming; Potential productivity; Productivity factors; Team composition; Team members; Productivity;[No abstract available]  ;Cloud computing is currently one of the major topics in distributed systems, with large numbers of papers being written on the topic, with major players in the industry releasing a range of software platforms offering novel Internet-based services and, most importantly, evidence of real impact on end user communities in terms of approaches to provisioning software services. Cloud computing though is at a formative stage, with a lot of hype surrounding the area, and this makes it difficult to see the true contribution and impact of the topic. Cloud computing is a central topic for the Journal of Internet Services and Applications (JISA) and indeed the most downloaded paper from the first year of JISA is concerned with the state-of-the-art and research challenges related to cloud computing. The Editors-in-Chief, Fabio Kon and Gordon Blair, therefore felt it was timely to seek clarification on the key issues around cloud computing and hence invited five leading scientists from industrial organizations central to cloud computing to answer a series of questions on the topic. © 2011 The Brazilian Computer Society.  ;Web service choreographies have been proposed as a decentralized scalable way of composing services in a SOA environment. In spite of all the benefits of choreographies, the decentralized flow of information, the parallelism, and multiple party communication restrict the automated testing of choreographies at design and runtime. The goal of our research is to adapt the automated testing techniques used by the Agile Software Development community to the SOA context. To achieve that, we seek to develop software tools and a methodology to enable test-driven development of Choreographies. In this paper, we present our first step in that direction, a software prototype composed of ad hoc automated test case scripts for testing a web service choreography. © 2011 Authors. agile methods; choreography testing; web services Agile methods; Agile software development; Automated test; Automated testing; choreography testing; Runtimes; Software prototypes; Test driven development; Web service choreography; Automation; Health care; Software design; Software prototyping; Software testing; Testing; User interfaces; Web services;This paper reports early findings of a longitudinal study designed to evaluate the impact of changes in the intellectual property policy of 756 free and open source projects on their attractiveness over 44 months. Copyright 2011 ACM. Attractiveness; Free software; Governance; GPL; Intellectual property; Licenses; Open source software Attractiveness; Free software; Governance; GPL; Licenses; Open source software; Copyrights; Groupware; Interactive computer systems; International law; Open systems;[No abstract available]  ;The software industry is very dynamic and new ideas arise all the time from virtually any part of the world. It is not guaranteed that these ideas will be adopted, mainly because, among other obstacles, the solution may imply on having people change their way of thinking. Different from people, computers receive well defined commands and execute them precisely. We should take into account that human beings are independent and unpredictable. Despite this unpredictability, we can find some behavioral patterns to help us deal with several situations, allowing us to achieve our objectives. In this paper, after a small introduction to the Patterns for Introducing New Ideas proposed by Mary Lynn Manns and Linda Rising, we propose four new patterns that can be added to the original catalog. In one of these new patterns, we show the great importance of combining artistic activities with day-to-day activities of people who work with software development and how Arts can help us to introduce new ideas. The study of some practices such as theater, painting, poetry, music, and meditation allowed us to find some connective elements between the purely mathematical side of the human mind and its creative, artistic one. Software development should be approached as a ""human activity"", rather than a solely technical or logical one. © Copyright 2011 Carnegie Mellon University. Agile; Art; Behavioral patterns; Do art; Fearless change; Let them play; Linda Rising; Mary Lynn Manns; Patterns for introducing new ideas Computer applications; Computer programming; Agile; Art; Behavioral patterns; Do art; Fearless change; Let them play; Linda Rising; Mary Lynn Manns; Patterns for introducing new ideas; Software design;Agile methods have become more popular since the early 2000s and, in some cases, can offer better results for software development projects when compared to traditional approaches. Agile methods promise to achieve high productivity and to deliver high-quality software, attracting the attention of companies, which demand ever-higher development speed and quality in their products. © 2011 Springer-Verlag Berlin Heidelberg. agile methods; empirical evaluation; team productivity Productivity; Software design; Agile methods; Agile practices; empirical evaluation; Empirical evaluations; High productivity; High-quality software; Software development projects; team productivity; Computer software selection and evaluation;[No abstract available]  ;The goal of a Network of Competence Centers is to provide to FLOSS users, developers, and consumers, high-quality resources and expertise on the various topics related to FLOSS. This may be achieved via education, training, consulting, hosting, and certification not only in terms of tools and platforms but also methodologies, studies, and best practices. Based on the experience of QualiPSo Competence Centres, we observe how such a Network is working as a mechanism for sharing success stories, failures, questions, recommendations, best practices, and any kind of information that could help the establishment of a solid international collaborative environment for supporting quality in FLOSS. New Competence Centres are invited to the QualiPSo Network after their proposals are evaluated by the QualiPSo Competence Centres Board to ensure that the prospective Competence Centre is compliant with the QualiPSo Network Agreement, sharing a common vision and ethics. Each Competence Centre acts in its geographical region to increase the awareness of FLOSS and to better prepare the IT workforce for developing and using FLOSS based solutions. As of 2009, the process for Competence Centre creation is sustainable and reusable; guidelines for establishing proposals and opening new Competence Centres have been created, and promotion of Qualipso Competence Centres is done world wide from India to USA thanks to key initiatives such as the Open World Forum and the FLOSS Competence Centre Summit. This lecture will expose how these Competence Centres relate to each other, which governance model is used and, based on existing experiences, will describe how they currently operate in Europe and Brazil and what is planned in Italy, Belgium, Japan, and China for 2010. © 2010 IFIP International Federation for Information Processing.  Belgium; Collaborative environments; High quality; IT Workforce; Open world; Information technology; Open systems;Organisations and individuals release source code on the Web to improve their software by attracting peers in the strategic move of "" opensourcing"" that has created thousands of open source projects (e.g., Eclipse-IBM, Thunderbird-Mozilla and Linux-Torvalds). Nevertheless, most of these projects fail to attract people and never become active. To minimize this problem, we developed a theoretical model around a crucial construct (attractiveness) to open source projects, proposing its causes (project characteristics), indicators (e.g., number of members) and consequences (levels of activeness, efficiency, likelihood of task completion, time for task completion and software quality). We tested this model empirically using 3 samples of over 4600 projects each in a multi-sample SEM analysis. The results confirm the central role that attractiveness plays to guarantee an active and efficient community of software development, shedding new light on whether more developers increase software quality by finding and fixing more bugs and providing upgrades. They also clarify the actual causal structure involving Web page visits, downloads and members, which can be easily mistaken. Moreover, the results can provide useful insights to strategists as we discuss the impacts of license restrictiveness, software development status, type of project and intended audience on attractiveness and its consequences. Attractiveness; Free and open source software; Software engineering; Software quality Attractiveness; Free and open source softwares; Open source projects; Project characteristics; Projects fail; SEM analysis; Software Quality; Source codes; Theoretical models; Computer operating systems; Computer software selection and evaluation; Information systems; Software design; Software engineering; World Wide Web; Open systems;The Internet has been growing at a impressive rate in many aspects such as size, heterogeneity, and usage. This growth forces the continuous improvement of Internet infrastructure technologies. The Future Internet concept magnifies the required shift for Internet technologies, which shall allow supporting the continuously growing scale of the converging networking world together with new generations of services made available to and brought by the broad mass of end users. The CHOReOS project positions itself in this vision of the Future Internet, whilst focusing on the Future Internet of Services. This research project aims at assisting the engineering of software service compositions in this novel networking environment by devising a dynamic development process, and associated methods, tools and middleware, to sustain the composition of services in the form of large-scale choreographies for the Internet of the future. © 2010 ACM. Choreography; Internet of the future; Web services Choreography; Composition of services; Continuous improvements; Dynamic development; End users; Future internet; Internet infrastructure; Internet of the future; Internet of the futures; Internet technology; Networking environment; Software services; Internet; Middleware; Telecommunication networks; User interfaces; Web services;A significant number of Free Software projects has been widely used and considered successful. However, there is an even larger number of them that cannot overcome the initial steps towards building an active community of users and developers. In this study, we investigated whether there are relationships between source code metrics and attractiveness, i.e., the ability of a project to attract users and developers. To verify these relationships, we analyzed 6,773 Free Software projects from the SourceForge.net repository. The results indicated that attractiveness is indeed correlated to some source code metrics. This suggests that measurable attributes of the project source code somehow affect the decision to contribute to and adopt a Free Software. The findings described in this paper show that it is relevant for project leaders to monitor source code quality, particularly a few objective metrics, since these can have a positive influence in projects chances of forming a community of contributors and users around their software, enabling further enhancement in quality. © 2010 IEEE.  Free software; Objective metrics; Project leaders; Source codes; Software engineering;[No abstract available]  ;Opportunistic computational grids use idle processor cycles from shared machines to enable the execution of long-running parallel applications. Besides computational power, these applications may also consume and generate large amounts of data, requiring an efficient data storage and management infrastructure. In this article, we present an integrated middleware infrastructure that enables the use of not only idle processor cycles, but also unused disk space of shared machines. Our middleware enables the reliable distributed storage of application data in the shared machines in a redundant and fault-tolerant way. A checkpointing-based mechanism monitors the execution of parallel applications, saves periodical checkpoints in the shared machines, and in case of node failures, supports the application migration across heterogeneous grid nodes. We evaluate the feasibility of our middleware using experiments and simulations. Our evaluation shows that the proposed middleware promotes important improvements in grid data management reliability while imposing a low performance overhead. © 2010 The Brazilian Computer Society. Distributed data storage; Grid computing; Grid middleware; Opportunistic grid Application migrations; Distributed data storages; GRID middleware; Integrated middlewares; Large amounts of data; Management infrastructure; Opportunistic grid; Parallel application; Digital storage; Information management; Middleware; Grid computing;[No abstract available]  ;The InteGrade project is a multi-university effort to build a novel grid computing middleware based on the opportunistic use of resources belonging to user workstations. The InteGrade middleware currently enables the execution of sequential, bag-of-tasks, and parallel applications that follow the BSP or the MPI programming models. This article presents the lessons learned over the last five years of the InteGrade development and describes the solutions achieved concerning the support for robust application execution. The contributions cover the related fields of application scheduling, execution management, and fault tolerance. We present our solutions, describing their implementation principles and evaluation through the analysis of several experimental results. © 2010 Elsevier Inc. All rights reserved. Fault tolerance; Grid computing; Opportunistic grid; Resource management Application execution; Application scheduling; Execution management; Grid computing middleware; GRID middleware; Grid resource management; Lessons learned; Opportunistic grid; Parallel application; Programming models; Robust application; Fault tolerance; Middleware; Natural resources management; Parallel programming; Quality assurance; Resource allocation; Scheduling; Grid computing;The mobile agent paradigm has emerged as a promising alternative to overcome the construction challenges of opportunistic grid environments. This model can be used to implement mechanisms that enable application execution progress even in the presence of failures, such as those presented by the MAG middleware (Mobile Agents for Grids). MAG includes retrying, replication, and checkpointing as fault-tolerance techniques; they operate independently from each other and are not capable of detecting changes on resource availability. In this paper, we describe a MAG extension that is capable of migrating agents when nodes fail, that optimizes application progress by keeping only the most advanced checkpoint, and that migrates slow replicas.  Adaptive middleware; Application execution; Check pointing; Grid environments; Implement mechanisms; Migrating agents; Resource availability; Clouds; Fault tolerance; Gas welding; Middleware; Quality assurance; Mobile agents;Opportunistic grids are a class of computational grids that can leverage the idle processing and storage capacity of shared workstations in laboratories, companies, and universities to perform useful computation. OppStore is a middleware that allows using the free disk space of machines from an opportunistic grid for the distributed storage of application data. But when machines depart from the grid, it is necessary to reconstruct the fragments that were stored in that machines. Depending on the amount of stored data and the rate of machine departures, the generated traffic may make the distributed storage of data infeasible. In this work we present and evaluate a fragment recovery mechanism that makes viable to achieve redundancy and large data scale in a dynamic environment. Copyright 2009 ACM. Data redundancy; Distributed storage; Grid computing Application data; Computational grids; Data redundancy; Distributed data; Distributed storage; Dynamic environments; Free disk; Large data; Recovery mechanisms; Storage capacity; Computer science; Middleware; Quality assurance; Redundancy; Grid computing;The Batching design pattern consists of a common piece of design and implementation that is shared by a wide variety of well-known techniques in Computing such as gather/scatter for input/output, code downloading for system extension, message batching, mobile agents, and deferred calls for disconnected operation. All techniques mentioned above are designed for applications running across multiple domains (e.g., multiple processes or multiple nodes in a network). In these techniques, multiple operations are bundled together and then sent to a different domain, where they are executed. In some cases, the objective is to reduce the number of domain-crossings. In other cases, it is to enable dynamic server extension. In this article, we present the Batching pattern, discuss the circumstances in which the pattern should and should not be used, and identify eight classes of existing techniques that instantiate it. © 2009 Springer-Verlag Berlin Heidelberg.  Client/server; Design Patterns; Different domains; Disconnected operations; Input/output; Multiple domains; Multiple nodes; Multiple operations; Multiple process; System extension; Wireless networks; Mobile agents;Scheduling parallel and distributed applications efficiently onto grid environments is a difficult task and a great variety of scheduling heuristics has been developed aiming to address this issue. A successful grid resource allocation depends, among other things, on the quality of the available information about software artifacts and grid resources. In this article, we propose a semantic approach to integrate selection of equivalent resources and selection of equivalent software artifacts to improve the scheduling of resources suitable for a given set of application execution requirements. We also describe a prototype implementation of our approach based on the Integrade grid middleware and experimental results that illustrate its benefits. Copyright ©2009 John Wiley & Sons, Ltd. Grid computing; Ontologies; OWL; Semantic grids Application execution; Grid environments; GRID middleware; Grid resource; Grid resource allocation; OWL; Parallel and distributed applications; Prototype implementations; Scheduling heuristics; Semantic approach; Semantic grids; Software artifacts; Computer system firewalls; Middleware; Ontology; Planning; Resource allocation; Scheduling; Semantics; Software prototyping; Grid computing;In this paper, we propose a decentralized group membership service that can be incorporated into existing grid middleware to make it more reliable. This service includes a flexible failure detector that adapts dynamically to changing network conditions and can be configured with a number of failure recovery strategies. Moreover, it disseminates information about membership changes (new processes, failures, etc.) in a scalable and efficient manner. We conducted a preliminary evaluation of the proposed service by simulating a grid with up to 140 nodes distributed across three domains separated by a wide-area network. This evaluation showed that the proposed service performs well both in the absence and in the presence of process failures. Copyright 2008 ACM. Failure detection; Grid computing; Group membership Failure detection; Failure Detectors; Failure recovery; GRID middleware; Group membership service; Group memberships; Network condition; New process; Process failure; Detectors; Middleware; Grid computing;Public homecare programs such as the Brazilian Family Health program, initiated in the late 1990s, have proven to be a very effective tool for Preventive Medicine. The goal of these programs is to bring physicians, nurses, and social workers to the homes of the lower income population in lesser-attended regions within multi-mill"
Karina Valdívia Delgado,Universidade de São Paulo - Escola de Artes Ciências e Humanidades,"The Goal-Directed Risk-Sensitive Markov Decision Process allows arbitrary risk attitudes for the probabilistic planning problem to reach a goal state. In this problem, the risk attitude is modeled by an expected exponential utility and a risk factor λ. However, the problem is not well defined for every λ, posing the problem of defining the maximum (extreme) value for this factor. In this paper, we propose an algorithm to find this e-extreme risk factor and the corresponding optimal policy. © 2016 IEEE. Exponential Utility; Markov Decision Process; Risk averse Intelligent systems; Risk analysis; Exponential utility; Extreme risk factors; Goal directed; Markov Decision Processes; Optimal policies; Probabilistic planning; Risk attitude; Risk averse; Markov processes;In probabilistic planning problems which are usually modeled as Markov Decision Processes (MDPs), it is often difficult, or impossible, to obtain an accurate estimate of the state transition probabilities. This limitation can be overcome by modeling these problems as Markov Decision Processes with imprecise probabilities (MDP-IPs). Robust LAO* and Robust LRTDP are efficient algorithms for solving a special class of MDP-IPs where the probabilities lie in a given interval, known as Bounded-Parameter Stochastic-Shortest Path MDP (BSSP-MDP). However, they do not make clear what assumptions must be made to find a robust solution (the best policy under the worst model). In this paper, we propose a new efficient algorithm for BSSP-MDPs, called Robust ILAO* which has a better performance than Robust LAO* and Robust LRTDP, considered the-state-of-the art of robust probabilistic planning. We also define the assumptions required to ensure a robust solution and prove that Robust ILAO* algorithm converges to optimal values if the initial value of all states is admissible. © 2016, Springer Science+Business Media New York. Bounded-parameter Markov decision process; Heuristic search; Probabilistic planning Algorithms; Heuristic algorithms; Learning algorithms; Markov processes; Probability; Stochastic systems; Bounded parameters; Heuristic search; Imprecise probabilities; Markov Decision Processes; Probabilistic planning; State of the art; State transition probabilities; Stochastic shortest paths; Behavioral research;Markov Decision Processes have become the standard model for probabilistic planning. However, when applied to many practical problems, the estimates of transition probabilities are inaccurate. This may be due to conflicting elicitations from experts or insufficient state transition information. The Markov Decision Process with Imprecise Transition Probabilities (MDP-IPs) was introduced to obtain a robust policy where there is uncertainty in the transition. Although it has been proposed a symbolic dynamic programming algorithm for MDP-IPs (called SPUDD-IP) that can solve problems up to 22 state variables, in practice, solving MDP-IP problems is time-consuming. In this paper we propose efficient algorithms for a more general class of MDP-IPs, called Stochastic Shortest Path MDP-IPs (SSP MDP-IPs) that use initial state information to solve complex problems by focusing on reachable states. The (L)RTDP-IP algorithm, a (Labeled) Real Time Dynamic Programming algorithm for SSP MDP-IPs, is proposed together with three different methods for sampling the next state. It is shown here that the convergence of (L)RTDP-IP can be obtained by using any of these three methods, although the Bellman backups for this class of problems prescribe a minimax optimization. As far as we are aware, this is the first asynchronous algorithm for SSP MDP-IPs given in terms of a general set of probability constraints that requires non-linear optimization over imprecise probabilities in the Bellman backup. Our results show up to three orders of magnitude speedup for (L)RTDP-IP when compared with the SPUDD-IP algorithm. © 2015 Elsevier B.V. All rights reserved. Markov decision process; Probabilistic planning; Robust planning Algorithms; Behavioral research; Learning algorithms; Markov processes; Nonlinear programming; Optimization; Probability; Problem solving; Stochastic systems; Markov Decision Processes; Probabilistic planning; Probability constraints; Real-time dynamic programming; Robust planning; Stochastic shortest paths; Three orders of magnitude; Transition probabilities; Dynamic programming;Model-based Diagnosis is a well known AI technique that has been applied to software debugging for senior programmers, called Model-Based Software Debugging (MBSD). In this paper we describe the basis of MBSD and show how it can be used for educational purposes. By extending the classical diagnosis technique to a hierarchical approach, we built a programming learning system to allow a student to debug his program in different abstraction levels. © 2015, Springer Science+Business Media New York. Intelligent tutoring system; Model-based diagnosis; Pedagogical patterns; Program debugging Computer aided instruction; Education computing; Hierarchical systems; Program diagnostics; Teaching; Diagnosis techniques; Hierarchical approach; Hierarchical model; Intelligent tutoring system; Model based diagnosis; Pedagogical patterns; Programming learning; Software debugging; Program debugging;Scientific collaboration has been studied by researchers for decades. Several approaches have been adopted to address the question of how collaboration has evolved in terms of publication output, numbers of coauthors, and multidisciplinary trends. One particular type of collaboration that has received very little attention concerns advisor and advisee relationships. In this paper, we examine this relationship for the researchers who are involved in the area of Exact and Earth Sciences in Brazil and its eight subareas. These pairs are registered in the Lattes Platform that manages the individual curricula vitae of Brazilian researchers. The individual features of these academic researchers and their coauthoring relationships were investigated. We have found evidence that there exists positive correlation between time of advisor-advisee relationship with the advisee's productivity. Additionally, there has been a gradual decline in advisor-advisee coauthoring over a number of years as measured by the Kulczynski index, which could be interpreted as decline of the dependence. © 2015 Tuesta et al.  Article; author; bibliometrics; Brazil; cluster analysis; coauthor; computer; controlled study; data analysis; data mining; exploratory research; faculty student relation; geology; human; Kulczynski index; mathematical computing; mathematical parameters; postgraduate student; productivity; publication; publishing; research student; scientific literature; scientist; writing; geology; human relation; mentor; personnel; Authorship; Bibliometrics; Brazil; Earth Sciences; Humans; Interpersonal Relations; Mentors; Research Personnel;Bounded-parameter Markov decision process (BMDP) can be used to model sequential decision problems, where the transitions probabilities are not completely know and are given by intervals. One of the criteria used to solve that kind of problems is the maxim in, i.e., the best action on the worst scenario. The algorithms to solve BMDPs that use this approach include interval value iteration and an extension of real time dynamic programming (Robust-LRTDP). In this paper, we introduce a new algorithm, named B2RTDP, also based on real time dynamic programming that makes a different choice of the next state to be visited using upper and lower bounds of the optimal value function. The empirical evaluation of the algorithm shows that it converges faster than the state-of-the-art algorithms that solve BMDPs. © 2014 IEEE. Dynamic Programming; Markov Decision Process; Probabilistic Planning Algorithms; Decision theory; Intelligent systems; Iterative methods; Markov processes; Empirical evaluations; Markov Decision Processes; Optimal value functions; Probabilistic planning; Real-time dynamic programming; Sequential decisions; State-of-the-art algorithms; Upper and lower bounds; Dynamic programming;Recent advances in solutions to Hybrid MDPs with discrete and continuous state and action spaces have significantly extended the class of MDPs for which exact solutions can be derived, albeit at the expense of a restricted transition noise model. In this paper, we work around limitations of previous solutions by adopting a robust optimization approach in which Nature is allowed to adversarially determine transition noise within pre-specified confidence intervals. This allows one to derive an optimal policy with an arbitrary (user-specified) level of success probability and significantly extends the class of transition noise models for which Hybrid MDPs can be solved. This work also significantly extends results for the related ""chance- constrained"" approach in stochastic hybrid control to accommodate state-dependent noise. We demonstrate our approach working on a variety of hybrid MDPs taken from AI planning, operations research, and control theory, noting that this is the first time robust solutions with strong guarantees over all states have been automatically derived for such problems.  Confidence interval; Continuous state; Hybrid controls; Optimal policies; Robust optimization; Robust solutions; State-dependent noise; Transition noise; Artificial intelligence; Operations research; Optimization;This paper investigates Factored Markov Decision Processes with Imprecise Probabilities (MDPIPs); that is, Factored Markov Decision Processes (MDPs) where transition probabilities are imprecisely specified. We derive efficient approximate solutions for Factored MDPIPs based on mathematical programming. To do this, we extend previous linear programming approaches for linear approximations in Factored MDPs, resulting in a multilinear formulation for robust ""maximin"" linear approximations in Factored MDPIPs. By exploiting the factored structure in MDPIPs we are able to demonstrate orders of magnitude reduction in solution time over standard exact non-factored approaches, in exchange for relatively low approximation errors, on a difficult class of benchmark problems with millions of states. © 2011 Elsevier Inc. All rights reserved. Imprecise Markov Decision Processes (MDPIPs); Multilinear programming; Probabilistic planning Approximate solution; Bench-mark problems; Factored Markov decision process; Imprecise probabilities; Linear approximations; Low approximation; Markov Decision Processes; Maximin; Multilinear programming; Orders of magnitude; Probabilistic planning; Solution time; Transition probabilities; Markov processes; Probability; Mathematical programming;Many real-world decision-theoretic planning problems can be naturally modeled with discrete and continuous state Markov decision processes (DC-MDPs). While previous work has addressed automated decision-theoretic planning for DCMDPs, optimal solutions have only been defined so far for limited settings, e.g., DC-MDPs having hyper-rectangular piecewise linear value functions. In this work, we extend symbolic dynamic programming (SDP) techniques to provide optimal solutions for a vastly expanded class of DCMDPs. To address the inherent combinatorial aspects of SDP, we introduce the XADD - a continuous variable extension of the algebraic decision diagram (ADD) - that maintains compact representations of the exact value function. Empirically, we demonstrate an implementation of SDP with XADDs on various DC-MDPs, showing the first optimal automated solutions to DCMDPs with linear and nonlinear piecewise partitioned value functions and showing the advantages of constraint-based pruning for XADDs.  Automated solutions; Combinatorial aspect; Compact representation; Constraint-based; Continuous state; Continuous variables; Decision diagram; Decision-theoretic; Markov Decision Processes; Optimal solutions; Piece-wise; Piecewise linear; Planning problem; Value functions; Artificial intelligence; Markov processes; Optimal systems; Optimization; Piecewise linear techniques; Dynamic programming;When modeling real-world decision-theoretic planning problems in the Markov Decision Process (MDP) framework, it is often impossible to obtain a completely accurate estimate of transition probabilities. For example, natural uncertainty arises in the transition specification due to elicitation of MDP transition models from an expert or estimation from data, or non-stationary transition distributions arising from insufficient state knowledge. In the interest of obtaining the most robust policy under transition uncertainty, the Markov Decision Process with Imprecise Transition Probabilities (MDP-IPs) has been introduced to model such scenarios. Unfortunately, while various solution algorithms exist for MDP-IPs, they often require external calls to optimization routines and thus can be extremely time-consuming in practice. To address this deficiency, we introduce the factored MDP-IP and propose efficient dynamic programming methods to exploit its structure. Noting that the key computational bottleneck in the solution of factored MDP-IPs is the need to repeatedly solve nonlinear constrained optimization problems, we show how to target approximation techniques to drastically reduce the computational overhead of the nonlinear solver while producing bounded, approximately optimal solutions. Our results show up to two orders of magnitude speedup in comparison to traditional ""flat"" dynamic programming approaches and up to an order of magnitude speedup over the extension of factored MDP approximate value iteration techniques to MDP-IPs while producing the lowest error of any approximation algorithm evaluated. © 2011 Elsevier B.V. All rights reserved. Markov Decision Process; Probabilistic planning; Robust planning Approximation techniques; Computational bottlenecks; Computational overheads; Decision-theoretic; Dynamic programming methods; Markov Decision Processes; Non-linear solver; Nonlinear constrained optimization problems; Nonstationary; Optimal solutions; Optimization routine; Orders of magnitude; Planning problem; Probabilistic planning; Robust planning; Solution algorithms; Transition model; Transition probabilities; Value iteration; Approximation algorithms; Constrained optimization; Markov processes; Probability; Dynamic programming;Real-time dynamic programming (RTDP) solves Markov decision processes (MDPs) when the initial state is restricted. By visiting (and updating) only a fraction of the state space, this approach can be used to solve problems with intractably large state space. In order to improve the performance of RTDP, a variant based on symbolic representation was proposed, named sRTDP. Traditional RTDP approaches work best on problems with sparse transition matrices where they can often efficiently achieve ε-convergence without visiting all states; however, on problems with dense transition matrices where most states are reachable in one step, the sRTDP approach shows an advantage over traditional RTDP by up to three orders of magnitude, as we demonstrate in this paper. We also specify a new variant of sRTDP based on BRTDP, named sBRTDP, which converges quickly when compared to RTDP variants, since it does less updating by making a better choice of the next state to be visited. © 2010 Springer-Verlag.  Initial state; Markov Decision Processes; One step; State space; Symbolic representation; Three orders of magnitude; Transition matrices; Markov processes; Problem solving; Dynamic programming;The Affin ADD (AADD) is an extension of the Algebraic Decision Diagram (ADD) that compactly represents context-specific, additive and multiplicative structure in functions from a discrete domain to a real-valued range. In this paper, we introduce a novel algorithm for efficientl findin AADD approximations that we use to develop the MADCAP algorithm for AADD-based structured approximate dynamic programming (ADP) with factored MDPs. MADCAP requires less time and space to achieve comparable or better approximate solutions than the current state-of-the-art ADD-based ADP algorithm of APRICODD and can provide approximate solutions for problems with context-specific additive and multiplicative structure on which APRICODD runs out of memory. Copyright © 2010, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved. Approximate Dynamic Programming; Markov Decision Processes; Planning Autonomous agents; Markov processes; Multi agent systems; Planning; Approximate dynamic programming; Approximate solution; Decision diagram; Discrete domains; Markov Decision Processes; Novel algorithm; Out-of-memory; Approximation algorithms;When modeling real-world decision-theoretic planning problems in the Markov decision process (MDP) framework, it is often impossible to obtain a completely accurate estimate of transition probabilities. For example, natural uncertainty arises in the transition specification due to elicitation of MDP transition models from an expert or data, or non-stationary transition distributions arising from insufficient state knowledge. In the interest of obtaining the most robust policy under transition uncertainty, the Markov Decision Process with Imprecise Transition Probabilities (MDP-1Ps) has been introduced to model such scenarios. Unfortunately, while solutions to the MDP-1P are well-known, they require nonlinear optimization and are extremely time-consuming in practice. To address this deficiency, we propose efficient dynamic programming methods to exploit the structure of factored MDP-1Ps. Noting that the key computational bottleneck in the solution of MDP-1Ps is the need to repeatedly solve nonlinear constrained optimization problems, we show how to target approximation techniques to drastically reduce the computational overhead of the nonlinear solver while producing bounded, approximately optimal solutions. Our results show up to two orders of magnitude speedup in comparison to traditional ""flat"" dynamic programming approaches and up to an order of magnitude speedup over the extension of factored MDP approximate value iteration techniques to MDP-1Ps. Copyright © 2009, Association for the Advancement of Artificial Intelligence. All rights reserved.  Approximation techniques; Computational bottlenecks; Computational overheads; Decision-theoretic; Dynamic programming methods; Markov Decision Processes; Non-linear optimization; Non-linear solver; Nonlinear constrained optimization problems; Nonstationary; Optimal solutions; Order of magnitude; Orders of magnitude; Planning problem; Real-world; Transition model; Transition probabilities; Value iteration; Constrained optimization; Markov processes; Probability; Scheduling; Dynamic programming;This paper investigates Factored Markov Decision Processes with Imprecise Probabilities; that is, Markov Decision Processes where transition probabilities are imprecisely specified, and where their specification does not deal directly with states, but rather with factored representations of states. We first define a Factored MDPIP, based on a multilinear formulation for MDPIPs; then we propose a novel algorithm for generation of Γ-maximin policies for FactoredMDPIPs. We also developed a representation language for Factored MDPIPs (based on the standard PPDDL language); finally, we describe experiments with a problem of practical significance, the well-known System Administrator Planning problem. Imprecise markov decision processes (MDPIPs); Knowledge representation languages; Multilinear programming; Probabilistic planning and PPDDL Factored Markov decision process; Imprecise probabilities; Knowledge representation language; Markov Decision Processes; Multi-linear programming; Probabilistic planning; Representation languages; Transition probabilities; Algorithms; Knowledge representation; Markov processes; Probabilistic logics; Probability;It is not easy for a student to develop programming skills and learn how to construct their own problem solving algorithms. Well designed materials and tools can guide programming students knowledge and skill construction. Such tools may allow students to acquire better and faster, the necessary programming skills. In this paper we show the results of some experiments realized on a set of faulty student's programs using PROPAT_DEBUG, an automatic program debugger, based on the Model Based Diagnosis technique of Artificial Intelligence. The results show that during the interactive debugging process it is possible for a student to learn by answering the questions posed by the AI diagnosis system to discriminate its fault hypotheses. © Springer-Verlag Berlin Heidelberg 2006.  Algorithms; Artificial intelligence; Computer aided software engineering; Learning systems; Problem solving; Students; Diagnosis system; Model Based Diagnosis; Program debugger; Programming learning; Computer programming;Programming Patterns help create a shared language for communicating insight and experience about programming problems and their solutions. Inspired by this idea, we developed the ProPAT e-learning tool: an Eclipse IDE that allows students of a first Computer Science course to learn how to program using pedagogical patterns, i.e., a set of programming patterns recommended by Computer Science educators. ProPAT has been implemented as an Eclipse plug- in with two main perspectives: the Teacher Perspective and the Student Perspective. To identify some of the students' mistakes, the ProPAT plug-in also includes a program diagnosis system that uses Model Based Diagnosis techniques from the Artificial Inteligence. Copyright © 2005 IBM. Computer-based learning; Debugging and testing tools; Teaching introductory undgraduate Programming Computer Science course; Computer-based learning; Diagnosis systems; E-learning tool; Model based diagnosis; Plug-ins; Programming learning; Programming patterns; Programming problem; Shared language; Computer debugging; Computer science; E-learning; Java programming language; Program debugging; Software testing; Students; Teaching; Program diagnostics;Research on cognitive theories about programming learning suggests that experienced programmers solve problems by looking for previous solutions that are related to the new problem and that can be adapted to the current situation. Inspired by these ideas, programming teachers have developed a pattern based programming instruction. In this model, learning can be seen as a process of pattern recognition, which compares experiences from the past with the current situation. In this work, we present a new Eclipse programming environment in which a student can program using a set of pedagogical patterns, i.e., elementary programming patterns recommended by a group of teachers. © Springer-Verlag 2004.  Computer aided instruction; Computer programming; Education; Intelligent vehicle highway systems; Pattern recognition; Teaching; Cognitive theory; Current situation; Programming environment; Programming instruction; Programming learning; Programming patterns; Highway planning"
Clodoaldo Aparecido Moraes de Lima,"Universidade de São Paulo - Escola de Artes, Ciências e Humanidades","An interaction between humans or between a human and a machine will be more effective if it is supported by gestures. In different levels of complexity, the communication system used in human interaction includes the use of gesture. In natural conversation, for instance, speakers use gestures for both to enhance the expressiveness of their speech and to support their own linguistic reasoning. The audience absorbs the content being transmitted also based on the speakers' gesticulation. Thus, an analysis of gestures should add value to the purpose of the interaction. One of the concerns in the analysis of gestures is the problem arising from the segmentation of phases of a gesture (rest position, preparation, stroke, hold and retraction), which, from the standpoint of Gesture Theory, may reveal information on prosody and semantics of what is being said in a discourse. Finding an automation solution to this problem involves enabling the development of theoretical and application areas that are based on the analysis of human behavior and on the interpretation and generation of natural language. In this study, the problem of gesture phase segmentation is modeled as a problem of classification, and then support vector machine is employed to design a model able to learn the patterns of gesture that are inherent to each phase. This work presents two main highlights. The first is to address the limitations of the segmentation approach through the study of its performance in different scenarios that represent the complexity of analyzing patterns of human behavior. In this study, we reached an F-score around 0.9 for rest position and around 0.8 for stroke and preparation as segmentation results in the best cases. Moreover, it was possible to investigate how classification models are influenced by human behavior. The second highlight refers to the conduction of an analysis by considering the standpoint of specialists concerned with gesture phase segmentation in the area of Linguistics and Psycholinguistics, through which we obtained impressive results. Thus, in regard to the suitability of our approach, it is a feasible means of supporting the development of the Gesture Theory as well as the Computational Linguistics and Human Machine Interaction fields. © 2016 Elsevier Ltd. All rights reserved. Gesture analysis; Gesture phase segmentation; Machine learning; Support vector machine Artificial intelligence; Behavioral research; Classification (of information); Computational linguistics; Learning systems; Linguistics; Social sciences; Support vector machines; Automation solutions; Classification models; Gesture analysis; Human interactions; Human machine interaction; Linguistic reasonings; Natural languages; Segmentation results; Semantics;Surface electromyography (EMG) signals have been studied extensively in the last years aiming at the automatic classification of hand gestures and movements as well as the early identification of latent neuromuscular disorders. In this paper, we investigate the potentials of the conjoint use of relevance vector machines (RVM) and fractal dimension (FD) for automatically identifying EMG signals related to different classes of limb motion. The adoption of FD as the mechanism for feature extraction is justified by the fact that EMG signals usually show traces of self-similarity. In particular, four well-known FD estimation methods, namely box-counting, Higuchi’s, Katz’s and Sevcik’s methods, have been considered in this study. With respect to RVM, besides the standard formulation for binary classification, we also investigate the performance of two recently proposed variants, namely constructive mRVM and top-down mRVM, that deal specifically with multiclass problems. These classifiers operate solely over the features extracted by the FD estimation methods, and since the number of such features is relatively small, the efficiency of the classifier induction process is ensured. Results of experiments conducted on a publicly available dataset involving seven distinct types of limb motions are reported whereby we assess the performance of different configurations of the proposed RVM+FD approach. Overall, the results evidence that kernel machines equipped with the FD feature values can be useful for achieving good levels of classification performance. In particular, we have empirically observed that the features extracted by the Katz’s method is of better quality than the features generated by other methods. © 2015, The Natural Computing Applications Forum. EMG signal classification; Feature extraction; Fractal dimension; Relevance vector machines Classification (of information); Electromyography; Extraction; Feature extraction; Finite difference method; Fractal dimension; Fractals; Automatic classification; Binary classification; Classification performance; Electromyography signals; Emg signal classifications; Neuromuscular disorders; Relevance Vector Machine; Surface electromyography signals; Biomedical signal processing;In recent years, human identification based on face recognition has attracted the attention of the scientific community and the general public due to its wide range of applications. A face recognition system involves three important phases: face detection, feature extraction and classification (identification and/or verification). The robustness of face recognition could be improved by treating the variations in these stages. One of the main issues in design of face recognition system is how to extract discriminative facial features. A precise extraction of a representative feature set will improve the performance of a face recognition system. Various techniques have been used to represent images efficiently, of which the most well-known and widely applied are Wavelet, Contourlet, Shearlet and Curvelet Transform. Their ability to capture localized time-frequency information of image motivates their use for feature extraction. In this paper, we conduct a systematic empirical study on these transforms as feature extractors from face images. To further reduce the feature dimensionality, we adopt Principal Component Analysis and Linear Discriminant Analysis to select the most discriminative feature sets. The performance levels delivered by each transform are contrasted in terms of the accuracy measure computed over the outputs generated by the Support Vector Machine classifier (SVM). Experimental results conducted on a publicly available database are reported whereby we observe that the Curvelet Transform followed by the Wavelet Transform significantly outperform the others according to accuracy measure calculated over the SVM classifier. © 2015 IEEE. Contourlet Transform; Curvelet Transform; Face Recognition; Shearlet Transform; Support Vector Machine; Wavelet Transform Classification (of information); Discriminant analysis; Extraction; Feature extraction; Image enhancement; Image processing; Image retrieval; Principal component analysis; Support vector machines; Wavelet transforms; Contourlet transform; Curvelet transforms; Face recognition systems; Feature extraction and classification; Linear discriminant analysis; Shearlet transforms; Support vector machine classifiers; Time frequency information; Face recognition;Epilepsy refers to a set of chronic neurological syndromes characterized by transient and unexpected electrical disturbances of the brain. The detailed analysis of the electroencephalogram (EEG) is one of the most influential steps for the proper diagnosis of this disorder. This work presents a systematic performance evaluation of the recently introduced optimum path forest (OPF) classifier when coping with the task of epilepsy diagnosis directly through EEG signal analysis. For this purpose, we have made extensive use of a benchmark dataset composed of five classes, whose full discrimination is very hard to achieve. Four types of wavelet functions and three well-known filter methods were considered for the tasks of feature extraction and selection, respectively. Moreover, support vector machines configured with radial basis function (SVM-RBF) kernel, multilayer perceptron neural networks (ANN-MLP), and Bayesian classifiers were used for comparison in terms of effectiveness and efficiency. Overall, the results evidence the outperformance of the OPF classifier in both types of criteria. Indeed, the OPF classifier was usually extremely fast, with average training/testing times much lower than those required by SVM-RBF and ANN-MLP. Moreover, when configured with Coiflets as feature extractors, the performance scores achieved by the OPF classifier include 89.2% as average accuracy and sensitivity/specificity values higher than 80% for all five classes. © 2014 Elsevier B.V. Bayesian; EEG signal classification; Multilayer perceptrons; Optimum path forest; Support vector machines; Wavelets Electric loads; Feature extraction; Forestry; Multilayer neural networks; Neural networks; Neurology; Radial basis function networks; Support vector machines; Bayesian; EEG signal classification; Effectiveness and efficiencies; Electro-encephalogram (EEG); Feature extraction and selection; Multi-layer perceptron neural networks; Optimum-path forests; Wavelets; Electroencephalography; article; artificial neural network; Bayesian learning; classifier; electroencephalogram; empirical research; epilepsy; learning algorithm; optimum path forest; perceptron; predictive value; priority journal; radial based function; support vector machine; wavelet analysis; Forestry; Neural Networks; Neurology;The study of electromyographic (EMG) signals has gained increased attention in the last decades since the proper analysis and processing of these signals can be instrumental for the diagnosis of neuromuscular diseases and the adaptive control of prosthetic devices. As a consequence, various pattern recognition approaches, consisting of different modules for feature extraction and classification of EMG signals, have been proposed. In this paper, we conduct a systematic empirical study on the use of Fractal Dimension (FD) estimation methods as feature extractors from EMG signals. The usage of FD as feature extraction mechanism is justified by the fact that EMG signals usually show traces of self-similarity and by the ability of FD to characterize and measure the complexity inherent to different types of muscle contraction. In total, eight different methods for calculating the FD of an EMG waveform are considered here, and their performance as feature extractors is comparatively assessed taking into account nine well-known classifiers of different types and complexities. Results of experiments conducted on a dataset involving seven distinct types of limb motions are reported whereby we could observe that the normalized version of the Katzs estimation method and the Hurst exponent significantly outperform the others according to a class separability measure and five well-known accuracy measures calculated over the induced classifiers. © 2014 Elsevier Ltd. All right sreserved. EMG signal classification; Feature extraction; Fractal dimension Fractal dimension; Emg signal classifications; Feature extractor; Fractal dimension method; Feature extraction;Currently, automated gesture analysis is being widely used in different research areas, such as human-computer interaction or human-behavior analysis. With regard to the latter area in particular, gesture analysis is closely related to studies on human communication. Linguists and psycholinguists analyze gestures from several standpoints, and one of them is the analysis of gesture segments. The aim of this paper is to outline an approach to automate gesture unit segmentation, as a way of assisting linguistic studies. This objective was attained by employing a Machine Learning technique with the aid of a spatial-temporal data representation. Copyright © 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.  Artificial intelligence; Behavioral research; Learning systems; Linguistics; Gesture analysis; Human behavior analysis; Human communications; Machine learning techniques; Spatial temporals; Spatial-temporal data; Human computer interaction;Gesture analysis has been widely used for developing new methods of human-computer interaction. The advancement reached in the gesture analysis area is also motivating its application to automate tasks related to discourse analysis, such as the gesture phases segmentation task. In this paper, we present an initiative that aims at segmenting gestures, especially considering the ""units""- the larger grain involved in gesture phases segmentation. Thereunto, we have captured the gestures using a Xbox Kinect™ device, modeled the problem as a classification task, and applied Support Vector Machines. Moreover, aiming at taking advantage from the temporal aspects involved in the problem, we have used several types of data pre-processing in order to consider time domain and frequency domain features. Copyright 2013 ACM. Gesture analysis; Gesture segmentation; Gesture unit; Support vector machine; Temporal modeling Classification tasks; Data preprocessing; Discourse analysis; Frequency domains; Gesture analysis; Gesture segmentation; Gesture unit; Temporal modeling; Computation theory; Support vector machines;Recently, Support Vector Machines have presented promissing results to various machine learning tasks, such as classification and regression. These good results have motivated its application to several complex problems, including temporal information analysis. In this context, some studies attempt to extract temporal features from data and submit these features in a vector representation to traditional Support Vector Machines. However, Support Vector Machines and its traditional variations do not consider temporal dependency among data. Thus, some approaches adapt Support Vector Machines internal mechanism in order to integrate some processing of temporal characteristics, attempting to make them able to interpret the temporal information inherent on data. This paper presents a review on studies covering this last approach for dealing with temporal information: incorporating temporal reasoning into Support Vector Machines and its variations. © 2012 IEEE. Machine learning; Support vector machine; Temporal reasoning Complex problems; Temporal characteristics; Temporal features; Temporal information; Temporal reasoning; Vector representations; Learning systems; Support vector machines;This paper describes a hybrid architecture that provides automatic classification for a set of gestures. Such architecture combines fuzzy-connectionist, heuristic and syntactical pattern recognition approaches, and deals with gesture recognition based on primitives. The modeling with primitives allows the use of multiples classifiers in order to achieve high classification accuracy. The heuristic classifier and the fuzzy syntactical integrating strategy are described in this paper. The fuzzy-connectionist classifiers were discussed in previous works and they are now revisited just to present the set of parameters that solves the current proof of concept, in the scope of Brazilian Sign Language Manual Alphabet. The fuzzy syntactical strategy coupled with the modeling with primitives has improved the pattern recognition results, enabling the design of architecture for classification with high flexibility and scalability to development of applications in different signed communication contexts. The experimental results show that the proposed approach is valid and has promising application. © 2012 IEEE. Fuzzy Syntactical Pattern Recognition; Fuzzy-Connectionist Pattern Recognition; Gesture Recognition; Hybrid Architecture; Multiple Classifiers Automatic classification; Classification accuracy; High flexibility; Hybrid architectures; Multiple classifiers; Proof of concept; Sign language; Syntactical pattern recognition; Neural networks; Pattern recognition; Gesture recognition;Objective: We carry out a systematic assessment on a suite of kernel-based learning machines while coping with the task of epilepsy diagnosis through automatic electroencephalogram (EEG) signal classification. Methods and materials: The kernel machines investigated include the standard support vector machine (SVM), the least squares SVM, the Lagrangian SVM, the smooth SVM, the proximal SVM, and the relevance vector machine. An extensive series of experiments was conducted on publicly available data, whose clinical EEG recordings were obtained from five normal subjects and five epileptic patients. The performance levels delivered by the different kernel machines are contrasted in terms of the criteria of predictive accuracy, sensitivity to the kernel function/parameter value, and sensitivity to the type of features extracted from the signal. For this purpose, 26 values for the kernel parameter (radius) of two well-known kernel functions (namely, Gaussian and exponential radial basis functions) were considered as well as 21 types of features extracted from the EEG signal, including statistical values derived from the discrete wavelet transform, Lyapunov exponents, and combinations thereof. Results: We first quantitatively assess the impact of the choice of the wavelet basis on the quality of the features extracted. Four wavelet basis functions were considered in this study. Then, we provide the average accuracy (i.e., cross-validation error) values delivered by 252 kernel machine configurations; in particular, 40%/35% of the best-calibrated models of the standard and least squares SVMs reached 100% accuracy rate for the two kernel functions considered. Moreover, we show the sensitivity profiles exhibited by a large sample of the configurations whereby one can visually inspect their levels of sensitiveness to the type of feature and to the kernel function/parameter value. Conclusions: Overall, the results evidence that all kernel machines are competitive in terms of accuracy, with the standard and least squares SVMs prevailing more consistently. Moreover, the choice of the kernel function and parameter value as well as the choice of the feature extractor are critical decisions to be taken, albeit the choice of the wavelet family seems not to be so relevant. Also, the statistical values calculated over the Lyapunov exponents were good sources of signal representation, but not as informative as their wavelet counterparts. Finally, a typical sensitivity profile has emerged among all types of machines, involving some regions of stability separated by zones of sharp variation, with some kernel parameter values clearly associated with better accuracy rates (zones of optimality). © 2011 Elsevier B.V. EEG signal classification; Epilepsy; Feature extraction; Kernel machines Accuracy rate; Comparative studies; Cross validation; EEG recording; EEG signal classification; EEG signals; Electroencephalogram signals; Epilepsy; Epileptic patients; Exponential radial basis; Feature extractor; Gaussians; Kernel function; Kernel machine; Kernel parameter; Kernel-based learning; Lagrangian; Least Square; Lyapunov exponent; Optimality; Parameter values; Performance level; Predictive accuracy; Relevance Vector Machine; Sensitivity profiles; Signal representations; Systematic assessment; Wavelet basis; Wavelet basis functions; Differential equations; Discrete wavelet transforms; Feature extraction; Least squares approximations; Lyapunov functions; Lyapunov methods; Radial basis function networks; Sensitivity analysis; Standards; Support vector machines; Electroencephalography; article; calibration; clinical article; controlled study; diagnostic accuracy; disease classification; electroencephalogram; epilepsy; human; kernel method; priority journal; quantitative analysis; sensitivity and specificity; support vector machine; validation process; Artificial Intelligence; Electroencephalography; Epilepsy; Humans; Sensitivity and Specificity; Support Vector Machines;The electroencephalogram (EEG) signal captures the electrical activity of the brain and is an important source of information for studying neurological disorders. The proper analysis of this biological signal plays an important role in the domain of brain-computer interface, which aims at the construction of communication channels between human brain and computers. In this paper, we investigate the application of least squares support vector machines (LS-SVM) to the task of epilepsy diagnosis through automatic EEG signal classification. More specifically, we present a sensitivity analysis study by means of which the performance levels exhibited by standard and least squares SVM classifiers are contrasted, taking into account the setting of the kernel function and of its parameter value. Results of experiments conducted over different types of features extracted from a benchmark EEG signal dataset evidence that the sensitivity profiles of the kernel machines are qualitatively similar, both showing notable performance in terms of accuracy and generalization. In addition, the performance accomplished by optimally configured LS-SVM models is also quantitatively contrasted with that obtained by related approaches for the same dataset. © 2010 Elsevier Ltd. EEG signal classification; Epilepsy; Kernel functions; Least squares support vector machines; Sensitivity analysis Biological signals; Communication channel; Data sets; EEG signal classification; EEG signals; Electrical activities; Electroencephalogram signals; Human brain; Kernel function; Kernel functions; Kernel machine; Least Square; Least squares support vector machines; Neurological disorders; Parameter values; Performance level; Sensitivity profiles; SVM classifiers; SVM model; Brain; Brain computer interface; Electroencephalography; Interfaces (computer); Optical sensors; Support vector machines; Vectors; Sensitivity analysis; article; electroencephalogram; epilepsy; human; human experiment; normal human; performance; priority journal; sensitivity analysis; support vector machine; Algorithms; Artificial Intelligence; Brain; Electroencephalography; Epilepsy; Humans; Least-Squares Analysis; Signal Processing, Computer-Assisted;In this paper we extend an estimation technique based on Viterbi algorithm for discrete-time chaotic signals immersed in noise. The proposed modification allows for orbits generated by maps with nonuniform invariant density to be estimated. This modified Viterbi algorithm is used in two digital modulation schemes: the Modified Maximum Likelihood Chaos Shift Keying using one and two maps. Both have better symbol error rate characteristics than non-coherent chaos communication schemes. © 2009 IFAC. Chaos theory; Communication systems; Difference equations; Digital communications; Estimation theory Chaos communications; Chaos-shift keying; Chaotic signal; Digital communications; Digital modulations; Discrete-time; Estimation techniques; Estimation theory; Invariant densities; Non-coherent; Symbol error rates; Communication systems; Difference equations; Digital communication systems; Estimation; Maximum likelihood estimation; Viterbi algorithm; Chaotic systems;Support vector machine (SVM) is a machine learning technique widely applied in classification problems. SVM are based on the Vapnik's Statistical Learning Theory, and successively extended by a number of researchers. On the order hand, the electroencephalogram (EEG) signal captures the electrical activity of the brain and is an important source of information for studying neurological disorders. In order to extract relevant information of EEG signal, a variety of computerized-analysis methods have been developed. Recent studies indicate that methods based on the nonlinear dynamics theory can extract valuable information from neuronal dynamics. However, many these of methods need large amount of data and are computationally expensive. From chaos theory, a global value that is relatively simple to compute is the fractal dimension (FD), it can be used to measure the geometrical complexity of a time series. The FD of a waveform represents a powerful tool for transient detection. In analysis of EEG this feature can been used to identify and distinguish specific states of physiologic function. A variety of algorithms are available for the computation of FD. In this work, we employ SVM to classify the EEG signals from healthy subjects and epileptic subjects using as the features vector the FD. From the experimental results, we can see that classification based on SVM with FD perform well in EEG signals classification, which indicates this classification method is valid and has promising application. © 2009 IEEE.  Analysis method; Automatic recognition; Classification methods; EEG signals; EEG signals classification; Electrical activities; Electroencephalogram signals; Epileptic seizures; Features vector; Geometrical complexity; Healthy subjects; Machine learning techniques; Neurological disorders; Neuronal dynamics; Nonlinear dynamics theory; Physiologic function; Specific state; Statistical learning theory; Transient detection; Wave forms; Brain; Chaos theory; Finite difference method; Fractal dimension; Image retrieval; Learning algorithms; Multilayer neural networks; Support vector machines; Time series; Electroencephalography;Nowadays, online games have an exponential increase in the market because many people interact for hours in a virtual gaming worlds called the Massive Multiplayer Online Role-Playing Games (MMORPGs). In this kind of environment players maintain relationships and build communities. To study the common characteristics and relationships of the communities formed in those games, it is possible to cluster a player's community. Moreover, player's community structure is common in various real-world networks; methods or algorithms for grouping such communities have attracted great attention in recent years. The analysis of those groups aim to better understand and examine the behaviour of players. In this paper, self-organizing maps were explored to obtain clusters of a player community from the game World of Warcraft (WoW). To improve the efficiency of the clustering methodology masks were applied that considered the player's individual score, player's guild degree (number of connections), and player's class. The results obtained indicate that the proposed methodology can be successfully applied to the clustering online game communities. © 2009 IEEE.  Community structures; Exponential increase; Mmorpgs; Multiplayers; On-line games; Real-world networks; Role-playing game; Conformal mapping; Self organizing maps;Support Vector Machine (SVM) classifiers are high-performance classification models devised to comply with the structural risk minimization principle and to properly exploit the several SVM variants, Least-Squares SVMs (LS-SVMs) have gained increased attention recently due mainly to their computationally attractive properties coming as the direct result of applying a modified formulation that makes use of a sum-squared-error cost function jointly with equality, instead of inequality, constraints. In this work, we present a flexible hybrid approach aimed at augmenting the proficiency of LS-SVM classifiers with regard to accuracy/generalization as well as to hyperparameter calibration issues. Such approach, named as Mixtures of Weighted Least-Squares Support Vector Machine Experts, centers around the fusion of the weighted variant of LS-SVMs with Mixtures of Experts models. After the formal characterization of the novel learning framework, simulation results obtained with respect to both binary and multiclass pattern classification problems are reported, ratifying the suitability of the novel hybrid approach in improving the performance issues considered. © Springer-Verlag London Limited 2008. Hybridization; Least squares support vector machines; Mixtures of experts; Pattern classification ;In this paper, we investigate the potentials of applying a kernel-based learning machine, the Relevance Vector Machine (RVM), to the task of epilepsy detection through automatic electroencephalogram (EEG) signal classification. For this purpose, some experiments have been conducted over publicly available data, contrasting the performance levels exhibited by RVM models with those achieved with Support Vector Machines (SVMs), both in terms of predictive accuracy and sensitivity to the choice of the kernel function. Four settings of both types of kernel machine were considered in this study, which vary in accord with the type of input data they receive, either raw EEG signal or some statistical features extracted from the wavelet-transformed data. The empirical results indicate that: (1) in terms of accuracy, the best-calibrated RVM models have shown very satisfactory performance levels, which are rather comparable to those of SVMs; (2) an increase of accuracy is sometimes accompanied by loss of sparseness in the resulting RVM models; (3) both types of machines present similar sensitivity profiles to the kernel functions considered, having some kernel parameter values clearly associated with better accuracy rate; (4) when not making use of a feature extraction technique, the choice of the kernel function seems to be very relevant for significantly leveraging the performance of RVMs; and (5) when making use of derived features, the choice of the feature extraction technique seems to be an important factor to one take into account. © 2009 Elsevier Ltd. All rights reserved. EEG signal classification; Epilepsy; Kernel machines; Sensitivity analysis Accuracy rates; EEG signal classification; EEG signals; Electro-encephalogram signals; Empirical results; Epilepsy; Epilepsy detections; Feature extraction techniques; Input datum; Kernel functions; Kernel machines; Kernel parameters; Kernel-based learning; Performance levels; Predictive accuracies; Relevance vector machines; Sensitivity profiles; Statistical features; Support vectors; Electroencephalography; Face recognition; Sensitivity analysis; Support vector machines; Feature extraction;Online games have had an exponential increase in the market. Today, many people interact for hours in a virtual gaming world called the Massive Multiplayer Online Role-Playing Games (MMORPGs). The players maintain relationships and build communities, formed by diverse people who establish links in very different ways. To study the communities formed in those games, it is possible to cluster a player's community by common characteristics, and examine their relationships. This paper employs self-organizing maps to obtain the clusters of a player community from the game Ragnarök. Aiming to improve the performance of the clusterization, a fuzzy system was designed through genetic algorithms to measure the relevance of the inputs. The results obtained indicate that the proposed methodology can be successfully applied to the clusterization of a multiplayer community. © 2008 IEEE.  Clusterization; Exponential increases; Multiplayer; On-line games; Virtual gamings; Conformal mapping; Diesel engines; Fuzzy logic; Fuzzy systems; Game theory; Genetic algorithms; Optical projectors; Self organizing maps;Support vector machines (SVMs) have established themselves as a state-of-the-art technique for coping with non-trivial machine learning problems. Among the SVM variants, least-squares SVMs have gained increased attention recently due to the computational benefits they usually entail. Although considered as high-performance models, it is consensual that the applicability of these vector machines depends very much on a proper choice of some control parameters. In this paper, we present a sensitivity analysis study contrasting the performance profiles exhibited by standard and least-squares SVM classifiers with respect to the calibration of the kernel parameter value alone. The results achieved with simulations involving seven dataseis indicate that the performance profiles are usually qualitatively similar for the two types of vector machines, both presenting kernel parameter values clearly associated with a better performance, and that the choice of the kernel function seems to be more critical than that of its parameter value. © 2007 IEEE.  Control parameters; International conferences; Kernel functions; Kernel parameters; Least squares (LS); Machine-learning; Multi-layer support vector machines (SVM) classifiers; Non-trivial; Parameter values; Performance modelling; Support vector machines ((SVM)); Systems design; Two types; Vector machines; Artificial intelligence; Classification (of information); Classifiers; Computational fluid dynamics; Control system analysis; Control theory; Intelligent control; Intelligent systems; Learning systems; Least squares approximations; Military data processing; Parameter estimation; Standards; Support vector machines; Systems analysis; Vectors; Sensitivity analysis;Constructive algorithms have shown to be reliable and effective methods for designing Artificial Neural Networks (ANN) with good accuracy and generalization capability, yet with parsimonious network structures. Projection Pursuit Learning (PPL) has demonstrated great flexibility and effectiveness in performing this task, though presenting some difficulties in the search for appropriate projection directions in input spaces with high dimensionality. Due to the existence of high-dimensional input spaces in the context of time series prediction, mainly under the existence of long-term dependencies in the time series, we propose here a method based on the wrapper methodology to perform variable selection, so that only a subset of highly-informative lags is going to be considered as the regression vector. The Yearly Sunspot Number time series is adopted as a case study and comparative analysis is performed considering alternative approaches in the literature, guiding to competitive results. ©2007 IEEE.  Artificial intelligence; Backpropagation; Computer networks; Solar energy; Input spaces; Joint conference; Projection pursuit learning; Time-series; Neural networks;Mixture of experts (ME) models comprise a family of modular neural network architectures aiming at distilling complex problems into simple subtasks. This is done by deploying a separate gating module for softly dividing the input space into overlapping regions to be each assigned to one or more expert networks. Conversely, support vector machines (SVMs) refer to kernel-based methods, neural-network-alike models that constitute an approximate implementation of the structural risk minimization principle. Such learning machines follow the simple, but powerful idea of nonlinearly mapping input data into high-dimensional feature spaces wherein a linear decision surface discriminating different regions is properly designed. In this work, we formally characterize and empirically evaluate a novel approach, named as Mixture of Support Vector Machine Experts (MSVME), whose main purpose is to combine the complementary properties of both SVM and ME models. In the formal characterization, an algorithm based on a maximum likelihood criterion is considered for the MSVME training, and we demonstrate that it is possible to train each expert based on an SVM perspective. Regarding the empirical evaluation, simulation results involving nonlinear dynamic system identification problems are reported, contrasting the performance shown by the MSVME approach with that exhibited by conventional SVM and ME models. © 2007 Elsevier Inc. All rights reserved. Hybridization; Kernels; Mixtures of experts; Nonlinear systems identification; Support vector machines Approximation theory; Computer simulation; Mathematical models; Mixtures; Risk management; Support vector machines; Hybridization; Hybridizing mixtures; Kernels; Mixture of experts (ME); Nonlinear systems identification; Risk minimization; Expert systems;A probabilistic learning technique, known as gated mixture of experts (MEs), is made more adaptive by employing a customized genetic algorithm based on the concepts of hierarchical mixed encoding and hybrid training. The objective of such effort is to promote the automatic design (i.e., structural configuration and parameter calibration) of whole gated ME instances more capable to cope with the intricacies of some difficult machine learning problems whose statistical properties are time-variant. In this chapter, we outline the main steps behind such novel hybrid intelligent system, focusing on its application to the nontrivial task of nonlinear time-series forecasting. Experiment results are reported with respect to three benchmarking time-series problems and confirmed our expectation that the new integrated approach is capable to outperform - both in terms of accuracy and generalization - other conventional approaches, such as single neural networks and non-adaptive, handcrafted gated MEs. © 2007, Idea Group Inc.  ;Projection Pursuit Learning (PPL) refers to a well-known constructive learning algorithm characterized by a very efficient and accurate computational procedure oriented to nonparametric regression. It has been employed as a means to counteract some problems related to the design of Artificial Neural Network (ANN) models, namely, the estimation of a (usually large) number of free parameters, the proper definition of the model's dimension, and the choice of the sources of nonlinearities (activation functions). In this work, the potentials of PPL are exploited through a different perspective, namely, in designing one-hidden-layer feedforward ANNs for the adaptive control of nonlinear dynamic systems. For such purpose, the proposed methodology is divided into three stages. In the first, the model identification process is undertaken. In the second, the ANN structure is defined according to an offline control setting. In these two stages, the PPL algorithm estimates not only the optimal number of hidden neurons but also the best activation function for each node. The final stage is performed online and promotes a fine-tuning in the parameters of the identification model and the controller. Simulation results indicate that it is possible to design effective neural models based on PPL for the control of nonlinear multivariate systems, with superior performance when compared to benchmarks. ©2006 IEEE. Artificial neural network; Control; Dynamic systems; Projection pursuit learning Adaptive control systems; Adaptive systems; Backpropagation; Boolean functions; Computer simulation; Control systems; Dynamic programming; Dynamical systems; Education; Estimation; Food processing; Function evaluation; Intelligent control; Intelligent systems; Learning algorithms; Learning systems; Modal analysis; Neural networks; Parameter estimation; Projection systems; Schrodinger equation; Stages; Activation function (AF); Adaptive Control; Artificial Neural Network (ANN) models; Constructive learning algorithm; Feed forward (FF); Free parameters; Hidden neurons; Identification models; International (CO); model identification; Neural modeling; Non-linear dynamic systems; non-linearities; Non-parametric regression; Nonlinear multivariate systems; offline control; Optimal number; Projection pursuit learning; simulation results; superior performance; Two stages; Identification (control systems);In this paper we present a methodology based on a combination of many distinct predictors in an ensemble, named hybrid ensemble model, to obtain a more accurate output using the results of single predictors. As basic components, we have used Artificial Neural Networks and Support Vector Machines models. In order to evaluate the performance, the hybrid model was required to predict a 24h daily series energy consumption of a Brazilian electrical operation unit located in the northeast of Brazil. The proposed ensemble model has reached an error 25% smaller than that achieved by the best single predictor. The model was initialized several times to confirm that ensembles of predictors also tend to produce low variance profiles.  Energy utilization; Error analysis; Mathematical models; Neural networks; Support vector machines; Hybrid ensemble model; Electric load forecasting;Support Vector Clustering (SVC) is a recently proposed clustering methodology with promising performance for high-dimensional and noisy datasets, and for clusters with arbitrary shape. This work addresses the application of SVC, a kernel-based method, in a context in which the channel equalization problem is conceived as a clustering task. The main challenge, in this case, is to perform unsupervised clustering aiming at the design of an optimal Bayesian or a blind prediction-based receiver without resorting to a priori information about the transmission medium. The proposed technique employs a two-stage procedure - a combination between the use of SVC to obtain a first set of clusters and an auxiliary heuristic to help separating eventual multiple clouds contained in a single cluster and attribute centers to them via an iterated local search (ILS) algorithm. The obtained results indicate that kernel methods can be successfully applied to the field of signal processing. © 2006 IEEE. Adaptive filtering; Bayesian equalization; Digital communication; Kernel methods; Local search; Support vector clustering Adaptive filtering; Data structures; Digital communication systems; Signal receivers; Support vector machines; Bayesian equalization; Kernel methods; Local search; Clustering algorithms;Prediction models for time series generally include preprocessing followed by the synthesis of an input-output mapping. Neural network models have been adopted to perform both steps, by means of unsupervised and supervised learning, respectively. The flexibility and the generalization capability are the most relevant attributes in favor of connectionist approaches. However, even though time series prediction can be roughly interpreted as learning from data, high levels of performance will solely be achieved if some peculiarities of each time series are properly considered in the design, particularly the existence of trend and seasonality. Instead of directly adopting detrend and/or deseasonality treatments, this paper proposes a novel paradigm for supervised learning based on a mixture of heterogeneous experts. Some mixture models have already been proved to produce good performance as predictors, but the present approach will be devoted to a hybrid mixture composed of a set of distinct experts. The purpose is not only to further explore the ""divide-and-conquer"" principle, but also to compare the performance of mixture of heterogeneous experts with the standard mixture of experts approach, using ten distinct time series. The obtained results indicate that mixture of heterogeneous experts generally requires a more elaborate gating device and performs better in the case of more challenging time series. © 2005 IEEE.  Data acquisition; Expert systems; Input output programs; Learning systems; Mapping; Mathematical models; Neural networks; Performance; Gating device; Input-output mapping; Mixture of heterogeneous experts; Prediction models; Time series analysis;Adaptive beamforming in antenna arrays aims at adjusting the weighted linear combination of the output signals provided by the antennas so that the power of the received signals at dominant paths is maximized at the same time that the power of interference and noise signals is minimized. The weight vectors, each one associated with one received signal, can be directly obtained if the direction of arrival (DOA) of the corresponding signal has already been estimated. The process of DOA estimation involves the prediction of the angle of arrival by means of monitoring the output produced by the antennas in the array, given that the number of antennas is higher than the number of signals to be detected. Even though signal subspace techniques have made a good job in DOA estimation, they present some important drawbacks that will be alleviated here using a supervised learning approach, in the form of a multiclass LS-SVM classification problem. The main contribution of this paper is twofold: a step-by-step description of the complete set of algebraic manipulation for data preprocessing and for the synthesis of the classification device, and an analysis of the effect in performance when relevant parameters vary in a given operational interval. © 2005 IEEE. Antenna arrays; Direction of arrival estimation; LS-SVM; Multi-class classification; Parameter sensitivity Classification (of information); Directional patterns (antenna); Learning systems; Sensitivity analysis; Signal receivers; Algebraic manipulation; Direction of arrival (DOA); Signal subspace techniques; Support vector machines; Antenna arrays;In this paper, a new constructive approach for the automatic definition of feedforward neural networks (FNNs) is introduced. Such approach (named MASCoNN) is multiagent-oriented and, thus, can be regarded as a kind of hybrid (synergetic) system. MASCoNN centers upon the employment of a two-level hierarchy of agent-based elements for the progressive allocation of neuronal building blocks. By this means, an FNN can be considered as an architectural organization of reactive neural agents, orchestrated by deliberative coordination entities via synaptic interactions. MASCoNN was successfully applied to implement nonlinear dynamic systems identification devices and some comparative results, involving alternative proposals, are analyzed here. © Springer-Verlag Berlin Heidelberg 2003.  Construction; Dynamical systems; Multi agent systems; Nonlinear dynamical systems; Agent based; Architectural organization; Building blockes; Constructive approach; Non-linear dynamic systems; Synaptic interactions; Feedforward neural networks;In this paper, we introduce a genetic algorithm-based training mechanism (HGT-GAME) toward the automatic structural design and parameter configuration of Gated Mixtures of Experts (ME). In HGT-GAME, a whole ME instance is codified into a given chromosome. By employing regulatory genes, our approach enables the automatic pruning and growing of experts in a way to properly match the complexity of the task at hand. Moreover, to leverage HGT-GAME's effectiveness, a local search refinement upon each ME chromosome is performed in each generation via the gradient descent learning algorithm. Forecasting experiments evaluate the performance of Gated MEs trained with HGT-GAME. Genetic algorithms; Hybrid training; Mixture of experts; Time series forecasting Automation; Learning algorithms; Mathematical models; Neural networks; Optimization; Structural design; Hybrid training; Nonlinear time series forecasting; Genetic algorithms;Several support vector machine (SVM) instances with distinct kernel functions may be separately created and properly combined into the same learning machine structure. This is the idea underlying heterogeneous ensembles of SVMs (HE-SVMs), an approach conceived to alleviate the performance bottlenecks incurred with the kernel function choice problem inherent in SVM design. In this paper, we assess the effectiveness of applying an evolutionary based mechanism (GASe1) in the search of the optimal subset of SVM models for automatic HE-SVM construction. GASe1 has the advantage of merging both the selection and combination of component SVMs into the same optimization process, and has shown sound performance when compared with two other component selection methods in complicated classification problems. © 2003 IEEE.  Computer science; Evolutionary algorithms; Choice problems; Component selection; Heterogeneous ensembles; Kernel function; Learning machines; Optimal subsets; Performance bottlenecks; SVM model; Support vector machines;Neurofuzzy networks come to be a powerful alternative strategy to develop fuzzy systems, since they are capable of learning and providing IF-THEN fuzzy rules in linguistic or explicit form. Amongst such models, ANFIS has been recognized as a reference framework, mainly for its flexible and adaptive character. In this paper, we extend ANFIS theory by experimenting with a multi-net approach wherein two or more differently structured ANFIS instances are coupled to play together. Ensembles of ANFIS (E-ANFIS) enhance ANFIS performance skills as well as alleviate some of its computational bottlenecks. Moreover, it promotes the automatic configuration of different ANFIS units and the a posteriori selective combination of their outputs. Experiments conducted to assess E-ANFIS generalization capability are also presented.  Adaptive network based fuzzy inference system; Fuzzy rules; Fuzzy system design; Neurofuzzy networks; Computational linguistics; Fuzzy sets; Learning systems; Mathematical models; Neural networks; Knowledge based systems;A mixture of experts (ME) model provides a modular approach wherein component neural networks are made specialists on subparts of a problem. In this framework, that follows the ""divide-and-conquer"" philosophy, a gating network learns how to softly partition the input space into regions to be each properly modeled by one or more expert networks. In this paper, we investigate the application of different ME variants to some multivariate nonlinear dynamic systems identification problems which are known to be difficult to be dealt with. The aim is to provide a comparative performance analysis between variable settings of the standard, gated, and localized ME models with more conventional NN models. © 2002 IEEE. Automation; Computer industry; Computer networks; Neural networks; Performance analysis; Predictive models; Probability distribution; Space charge; System identification; Transfer functions Automation; Computer networks; Electric space charge; Expert systems; Identification (control systems); Mixtures; Neural networks; Nonlinear dynamical systems; Probability distributions; Transfer functions; Comparative performance analysis; Comparative studies; Component neural networks; Computer industry; Mixture of experts; Non-linear dynamic systems; Performance analysis; Predictive models; Dynamical systems;Support vector machines (SVMs) tackle classification and regression problems by non-linearly mapping input data into high-dimensional feature spaces, wherein a linear decision surface is designed. Even though the high potential of these techniques has been demonstrated, their applicability has been swamped by the necessity of the a priori choice of the kernel function to realize the non-linear mapping, which, sometimes, turns to be a complex and non-effective process. In this paper, we advocate that the application of neural ensembles theory to SVMs should alleviate such performance bottlenecks, because different networks with distinct kernel functions such as polynomials or radial basis functions may be created and properly combined into the same neural structure. Ensembles of SVMs, thus, promote the automatic configuration and tuning of SVMs, and have their generalization capability assessed here by means of some function regression experiments.  Approximation theory; Conformal mapping; Functions; Polynomials; Quadratic programming; Radial basis function networks; Regression analysis; Mean squared error; Neural ensembles theory; Optimal linear combinations; Support vector machines; Learning systems"
Ivandré Paraboni,"Universidade de São Paulo - Escola de Artes, Ciências e Humanidades","Studies in referring expression generation (REG) have shown different effects of referential overspecification on the resolution of certain descriptions. To further investigate effects of this kind, this article reports two eye-tracking experiments that measure the time required to recognize target objects based on different kinds of information. Results suggest that referential overspecification may be either helpful or detrimental to identification depending on the kind of information that is actually overspecified, an insight that may be useful for the design of more informed hearer-oriented REG algorithms. © 2017 Association for Computational Linguistics.  ;This paper presents the Stars2 corpus of definite descriptions for referring expression generation (REG). The corpus was produced in collaborative communication involving speaker-hearer pairs, and includes situations of reference that are arguably under-represented in similar work. Stars2 is intended as an incremental contribution to the research in REG and related fields, and it may be used both as training/test data for algorithms of this kind, and also to gain further insights into reference phenomena in general, with a particular focus on the issue of attribute choice in referential overspecification. © 2016, Springer Science+Business Media Dordrecht. Content selection; Corpora; Natural language generation; Referring expressions ;This paper discusses the issue of human variation in natural language referring expression generation. We introduce a model of content selection that takes speaker-dependent information into account to produce descriptions that closely resemble those produced by each individual, as seen in a number of reference corpora. Results show that our speaker-dependent referring expression generation model outperforms alternatives that do not take human variation into account, or which do so less extensively, and suggest that the use of machine-learning methods may be an ideal approach to mimic complex referential behaviour. Copyright © Cambridge University Press 2017  Artificial intelligence; Software engineering; Machine learning methods; Natural languages; Referring expressions; Speaker dependents; Learning systems;This paper discusses the computational problem of generating referring expressions (REG) in 3D virtual worlds. We propose a REG algorithm that attempts to make adequate choices of spatial relations for the purpose of disambiguation (as opposed to, e.g., determining the localisation of a previously identified object). The decisions made by the algorithm are based on existing computational models of spatial reference, and further refined by the use of domain knowledge obtained from a corpus of instructions in virtual environments. The proposed approach is shown to outperform a number of baseline systems, and provides evidence on how a standard REG approach may be applied to the generation of these descriptions in 3D virtual worlds. © 2015, Copyright © Taylor & Francis Group, LLC. natural language generation; referring expressions Natural language processing systems; 3d virtual worlds; Computational model; Computational problem; Generating referring expressions; Natural language generation; Referring expressions; Spatial reference; Spatial relations; Virtual reality;This paper describes an experiment to elicit referring expressions from human subjects for research in natural language generation and related fields, and prelim-inary results of a computational model for the generation of these expressions. Un-like existing resources of this kind, the re-sulting data set-the Zoom corpus of natu-ral language descriptions of map locations-takes into account a domain that is sig-nificantly closer to real-world applications than what has been considered in previous work, and addresses more complex situa-tions of reference, including contexts with different levels of detail, and instances of singular and plural reference produced by speakers of Spanish and Portuguese. © 2015 Association for Computational Linguistics.  Linguistics; Natural language processing systems; Computational linguistics; Computational model; Human subjects; Language description; Levels of detail; Map location; Natural language generation; Natural languages; Referring expressions; Real-world; Computational linguistics; Natural language processing systems;We present an experiment to compare a standard, minimally distinguishing algo-rithm for the generation of relational refer-ring expressions with two alternatives that produce overspecified descriptions. The experiment shows that discrimination-which normally plays a major role in the disambiguation task-is also a major influ-ence in referential overspecification, even though disambiguation is in principle not relevant. © 2015 Association for Computational Linguistics.  Computational linguistics; Linguistics; Overspecification; Referring expressions; Natural language processing systems;This paper presents a study in the field of Natural Language Generation (NLG), focusing on the computational task of referring expression generation (REG). We describe a standard REG implementation based on the well-known Dale & Reiter Incremental algorithm, and a classification-based approach that combines the output of several support vector machines (SVMs) to generate definite descriptions from two publicly available corpora. Preliminary results suggest that the SVM approach generally outperforms incremental generation, which paves the way to further research on machine learning methods applied to the task. © 2014 Springer-Verlag Berlin Heidelberg. Classification; Natural Language Generation; Referring Expressions; SVM Classification (of information); Computational linguistics; Text processing; Computational task; Definite descriptions; Incremental algorithm; Natural language generation; On-machines; Referring expressions; Support vector machine (SVMs); SVM; Support vector machines;We describe a classification-based approach to referring expression generation (REG) making use of standard context-related features, and an extension that adds speaker-related features. Results show that taking speakers' preferences into account outperforms the standard REG model in four test corpora of definite descriptions. © 2014 Springer International Publishing. Natural Language Generation; Referring Expressions Computer science; Computers; Definite descriptions; Natural language generation; Referring expressions; Test corpus; Artificial intelligence;This paper presents a simple experiment to compare content recommendation on Twitter with and without the use of temporal information. Preliminary results suggest that the use of a particular kind of temporal information extracted from corpora (namely, the time frame within which a user browses the microblog) may lead to more accurate content recommendation. © Springer International Publishing Switzerland 2014. Content recommendation; Microblogs; Temporal aspects Content recommendations; Micro-blog; Microblogs; Temporal aspects;We present an experiment to collect referring expressions produced by human speakers under conditions that favour landmark underspecification. The experiment shows that underspecified landmark descriptions are not only common but, under certain conditions, may be largely preferred over minimally and fully-specified descriptions alike. © 2014 Springer International Publishing. Natural Language Generation; Referring Expressions Artificial intelligence; Computer science; Computers; Natural language generation; Referring expressions; Underspecification; Experiments;This paper introduces Lausanne - a tool for collaborative online NLP experiments. Lausanne has been successfully applied to the implementation of a practical experiment to collect human-produced natural language descriptions, and it is freely available for research purposes. © Springer International Publishing Switzerland 2014. Data collection; Experimental NLP Data collection; Experimental NLP;This paper discusses the generation of relational referring expressions in which target and landmark descriptions are allowed to help disambiguate each other. Using a corpus of referring expressions in a simple visual domain - in which these descriptions are likely to occur - we propose a classification approach to decide when to generate them. The classifier is then embedded in a REG algorithm whose results outperform a number of naive baseline systems, suggesting that mutual disambiguation is fairly common in language use, and that this may not be entirely accounted for by existing REG algorithms. © 2014 Springer-Verlag Berlin Heidelberg. Natural Language Generation; Relational Referring Expressions; Underspecification Computational linguistics; Embedded systems; Text processing; Baseline systems; Classification approach; Mutual disambiguation; Natural language generation; Referring expressions; Underspecification; Algorithms;[No abstract available]  ;Earlier work has suggested that, in hierarchically ordered domains (e.g., a document divided into sections and subsections), referring expressions that are judiciously over-specified to a higher extent than is achieved by existing generation algorithms can make it considerably easier for a hearer to find the referent of the referring expression. The present paper investigates over-specification in spatial domains, which plays an important role in daily life. We report an experiment whose aim is (1) to find out whether over-specification plays a similar role in spatial domains as in hierarchically ordered domains, (2) to obtain a better understanding of the reasons why over-specification can be helpful to hearers, and (3) to propose an algorithmic model of reference production that takes these findings into account. The results suggest that judicious over-specification can facilitate search in a precisely defined class of problematic conditions (but less so in other cases) even if the hearer has previous knowledge about the domain. The implications of these findings are discussed and an algorithm for the generation of referring expressions is proposed that reflects them as closely as possible. © 2013 Copyright Taylor and Francis Group, LLC. natural language generation; over-specification; referring expressions; search effort ;As in many other natural language processing (NLP) fields, the use of statistical methods is now part of mainstream natural language generation (NLG). In the development of systems of this kind, however, there is the issue of data sparseness, a problem that is particularly evident in the case of morphologically-rich languages such as Portuguese. This work presents a shallow surface realisation system that makes use of factored language models (FLMs) of Portuguese to overcome some of these difficulties. The system combines FLMs trained on a large corpus with a number of NLP resources that have been made publicly available by the Brazilian NLP research community in recent years, such as corpora, dictionaries, thesauri and others. Our FLM-based approach to surface realisation has been successfully applied to the generation of Brazilian newspapers headlines, and the results are shown to outperform a number of statistical and non-statistical baseline systems alike. © 2012 The Brazilian Computer Society. Natural language generation; Surface realisation; Text generation Baseline systems; Data sparseness; Language model; Large corpora; Natural language generation; NAtural language processing; Research communities; Text generations; Computational linguistics; Natural language processing systems;Earlier work has suggested that, in hierarchically ordered domains (e.g., a document divided into sections and subsections), referring expressions that are judiciously over-specified to a higher extent than is achieved by existing generation algorithms can make it considerably easier for a hearer to find the referent of the referring expression. The present paper investigates over-specification in spatial domains, which plays an important role in daily life. We report an experiment whose aim is (1) to find out whether over-specification plays a similar role in spatial domains as in hierarchically ordered domains, (2) to obtain a better understanding of the reasons why over-specification can be helpful to hearers, and (3) to propose an algorithmic model of reference production that takes these findings into account. The results suggest that judicious over-specification can facilitate search in a precisely defined class of problematic conditions (but less so in other cases) even if the hearer has previous knowledge about the domain. The implications of these findings are discussed and an algorithm for the generation of referring expressions is proposed that reflects them as closely as possible. © 2013 Taylor & Francis Natural language generation; Over-specification; Referring expressions; Search effort algorithm; Article; conversation; information processing; information retrieval; language; language processing; linguistics; mathematical model; reference memory; semantics; speech; virtual reality;It is often desirable that referring expressions be chosen in such a way that their referents are easy to identify. In this paper, we investigate to what extent identification becomes easier by the addition of logically redundant properties.We focus on hierarchically structured domains, whose content is not fully known to the reader when the referring expression is uttered. © 2006 Association for Computational Linguistics.  Hierarchical domains;Statistical language models based on n-gram counts have been shown to successfully replace grammar rules in standard 2-stage (or 'generate-and- select') Natural Language Generation (NLG). In highly-inflected languages, however, the amount of training data required to cope with n-gram sparseness may be simply unobtainable, and the benefits of a statistical approach become less obvious. In this work we address the issue of text generation in a highly-inflected language by making use of factored language models (FLM) that take morphological information into account. We present a number of experiments involving the use of simple FLMs applied to various surface realisation tasks, showing that FLMs may implement 2-stage generation with results that are far superior to standard n-gram models alone. © 2011 Springer-Verlag. Language Modelling; Surface Realisation; Text Generation Grammar rules; Language generation; Language model; Language modelling; Morphological information; N-gram models; Natural language generation; Statistical approach; Statistical language models; Text Generation; Text generations; Training data; Natural language processing systems; Text processing; Word processing; Computational linguistics;In Natural Language Generation (NLG) systems, a general-purpose surface realisation module will usually require the underlying application to provide highly detailed input knowledge about the target sentence. As an attempt to reduce some of this complexity, in this paper we follow a traditional approach to NLG and present a number of experiments involving the use of n-gram language models as an aid to an otherwise rule-based text generation approach. By freeing the application from the burden of providing a linguistically- rich input specification, and also by taking some of the generation decisions away from the surface realisation module, we expect to make NLG techniques accessible to a wider range of potential applications. © 2010 Springer-Verlag. Language Modelling; Surface Realisation; Text Generation Language modelling; N-gram language models; N-gram statistics; Natural language generation systems; Potential applications; Rule based; Text generations; Computational linguistics; Natural language processing systems; Artificial intelligence;Surface realisation - the task of producing word strings from non-linguistic input data - has been the focus of a great deal of research in the field of data-to-text Natural Language Generation (NLG). In this work we discuss an alternative approach to surface realisation, in which we borrow NLG techniques from the sister field of text-to-text generation to implement text generation based on examples in natural language. Our approach is suitable to simpler applications that are not linguistically-oriented by design, and which may be able to provide only minimal input knowledge to the NLG module. © 2010 Springer-Verlag. Natural Language Generation; Surface Realisation Alternative approach; Input datas; Natural language generation; Natural languages; Text generations; Artificial intelligence; Linguistics;In previous work we have shown results of a first experiment in Statistical Machine Translation (SMT) for Brazilian Portuguese and American English using state-of-the-art phrase-based models. In this paper we compare a number of training and decoding parameter choices for fine-tuning the system as an attempt to obtain optimal results for this language pair. © 2009 IEEE.  American English; Language pairs; Optimal results; Parameter choice; Statistical machine translation; Information theory; Linguistics; Speech transmission; Tuning; Translation (languages);We present a data-text aligned corpus for Brazilian Portuguese Natural Language Generation (NLG) called SINotas, which we believe to be the first of its kind. SINotas provides a testbed for research on various aspects of trainable, corpus-based NLG, and it is the basis of a simple NLG application under development in the education domain. © 2009 IEEE.  Education domain; Natural language generation; Test facilities; Testbeds; Linguistics;At both semantic and syntactic levels, the generation of referring expressions (REG) involves far more than simply producing 'correct' output strings and, accordingly, remains central to the study and development of Natural Language Generation (NLG) systems. In particular, REG algorithms have to pay regard to humanlikeness, an issue that lies at the very heart of the classic definition of Artificial Intelligence as, e.g., motivated by the Turing test. In this work we present an end-to-end approach to REG that takes humanlikeness into account, addressing both the issues of semantic content determination and surface realisation as natural language. © AEPIA. Attribute selection; Natural language generation; Referring expressions; Surface realisation Attribute selection; Natural language generation; Natural language generation systems; Natural languages; Object description; Semantic content; Semantic properties; Surface text; Turing tests; Artificial intelligence; Semantics; Linguistics;We present a pilot experiment to measure the effects of redundancy in the resolution of definite descriptions as performed by a small number of human readers. Although originally intended to provide evidence of how much redundancy should ideally be included in generated anaphoric descriptions, preliminary findings reveal a number of little explored issues that are relevant to both referring expressions generation and interpretation. Anaphora resolution; Referring expressions generation Anaphora resolution; Definite descriptions; Human readers; Pilot experiment; Referring expressions generation; Natural language processing systems; Redundancy; Experiments;We present a follow-up of our previous frequency- based greedy attribute selection strategy. The current version takes into account also the instructions given to the participants of TUNA trials regarding the use of location information, showing an overall improvement on string-edit distance values driven by the results on the Furniture domain. © 2009 Association for Computational Linguistics.  Attribute selection; Attribute-selection strategy; Distance values; Location information;Document Planning - the task of deciding which content messages should be realised in a target document based on raw data provided by an underlying application, and how these messages should be structured - is arguably one of the most crucial tasks in Natural Language Generation (NLG). In this work we present a machine learning approach to Document Planning that is entirely trainable from annotated corpora, and which paves the way to our long-term goal of developing a text generator system based on a series of classifiers for a simple NLG application in the education domain. Content selection; Document Planning Content messages; Content selection; Education domain; Generator systems; Learning approach; Long-term goals; Natural language generation; Digital storage; Learning algorithms; Natural language processing systems; Information retrieval systems;Statistical approaches to machine translation have long been successfully applied to a number of 'distant' language pairs such as English-Arabic and English-Chinese. In this work we describe an experiment in statistical machine translation between two 'related' languages: European Spanish and Brazilian Portuguese. Preliminary results suggest not only that statistical approaches are comparable to a rule-based system, but also that they are more adaptive and take considerably less effort to be developed. © 2008 Springer Berlin Heidelberg.  Artificial intelligence; Bionics; Computer aided language translation; Information theory; Query languages; Speech transmission; Translation (languages); Europeans; Language pairs; Machine translations; Rule-based systems; Spanishs; Statistical approaches; Statistical machine translations; Linguistics;Both greedy and domain-oriented REG algorithms have significant strengths but tend to perform poorly according to humanlikeness criteria as measured by, e.g., Dice scores. In this work we describe an attempt to combine both perspectives into a single attribute selection strategy to be used as part of the Dale & Reiter Incremental algorithm in the REG Challenge 2008, and the results in both Furniture and People domains.  Attribute selection; Attribute-selection strategy; Incremental algorithm; Algorithms;Anaphora resolution is an essential component of most NLP applications, from text understanding to Machine Translation. In this work we discuss a supervised machine learning approach to the problem, focusing on instances of anaphora ubiquitously found in a corpus of Brazilian Portuguese texts, namely, third-person pronominal references. Although still limited to a subset of the more general co-reference resolution problem, our present results are comparable to existing work in the field in both English and Portuguese languages, representing the highest accuracy rates that we are aware of in (Brazilian) Portuguese pronoun resolution. © 2008 Springer-Verlag.  Accuracy rate; Anaphora resolution; Essential component; Machine translations; Machine-learning; Portuguese languages; Pronoun resolution; Reference resolution; Supervised machine learning; Computer aided language translation; Information theory; Robot learning; Speech transmission; Artificial intelligence;We investigate a machine learning approach to Portuguese pronoun resolution. We presently focus on so-called 'low-cost' learning features readily obtainable from the output of a part-of-speech tagger, and we largely bypass deep syntactic and semantic analysis. Preliminary results show significant improvement in resolution precision and recall, and are comparable to existing rule-based approaches for the Portuguese language spoken in Brazil. Anaphora resolution; Machine learning Anaphora resolution; Learning approach; Part-of-speech tagger; Portuguese languages; Precision and recall; Pronoun resolution; Rule-based approach; Semantic analysis; Computational grammars; Learning systems; Semantics; Cost benefit analysis;The semantic content determination or attribute selection of definite descriptions is one of the most traditional tasks in natural language generation. Algorithms of this kind are required to produce descriptions that are brief (or even minimal) and, at the same time, as close as possible to the choices made by human speakers. In this work we attempt to achieve a balance between brevity and humanlikeness by implementing a number of algorithms for the task. The algorithms are tested against descriptions produced by humans in two different domains, suggesting a strategy that is both computationally simple and comparable to the state of the art in the field. © 2008 Springer-Verlag.  Attribute selection; Definite descriptions; Different domains; Natural language generation; Semantic content; State of the art; Artificial intelligence;This document describes the development of a surface realisation component for the Portuguese language that takes advantage of the data and evaluation tools provided by the REG-2008 team. At this initial stage, our work uses simple n-gram statistics to produce descriptions in the Furniture domain, with little or no linguistic variation. Preliminary results suggest that, unlike the generation of English descriptions, contextual information may be required to account for Portuguese word order.  Contextual information; Evaluation tool; Initial stages; N-gram statistics; Portuguese languages; Word orders;Natural Language Generation systems usually require substantial knowledge about the structure of the target language in order to perform the final task in the generation process - the mapping from semantic representation to text known as surface realisation. Designing knowledge bases of this kind, typically represented as sets of grammar rules, may however become a costly, labour-intensive enterprise. In this work we take a statistical approach to surface realisation in which no linguistic knowledge is hard-coded, but rather trained automatically from large corpora. Results of a small experiment in the generation of referring expressions show significant levels of similarity between our (computer-generated) text and those produced by humans, besides the usual benefits commonly associated with statistical NLP such as low development costs, domain- and language-independency. © 2008 Springer-Verlag Berlin Heidelberg.  Artificial intelligence; Computational linguistics; Costs; Information retrieval systems; Information theory; Knowledge representation; Linguistics; Natural language processing systems; Statistical methods; Development costs; Generation process; Grammar rules; International conferences; Knowledge bases; Labour-intensive; Linguistic knowledge; Natural language generation systems; NAtural language processing; Semantic representations; Statistical approaches; Target languages; Finance;Despite being one of the most widely-spoken languages in the world, Portuguese remains a relatively resource-poor language, for which only in recently years NLP tools such as parsers, taggers and (fairly) large corpora have become available. In this work we describe the task of pronominal co-reference annotation and resolution in Portuguese texts, in which we take advantage of information provided by a tagged corpus and a simple annotation tool that has been developed for this purpose. Besides developing some of these basic resources from scratch, our ultimate goal is to investigate the multilingual resolution of Portuguese personal pronouns to improve the accuracy of their translations to both Spanish and English in an underlying MT project. © 2008 Springer-Verlag Berlin Heidelberg.  Computational linguistics; Laws and legislation; Query languages; Software agents; Text processing; Word processing; Intelligent text processing; International conferences; Pronoun resolution; Spanish; Spoken languages; Linguistics;It is often desirable that referring expressions be chosen in such a way that their referents are easy to identify. This article focuses on referring expressions in hierarchically structured domains, exploring the hypothesis that referring expressions can be improved by including logically redundant information in them if this leads to a significant reduction in the amount of search that is needed to identify the referent. Generation algorithms are presented that implement this idea by including logically redundant information into the generated expression, in certain well-circumscribed situations. To test our hypotheses, and to assess the performance of our algorithms, two controlled experiments with human subjects were conducted. The first experiment confirms that human judges have a preference for logically redundant expressions in the cases where our model predicts this to be the case. The second experiment suggests that readers benefit from the kind of logical redundancy that our algorithms produce, as measured in terms of the effort needed to identify the referent of the expression. © 2007 Association for Computational Linguistics.  ;Documents in a wide range of genres often contain references to their own sections, pictures etc. We call such referring expressions instances of Document Deixis. The present work focuses on the generation of Document Deixis in the context of a particular kind of natural language generation system in which these descriptions are not specified as part of the input, i.e., when it is up to the system to decide whether a reference is called for and, if so, which document entity it should refer to. We ask under what circumstances it is advantageous to describe domain objects in terms of the document parts where they are mentioned (as in ""the insulin described in section 2""). We report on an experiment suggesting that such indirect descriptions are preferred by human readers whenever they cause the generated descriptions to be shorter than they would otherwise be. © Springer-Verlag Berlin Heidelberg 2006.  Data reduction; Linguistics; Natural language processing systems; Text processing; Document entity; Domain objects; Natural language generation system; Information retrieval systems"
Sarajane Marques Peres,"Universidade de São Paulo - Escola de Artes, Ciências e Humanidades","This paper presents an overview of studies on automated hand gesture analysis, which is mainly concerned with recognition and segmentation issues related to functional types and gesture phases. The issues selected for discussion have been arranged in a way that takes account of problems within the Theory of Gestures that each study seeks to address. Their principal computational factors that were involved in conducting the analysis of automated hand gesture have been examined, and an analysis of open research issues has been carried out for each application dealt with in the studies. © 2016, Springer Science+Business Media Dordrecht. Automated gesture analysis; Gesture recognition; Gesture segmentation; Gesture studies; Gestures phases; Type of gestures ;Facial Expression Recognition is an already well-developed research area, mainly due to its applicability in the construction of different system types. Facial expressions are especially important in the area which relates to the construction of discourses through sign language. Sign languages are visual-spatial languages that are not assisted by voice intonation. Therefore, they use facial expressions to support the manifestation of prosody aspects and some grammatical constructions. Such expressions are called Grammatical Facial Expressions (GFEs) and they are present at sign language morphological and syntactic levels. GFEs stand out in automated recognition processes for sign languages, as they help removing ambiguity among signals, and they also contribute to compose the semantic meaning of discourse. This paper aims to present a study which applies inductive reasoning to recognize patterns, as a way to study the problem involving the automated recognition of GFEs at the discourse syntactic level in the Libras Sign Language (Brazilian Sign Language). In this study, sensor Microsoft Kinect was used to capture three-dimensional points in the faces of subjects who were fluent in sign language, generating a corpus of Libras phrases, which comprised different syntactic constructions. This corpus was analyzed through classifiers that were implemented through neural network Multilayer Perceptron, and then a series of experiments was conducted. The experiments allowed investigating: the recognition complexity that is inherent to each of the GFEs that are present in the corpus; the use suitability of different vector representations, considering descriptive characteristics that are based on coordinates of points in three dimensions, distances and angles therefrom; the need for using time data regarding the execution of expressions during speech; and particularities that are connected to data labeling and the evaluation of classifying models in the context of a sign language. © 2017 Springer Science+Business Media New York Grammatical facial expression; Machine learning; Pattern recognition; Sign language Learning systems; Pattern recognition; Semantics; Speech recognition; Syntactics; Visual languages; Automated recognition; Facial expression recognition; Facial Expressions; Grammatical construction; Inductive reasoning; Microsoft kinect; Sign language; Vector representations; Face recognition;An interaction between humans or between a human and a machine will be more effective if it is supported by gestures. In different levels of complexity, the communication system used in human interaction includes the use of gesture. In natural conversation, for instance, speakers use gestures for both to enhance the expressiveness of their speech and to support their own linguistic reasoning. The audience absorbs the content being transmitted also based on the speakers' gesticulation. Thus, an analysis of gestures should add value to the purpose of the interaction. One of the concerns in the analysis of gestures is the problem arising from the segmentation of phases of a gesture (rest position, preparation, stroke, hold and retraction), which, from the standpoint of Gesture Theory, may reveal information on prosody and semantics of what is being said in a discourse. Finding an automation solution to this problem involves enabling the development of theoretical and application areas that are based on the analysis of human behavior and on the interpretation and generation of natural language. In this study, the problem of gesture phase segmentation is modeled as a problem of classification, and then support vector machine is employed to design a model able to learn the patterns of gesture that are inherent to each phase. This work presents two main highlights. The first is to address the limitations of the segmentation approach through the study of its performance in different scenarios that represent the complexity of analyzing patterns of human behavior. In this study, we reached an F-score around 0.9 for rest position and around 0.8 for stroke and preparation as segmentation results in the best cases. Moreover, it was possible to investigate how classification models are influenced by human behavior. The second highlight refers to the conduction of an analysis by considering the standpoint of specialists concerned with gesture phase segmentation in the area of Linguistics and Psycholinguistics, through which we obtained impressive results. Thus, in regard to the suitability of our approach, it is a feasible means of supporting the development of the Gesture Theory as well as the Computational Linguistics and Human Machine Interaction fields. © 2016 Elsevier Ltd. All rights reserved. Gesture analysis; Gesture phase segmentation; Machine learning; Support vector machine Artificial intelligence; Behavioral research; Classification (of information); Computational linguistics; Learning systems; Linguistics; Social sciences; Support vector machines; Automation solutions; Classification models; Gesture analysis; Human interactions; Human machine interaction; Linguistic reasonings; Natural languages; Segmentation results; Semantics;Surface electromyography (EMG) signals have been studied extensively in the last years aiming at the automatic classification of hand gestures and movements as well as the early identification of latent neuromuscular disorders. In this paper, we investigate the potentials of the conjoint use of relevance vector machines (RVM) and fractal dimension (FD) for automatically identifying EMG signals related to different classes of limb motion. The adoption of FD as the mechanism for feature extraction is justified by the fact that EMG signals usually show traces of self-similarity. In particular, four well-known FD estimation methods, namely box-counting, Higuchi’s, Katz’s and Sevcik’s methods, have been considered in this study. With respect to RVM, besides the standard formulation for binary classification, we also investigate the performance of two recently proposed variants, namely constructive mRVM and top-down mRVM, that deal specifically with multiclass problems. These classifiers operate solely over the features extracted by the FD estimation methods, and since the number of such features is relatively small, the efficiency of the classifier induction process is ensured. Results of experiments conducted on a publicly available dataset involving seven distinct types of limb motions are reported whereby we assess the performance of different configurations of the proposed RVM+FD approach. Overall, the results evidence that kernel machines equipped with the FD feature values can be useful for achieving good levels of classification performance. In particular, we have empirically observed that the features extracted by the Katz’s method is of better quality than the features generated by other methods. © 2015, The Natural Computing Applications Forum. EMG signal classification; Feature extraction; Fractal dimension; Relevance vector machines Classification (of information); Electromyography; Extraction; Feature extraction; Finite difference method; Fractal dimension; Fractals; Automatic classification; Binary classification; Classification performance; Electromyography signals; Emg signal classifications; Neuromuscular disorders; Relevance Vector Machine; Surface electromyography signals; Biomedical signal processing;The increase in the international consumer market in recent years allowed the opening of new stores, increasing price competition and diversifying the product offering. In this context, could be profitable knowing the best combination of price, quality and services, considering the cost of transport between stores. Such a problem assumes a high complexity when more products and stores were involved, and it is a classical problem in supply chain network design - one of the most important issues in modern business management. The worldwide industry has been demanded innovative solutions from its information technology staff, and this is why it is worth-while exploring new directions for finding efficient solutions. In this paper, we analyse the use of metaheuristics algorithms combined to a new genetic operator, to optimize purchases of products in stores geographically separated, searching for a purchase with the lowest possible total cost with a minimum route. Overall, the results evidence metaheuristics algorithms equipped with new genetic operator can be useful for achieving relatively good solutions in a short-time interval when compared with traditional operators. © 2015 IEEE. Crossover Operators; Genetic Algorithms; Memetic Algorithms; Operating Cost; Suply Chain Management Algorithms; Artificial intelligence; Competition; Complex networks; Costs; Genetic algorithms; Heuristic algorithms; Operating costs; Product design; Purchasing; Chain management; Crossover operator; Innovative solutions; Memetic algorithms; Meta heuristic algorithm; Meta-heuristics algorithms; Short time intervals; Supply chain network design; Supply chain management;In recent years, human identification based on face recognition has attracted the attention of the scientific community and the general public due to its wide range of applications. A face recognition system involves three important phases: face detection, feature extraction and classification (identification and/or verification). The robustness of face recognition could be improved by treating the variations in these stages. One of the main issues in design of face recognition system is how to extract discriminative facial features. A precise extraction of a representative feature set will improve the performance of a face recognition system. Various techniques have been used to represent images efficiently, of which the most well-known and widely applied are Wavelet, Contourlet, Shearlet and Curvelet Transform. Their ability to capture localized time-frequency information of image motivates their use for feature extraction. In this paper, we conduct a systematic empirical study on these transforms as feature extractors from face images. To further reduce the feature dimensionality, we adopt Principal Component Analysis and Linear Discriminant Analysis to select the most discriminative feature sets. The performance levels delivered by each transform are contrasted in terms of the accuracy measure computed over the outputs generated by the Support Vector Machine classifier (SVM). Experimental results conducted on a publicly available database are reported whereby we observe that the Curvelet Transform followed by the Wavelet Transform significantly outperform the others according to accuracy measure calculated over the SVM classifier. © 2015 IEEE. Contourlet Transform; Curvelet Transform; Face Recognition; Shearlet Transform; Support Vector Machine; Wavelet Transform Classification (of information); Discriminant analysis; Extraction; Feature extraction; Image enhancement; Image processing; Image retrieval; Principal component analysis; Support vector machines; Wavelet transforms; Contourlet transform; Curvelet transforms; Face recognition systems; Feature extraction and classification; Linear discriminant analysis; Shearlet transforms; Support vector machine classifiers; Time frequency information; Face recognition;Purpose – Process mining is a research area used to discover, monitor and improve real business processes by extracting knowledge from event logs available in process-aware information systems. The purpose of this paper is to evaluate the application of artificial neural networks (ANNs) and support vector machines (SVMs) in data mining tasks in the process mining context. The goal was to understand how these computational intelligence techniques are currently being applied in process mining. Design/methodology/approach – The authors conducted a systematic literature review with three research questions formulated to evaluate the use of ANNs and SVMs in process mining. Findings – The authors identified 11 papers as primary studies according to the criteria established in the review protocol. Most of them deal with process mining enhancement, mainly using ANNs. Regarding the data mining task, the authors identified three types of tasks used: categorical prediction (or classification); numeric prediction, considering the “regression” type, and clustering analysis. Originality/value – Although there is scientific interest in process mining, little attention has been specifically given to ANNs and SVM. This scenario does not reflect the general context of data mining, where these two techniques are widely used. This low use may be possibly due to a relative lack of knowledge about their potential for this type of problem, which the authors seek to reverse with the completion of this study. © 2015, Emerald Group Publishing Limited. Artificial neural networks; Computational intelligence; Data mining; Process mining; Support-vector machines ;The automated analysis of facial expressions has been widely used in different research areas, such as biometrics or emotional analysis. Special importance is attached to facial expressions in the area of sign language, since they help to form the grammatical structure of the language and allow for the creation of language disambiguation, and thus are called Grammatical Facial Expressions (GFEs). In this paper we outline the recognition of GFEs used in the Brazilian Sign Language. In order to reach this objective, we have captured nine types of GFEs using a Kinect™sensor, designed a spatial-temporal data representation, modeled the research question as a set of binary classification problems, and employed a Machine Learning technique. Copyright © 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.  Artificial intelligence; Biometrics; Computational linguistics; Learning systems; Automated analysis; Binary classification problems; Emotional analysis; Facial expressions recognition; Grammatical structure; Machine learning techniques; Research questions; Spatial-temporal data; Face recognition;Currently, automated gesture analysis is being widely used in different research areas, such as human-computer interaction or human-behavior analysis. With regard to the latter area in particular, gesture analysis is closely related to studies on human communication. Linguists and psycholinguists analyze gestures from several standpoints, and one of them is the analysis of gesture segments. The aim of this paper is to outline an approach to automate gesture unit segmentation, as a way of assisting linguistic studies. This objective was attained by employing a Machine Learning technique with the aid of a spatial-temporal data representation. Copyright © 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.  Artificial intelligence; Behavioral research; Learning systems; Linguistics; Gesture analysis; Human behavior analysis; Human communications; Machine learning techniques; Spatial temporals; Spatial-temporal data; Human computer interaction;Camera calibration is an attractive problem that have received relatively great attention from scientists in the last years. It has several interesting applications, particularly in sports broadcasting technology. We present in this paper a complete system that receives a sport image, automatically detects control points in the image with a RANSAC-based strategy and solves the optimization problem by using a genetic algorithm. In fact, our approach introduces an improvement in the RANSAC-based strategy which is able to refine its results, and shows that a simple genetic algorithm is capable to solve the camera calibration problem with relatively small computational effort. © 2013 IEEE.  Broadcasting technologies; Camera calibration; Complete system; Computational effort; Control point; Optimization problems; Simple genetic algorithm; Broadcasting; Calibration; Cameras; SportS; Genetic algorithms;Gesture analysis has been widely used for developing new methods of human-computer interaction. The advancement reached in the gesture analysis area is also motivating its application to automate tasks related to discourse analysis, such as the gesture phases segmentation task. In this paper, we present an initiative that aims at segmenting gestures, especially considering the ""units""- the larger grain involved in gesture phases segmentation. Thereunto, we have captured the gestures using a Xbox Kinect™ device, modeled the problem as a classification task, and applied Support Vector Machines. Moreover, aiming at taking advantage from the temporal aspects involved in the problem, we have used several types of data pre-processing in order to consider time domain and frequency domain features. Copyright 2013 ACM. Gesture analysis; Gesture segmentation; Gesture unit; Support vector machine; Temporal modeling Classification tasks; Data preprocessing; Discourse analysis; Frequency domains; Gesture analysis; Gesture segmentation; Gesture unit; Temporal modeling; Computation theory; Support vector machines;Recently, Support Vector Machines have presented promissing results to various machine learning tasks, such as classification and regression. These good results have motivated its application to several complex problems, including temporal information analysis. In this context, some studies attempt to extract temporal features from data and submit these features in a vector representation to traditional Support Vector Machines. However, Support Vector Machines and its traditional variations do not consider temporal dependency among data. Thus, some approaches adapt Support Vector Machines internal mechanism in order to integrate some processing of temporal characteristics, attempting to make them able to interpret the temporal information inherent on data. This paper presents a review on studies covering this last approach for dealing with temporal information: incorporating temporal reasoning into Support Vector Machines and its variations. © 2012 IEEE. Machine learning; Support vector machine; Temporal reasoning Complex problems; Temporal characteristics; Temporal features; Temporal information; Temporal reasoning; Vector representations; Learning systems; Support vector machines;This paper describes a hybrid architecture that provides automatic classification for a set of gestures. Such architecture combines fuzzy-connectionist, heuristic and syntactical pattern recognition approaches, and deals with gesture recognition based on primitives. The modeling with primitives allows the use of multiples classifiers in order to achieve high classification accuracy. The heuristic classifier and the fuzzy syntactical integrating strategy are described in this paper. The fuzzy-connectionist classifiers were discussed in previous works and they are now revisited just to present the set of parameters that solves the current proof of concept, in the scope of Brazilian Sign Language Manual Alphabet. The fuzzy syntactical strategy coupled with the modeling with primitives has improved the pattern recognition results, enabling the design of architecture for classification with high flexibility and scalability to development of applications in different signed communication contexts. The experimental results show that the proposed approach is valid and has promising application. © 2012 IEEE. Fuzzy Syntactical Pattern Recognition; Fuzzy-Connectionist Pattern Recognition; Gesture Recognition; Hybrid Architecture; Multiple Classifiers Automatic classification; Classification accuracy; High flexibility; Hybrid architectures; Multiple classifiers; Proof of concept; Sign language; Syntactical pattern recognition; Neural networks; Pattern recognition; Gesture recognition;Contests are usually applied in the academic environment to simulate real professional situations that require from the participants a more pro-active attitude than the one shown in conventional coursework. Although they are commonly applied in the scope of a unique course, the contest described here was an extracurricular experience applied in an Information System undergraduate program. The evaluation of the contest is also presented; the objective was to assess the role of the contest as a tool to bring together interdisciplinary subjects, complementary to the traditional disciplinary structure of the program curriculum. The results indicate that a significant portion of the participants noticed increase in their knowledge after the contest, which is verified by statistical tests. However, students from the first stages received more benefits, probably because such students were more motivated and had more available time to be involved in the contest activities. © 2011 Vilnius University. Contest; Education in information systems; Education in software engineering; Problembased learning ;Nowadays, efforts in computer game development have been concerned to overcome entertainment objectives. In fact, there has been much effort aiming at finding, in Computer Science, resources to improve games in order to allow their application into education, business or politics processes. The effective introduction of these products in society requires that they are designed as accessible as possible to all individuals, including those ones belonging to minorities with special needs. In order to reach accessibility requirements, it is desirable to attend adaptability and usability requirements to provide products with higher quality and acceptance rate. In this chapter, we discuss the potential of combining the Human-Computer Interaction and the Artificial Intelligence areas aiming at promoting accessibility in games and, as a result, making them more democratic and useful for society, particularly for people who depend on the assistive technology resources. © 2011, IGI Global.  ;The Brazilian education model has undergone many transformations. Some of these are related to at least two factors: the need for including students with special needs in the educational system, and the use of computers in learning-teaching processes in order to enhance such students' learning outcomes. Despite the efforts applied to both the issue of educational inclusion and to promoting computer access to educational activities in school environments, the major difficulty is how to adequately serve those students who currently represent a significant portion of the population enrolled in public, or private, school systems. Among the special needs students, there are those with a significant degree of reduction in visual acuity, called ""Low Vision"". Knowing that the use of computers in Brazilian schools has increased, there are strong reasons to believe that the gap currently inherent in the education of these individuals may be minimized by promoting access to computer resources in the learning-teaching process. In this paper, we describe the computer tool called xLupa, a screen magnifier, applied to images and texts, for students with low vision. To this end, the topics covered in this paper are: a) description of the tool specification and development process, which involved a study of the target users' visual needs, and the continuous interaction between developers and users, b) presentation of xLupa and its functionalities; c) assessment of how its use can help students and teachers in daily school activities. Digital accessibility; Equipment for special education; Low vision; Special education; Technology and education ;This paper presents an approach for carrying out gesture recognition for the Brazilian Sign Language Manual Alphabet. The gestural patterns are treated as a combination of three primitives, or cheremes - hand configuration, hand orientation and hand movement. The recognizer is built in a modular architecture composed by inductive reasoning modules, which use the artificial neural network Fuzzy Learning Vector Quantization; and rule-based modules. This architecture has been tested and results are presented here. Some strengths of such approach are: robustness of recognition, portability to similar contexts, extensibility of the dataset to be recognize and reduction of the vocabulary recognition problem to the recognition of its primitives. Cheremes; Finger-spelling applications; Gesture recognition; Sign language Artificial Neural Network; Cheremes; Data sets; Finger-spelling applications; Fuzzy learning vector quantizations; Hand configuration; Hand movement; Inductive reasoning; Modular architectures; Rule based; Sign language; Vocabulary recognition; Network architecture; Neural networks; Vector quantization; Gesture recognition;In several countries, deaf communities adopt the sign language as their official and natural language. This fact inserts a new field for the software application development that can improve sign language dissemination and social inclusion of deaf people. Alphabetic character-based applications, like games and educational softwares, can be adapted to run as fingerspelling-based applications, in which the inputs are signs (static images or videos) rather than letters (typed letters). In this paper, we present a Pattern Recognition Module, implemented by Committee Machine, for fingerspelling applications. The committee experts are built with supervised and unsupervised Fuzzy Learning Vector Quantization models using the ""boosting by filtering"" strategy. The module was tested in a specific sign language context considering hand configurations and hand movements. © 2010 ACM. committee machine; fuzzy learning vector quantization; image analysis; pattern recognition; sign language Boosting by filtering; Committee machines; Educational software; Fuzzy learning; fuzzy learning vector quantization; Hand configuration; Hand movement; Natural languages; Sign language; Social inclusion; Software applications; Static images; Image analysis; Linguistics; Pattern recognition; Software design; Vector quantization;In this paper, the vision-based hand movement recognition problem is formulated for the universe of discourse of the Brazilian Sign Language. In order to analyze this specific domain we have used the artificial neural networks models based on distance, including neural-fuzzy models. The experiments explored here show the usefulness of these models to extract helpful knowledge about the classes of movements and to support the project of adaptative recognizer modules for Libras-oriented computational tools. Using artificial neural networks architectures mdash; Self Organizing Maps and (Fuzzy) Learning Vector Quantization, it was possible to understand the data space and to build models able to recognize hand movements performed for one or more than one specific Libras users. © 2009 IEEE.  Artificial Neural Network; Computational tools; Data space; Distance-based; Hand movement; Learning Vector Quantization; Neural-fuzzy model; Sign language; Universe of discourse; Vision based; Backpropagation; Linguistics; Self organizing maps; Vector quantization; Fuzzy neural networks;This paper presents the principal results of a detailed study about the use of the Meaningful Fractal Fuzzy Dimension measure in the problem in determining adequately the topological dimension of output space of a Self-Organizing Map. This fractal measure is conceived by combining the Fractals Theory and Fuzzy Approximate Reasoning. In this work this measure was applied on the dataset in order to obtain a priori knowledge, which is used to support the decision making about the SOM output space design. Several maps were designed with this approach and their evaluations are discussed here. ©2008 IEEE.  Conformal mapping; Fractal dimension; Fractals; Maps; Neural networks; Optical projectors; Problem solving; Approximate reasonings; Priori knowledges; Self-organizing; Space designs; Topological dimensions; Self organizing maps;Self Organizing Map (SOM) is a kind of artificial neural network with a competitive and unsupervised learning. This technique is commonly used to dataset clustering tasks and can be useful in patterns recognition problems. This paper presents an artificial neural network application to signals language recognition problem, where the image representation is given by bit signatures. The recognition results are promising and are presented in this paper. More, some analysis about the combination ""SOM + bit signature"" improved our understanding about the characteristics of the LIBRAS signals and the conclusions are also listed in this paper. © 2008 IEEE.  Backpropagation; Image classification; Linguistics; Self organizing maps; Strength of materials; Vegetation; Artificial neural networks; Image representations; Language recognitions; Self-organizing; Signal languages; Neural networks;Learning Vector Quantization is a kind of artificial neural network with a competitive and supervised learning. This technique is commonly used to patterns recognition tasks. This artificial neural network was applied to the LIBRAS signals recognition problem, where the images representation was done with bits signatures. This is an unusual combination form and it seems promising, as inferred by the analysis of the results which are shown in this paper. © 2006 IEEE.  Electronic document identification systems; Neural networks; Supervised learning; Vector quantization; Bit signature; Images representation; Signals recognition; Pattern recognition;A hybrid system WELS implemented with the combination of Fractal Dimension Theory and Fuzzy Approximate Reasoning, in order to analyze datasets. In this paper, we describe its application in the initial phase of clustering methodology: the clustering tendency analysis. The Box-Counting Algorithm is carried out on a dataset, and with its resultant curve one obtains numeric indications related to the features of the dataset. Then, a fuzzy inference system acts upon these indications and produces information which enable the analysis mentioned above. © Springer-Verlag 2004. Clustering Tendency Analysis; Fractal Dimension Theory; Fuzzy Approximate Reasoning Approximation theory; Artificial intelligence; Fractals; Fuzzy inference; Hybrid systems; Approximate reasoning; Box-counting algorithm; Fuzzy approach; Fuzzy inference systems; ITS applications; Tendency analysis; Fractal dimension;In this paper, we describe a new methodology, based on resources of Fractal Dimension Theory and Fuzzy Approximated Reasoning, to determine the adequate dimension for the output space of Self-Organizing Maps (SOM). This approach considers the dataset under analysis and proposes the dimension that best satisfies the dilemma: to maximize the reduction of dimension and to minimize the topographic distortions - measured with the Topographic Product. In order to reach this objective, we create the ""significant fractal fuzzy dimension measure"" as a variation of a fractal dimension measure.  Algorithms; Decision making; Decision theory; Fractals; Fuzzy sets; Learning systems; Fractal dimension theory; Fuzzy approximated reasoning; Topographic mapping; Self organizing maps;W-operators compose a family of translation invariant and locally defined operators. This work proposes a new approach to automatic design of W-operators using learning vector quantization neural networks. It is also presented here some experimental results associated with the application of these operators in morphological image segmentation.  Algorithms; Gradient methods; Image enhancement; Image segmentation; Learning systems; Matrix algebra; Vector quantization; Learning vector quantization neural networks; Morphological image segmentation; W-operators; Watershed with markers; Neural networks"
Roberto Kalil Filho,Universidade de Sâo Paulo - Faculdade de Medicina,"Ovarian cancer is often diagnosed at advanced stages, when poorly responsive to standard treatment. First-line treatment consists in schemes including cytoreductive surgery followed by adjuvant chemotherapy schemes with platinum and taxane derivatives. Second-line regimens are based on gemcitabine and liposomal doxorubicin. Third line is often not worthwhile because of the high toxicity with poor response to treatment. Previously, we showed that paclitaxel (PTX) carried in non-protein lipid core nanoparticles (LDE) resembling the chemical structure of LDL has remarkably reduced toxicity. Here, the hypothesis was tested whether PTX-LDE could safely benefit patients in third-line treatment setting. Fourteen women unresponsive to second-line chemotherapy for ovarian cancer, aged 61 ± 10 years, clinical stage IV and TqNqM1, were included. PTX-LDE was administered at 175 mg/m2, 3/3 week dose. Patients were submitted to clinical examinations before each chemotherapy cycle. Serum biochemistry and imaging examinations to monitor disease progression were performed. In total, 74 cycles of chemotherapy were done and, in all cycles, clinical or laboratorial toxicities were not observed. Median progression-free survival (PFS) was 3.0 months (95% CI 2.0–3.9). In four patients, PFS was &gt;6 months and in 2 &gt; 1 year. The unpreceded, striking absence of toxicity and consistently long PFS, compared to previous results, indicate that at least 4 among 14 patients had tumor arrest by the treatment and clear benefit of PTX-LDE at third-line setting. The absence of observable toxicity allows dose escalating to improve response to treatment, as perspective to be tested in the ensuing studies. © 2017, Springer Science+Business Media, LLC. Drug delivery and cancer; Drug targeting and cancer; LDL receptors; Ovarian cancer treatment; Solid lipid particles ;Objectives: This study aimed to evaluate the amount and pattern of cardiac biomarker release after elective percutaneous coronary intervention (PCI) in patients without evidence of a new myocardial infarction (MI) after the procedure as assessed by cardiac magnetic resonance (CMR) with late gadolinium enhancement (LGE). Background: The release of myocardial necrosis biomarkers after PCI frequently occurs. However, the correlation between biomarker release and the diagnosis of procedure-related MI type 4a has been controversial. Methods: Patients with normal baseline cardiac biomarkers who were referred for elective PCI were prospectively included. CMR with LGE was performed in all of the patients before and after the intervention. Measurements of troponin I (TnI) and creatine kinase MB fraction (CK-MB) were systematically performed before and after the procedure. Patients with a new LGE on the post-procedure CMR were excluded. Results: Of the 56 patients with no evidence of a procedure-related MI as assessed by CMR after the PCI, 48 (85.1%) exhibited an elevation of TnI above the 99th percentile. In 32 patients (57.1%), the peak was greater than five times this limit. Additionally, 17 patients (30.4%) had a CK-MB peak above the 99th percentile limit, but this peak was greater than five times the 99th percentile in only two patients (3.6%). The median peak release of TnI was 0.290 (0.061–1.09) ng/mL, which was 7.25-fold higher than the 99th percentile. Conclusions: In contrast to CK-MB, an abnormal release of TnI often occurs after an elective PCI procedure, despite the absence of a new LGE on CMR. © 2017 Wiley Periodicals, Inc. cardiac magnetic resonance imaging; coronary artery disease; percutaneous coronary intervention; troponin ;Purpose: Acute myocardial infarction (MI) is accompanied by myocardial inflammation, fibrosis, and ventricular remodeling that, when excessive or not properly regulated, may lead to heart failure. Previously, lipid core nanoparticles (LDE) used as carriers of the anti-inflammatory drug methotrexate (MTX) produced an 80-fold increase in the cell uptake of MTX. LDE-MTX treatment reduced vessel inflammation and atheromatous lesions induced in rabbits by cholesterol feeding. The aim of the study was to investigate the effects of LDE-MTX on rats with MI, compared with commercial MTX treatment. Materials and methods: Thirty-eight Wistar rats underwent left coronary artery ligation and were treated with LDE-MTX, or with MTX (1 mg/kg intraperitoneally, once/week, starting 24 hours after surgery) or with LDE without drug (MI-controls). A sham-surgery group (n=12) was also included. Echocardiography was performed 24 hours and 6 weeks after surgery. The animals were euthanized and their hearts were analyzed for morphometry, protein expression, and confocal microscopy. Results: LDE-MTX treatment achieved a 40% improvement in left ventricular (LV) systolic function and reduced cardiac dilation and LV mass, as shown by echocardiography. LDE-MTX reduced the infarction size, myocyte hypertrophy and necrosis, number of inflammatory cells, and myocardial fibrosis, as shown by morphometric analysis. LDE-MTX increased antioxidant enzymes; decreased apoptosis, macrophages, reactive oxygen species production; and tissue hypoxia in non-infarcted myocardium. LDE-MTX increased adenosine bioavailability in the LV by increasing adenosine receptors and modulating adenosine catabolic enzymes. LDE-MTX increased the expression of myocardial vascular endothelium growth factor (VEGF) associated with adenosine release; this correlated not only with an increase in angiogenesis, but also with other parameters improved by LDE-MTX, suggesting that VEGF increase played an important role in the beneficial effects of LDE-MTX. Overall effects of commercial MTX were minor, and did not improve LV function or infarction size. Both treatments did not induce any toxicity. Conclusion: The remarkable improvement in heart function and reduction in infarction size achieved by LDE-MTX supports future clinical trials. © 2017 Maranhão et al. Adenosine; Drug delivery; Lipid particle; Myocardial infarction; VEGF ;The release of myocardial necrosis biomarkers after off-pump coronary artery bypass grafting (OPCAB) frequently occurs. However, the correlation between biomarker release and the diagnosis of procedure-related myocardial infarction (MI) (type 5) has been controversial. This study aimed to evaluate the amount and pattern of cardiac biomarker release after elective OPCAB in patients without evidence of a new MI on cardiac magnetic resonance (CMR) imaging with late gadolinium enhancement (LGE).Patients with normal baseline cardiac biomarkers referred for elective OPCAB were prospectively included. CMR with LGE was performed in all patients before and after interventions. Measurements of troponin I (cTnI) and creatine kinase MB fraction (CK-MB) were systematically performed before and after the procedure. Patients with new LGE on the postprocedure CMR were excluded.All of the 53 patients without CMR evidence of a procedure-related MI after OPCAB exhibited a cTnI elevation peak above the 99th percentile. In 48 (91%), the peak value was >10 times this threshold. However, 41 (77%) had a CK-MB peak above the limit of the 99th percentile, and this peak was >10 times the 99th percentile in only 7 patients (13%). The median peak release of cTnI was 0.290 (0.8-3.7) ng/mL, which is 50-fold higher than the 99th percentile.In contrast with CK-MB, considerable cTnI release often occurs after an elective OPCAB procedure, despite the absence of new LGE on CMR.  biological marker; creatine kinase MB; troponin I; adverse effects; aged; blood; cardiac muscle; female; human; male; middle aged; necrosis; nuclear magnetic resonance imaging; off pump coronary surgery; pathology; prospective study; Aged; Biomarkers; Coronary Artery Bypass, Off-Pump; Creatine Kinase, MB Form; Female; Humans; Magnetic Resonance Imaging; Male; Middle Aged; Myocardium; Necrosis; Prospective Studies; Troponin I;Background: Since a male-related higher cardiovascular morbidity and mortality in patients with Chagas' heart disease has been reported, we aimed to investigate gender differences in myocardial damage assessed by cardiovascular magnetic resonance (CMR). Methods and results: Retrospectively, 62 seropositive Chagas' heart disease patients referred to CMR (1.5 T) and with low probability of having significant coronary artery disease were included in this analysis. Amongst both sexes, there was a strong negative correlation between LV ejection fraction and myocardial fibrosis (male r = 0.64, female r = 0.73, both P < 0.001), with males showing significantly greater myocardial fibrosis (P = 0.002) and lower LV ejection fraction (P < 0.001) than females. After adjustment for potential confounders, gender remained associated with myocardial dysfunction, and 53% of the effect was mediated by myocardial fibrosis (P for mediation = 0.004). Also, the transmural pattern was more prevalent among male patients (23.7 vs. 9.9%, P < 0.001) as well as the myocardial heterogeneity or gray zone (2.2 vs. 1.3 g, P = 0.003). Conclusions: We observed gender-related differences in myocardial damage assessed by CMR in patients with Chagas' heart disease. As myocardial fibrosis and myocardial dysfunction are associated to cardiovascular outcomes, our findings might help to understand the poorer prognosis observed in males in Chagas' disease. © 2016 The Author(s). Chagas' heart disease; Gender differences; Myocardial dysfunction; Myocardial fibrosis ;Background: Chagas disease (CD) is an important cause of heart failure and mortality, mainly in Latin America. This study evaluated the morphological and functional characteristics of the heart as well the extent of myocardial fibrosis (MF) in patients with CD by cardiac magnetic resonance (CMR). The prognostic value of MF evaluated by myocardialdelayed enhancement (MDE) was compared with that via Rassi score. Methods: This study assessed 39 patients divided into 2 groups: 28 asymptomatic patients as indeterminate form group (IND); and symptomatic patients as Chagas Heart Disease (CHD) group. All patients underwent CMR using the techniques of cine-MRI and MDE, and the amount of MF was compared with the Rassi score. Results: Regarding the morphological and functional analysis, significant differences were observed between both groups (p < 0.001). Furthermore, there was a strong correlation between the extent of MF and the Rassi score (r = 0.76). Conclusions: CMR is an important technique for evaluating patients with CD, stressing morphological and functional differences in all clinical presentations. The strong correlation with the Rassi score and the extent of MF detected by CMR emphasizes its role in the prognostic stratification of patients with CD. © 2016, Arquivos Brasileiros de Cardiologia. All rights reserved. Chagas cardiomyopathy; Chagas disease; Magnetic resonance spectroscopy; Risk factors, prognosis ;Background The lack of a correlation between myocardial necrosis biomarkers and electrocardiographic abnormalities after revascularization procedures has resulted in a change in the myocardial infarction (MI) definition. Methods Patients with stable multivessel disease who underwent percutaneous or surgical revascularization were included. Electrocardiograms and concentrations of high-sensitive cardiac troponin I (cTnI) and creatine kinase (CK)-MB were assessed before and after procedures. Cardiac magnetic resonance and late gadolinium enhancement were performed before and after procedures. MI was defined as more than five times the 99th percentile upper reference limit for cTnI and 10 times for CK-MB in percutaneous coronary intervention (PCI) and coronary artery bypass grafting (CABG), respectively, and new late gadolinium enhancement for cardiac magnetic resonance. Results Of the 202 patients studied, 69 (34.1%) underwent on-pump CABG, 67 (33.2%) off-pump CABG, and 66 (32.7%) PCI. The receiver operating characteristic curve showed the accuracy of cTnI for on-pump CABG, off-pump CABG, and PCI patients was 21.7%, 28.3%, and 52.4% and for CK-MB was 72.5%, 81.2%, and 90.5%, respectively. The specificity of cTnI was 3.6%, 9.4%, and 42.1% and of CK-MB was 73.2%, 86.8%, and 96.4%, respectively. Sensitivity of cTnI was 100%, 100%, and 100% and of CK-MB was 69.2%, 64.3%, and 44.4%, respectively. The best cutoff of cTnI for on-pump CABG, off-pump CABG, and PCI was 6.5 ng/mL, 4.5 ng/mL, and 4.5 ng/mL (162.5, 112.5, and 112.5 times the 99th percentile upper reference limit) and of CK-MB was 37.5 ng/mL, 22.5 ng/mL, and 11.5 ng/mL (8.5, 5.1, and 2.6 times the 99th percentile upper reference limit), respectively. Conclusions Compared with cardiac magnetic resonance, CK-MB was more accurate than cTnI for diagnosing MI. These data suggest a higher troponin cutoff for the diagnosis of procedure-related MI. © 2016 The Society of Thoracic Surgeons.  biological marker; creatine kinase MB; gadolinium; troponin I; biological marker; creatine kinase MB; troponin I; adult; Article; cardiovascular magnetic resonance; coronary artery bypass graft; diagnostic accuracy; diagnostic test accuracy study; electrocardiogram; female; heart infarction; human; major clinical study; male; off pump coronary surgery; on pump coronary surgery; percutaneous coronary intervention; priority journal; prospective study; receiver operating characteristic; revascularization; sensitivity and specificity; vascular disease; adverse effects; aged; blood; cine magnetic resonance imaging; cohort analysis; comparative study; diagnostic imaging; middle aged; mortality; Myocardial Infarction; procedures; prognosis; retrospective study; severity of illness index; survival analysis; Aged; Biomarkers; Cohort Studies; Coronary Artery Bypass; Creatine Kinase, MB Form; Female; Humans; Magnetic Resonance Imaging, Cine; Male; Middle Aged; Myocardial Infarction; Percutaneous Coronary Intervention; Prognosis; Retrospective Studies; Sensitivity and Specificity; Severity of Illness Index; Survival Analysis; Troponin I;Aims: While the atherosclerotic plaque volume can be manually quantified in coronary computed tomography angiography (CTA) it is impractical for clinical routine use. Several anatomical scores have been developed as surrogates for overall atherosclerotic burden in coronary CTA and even proven to be highly predictive for future adverse events. However, they have not been validated against the gold standard for atherosclerotic burden, intra-vascular ultrasound (IVUS). In the present study we have compared several coronary CTA scores with the coronary IVUS. Methods and results: A total of 62 patients with diagnosed coronary disease scheduled for percutaneous intervention were prospectively enrolled. For all patients, coronary CTA and multivessel IVUS were obtained. Calcium score and 6 previously reported scores were calculated from coronary CTA imaging and compared to average IVUS-derived percent atheroma volume (PAV). On average, 3.8 ± 0.7 vessels, comprising 123.8 ± 31.3 mm in length, were imaged with IVUS per patient. All but one previously described scoring systems showed a significant association with IVUS-derived PAV. Among them, the SSS score demonstrated the strongest correlation with IVUS-PAV (r = 0.61, p < 0.001) and the greatest area under the ROC curve (C-statistic = 0.87), to predict a high PAV. Conclusions: Most frequently used coronary CTA scores have a good correlation with global coronary atherosclerotic burden measured by multivessel IVUS derived atheroma volume. Among them, the SSS score shows the best performance being a good non-invasive alternative to IVUS for global coronary atherosclerotic burden assessment. © 2016 Elsevier Ireland Ltd. Atherosclerosis; Computed tomography; Coronary disease; Intravascular ultrasound glyceryl trinitrate; adult; angiocardiography; angiography device; area under the curve; Article; atheroma; cardiovascular disease assessment; comparative study; computed tomographic angiography; computed tomography scanner; coronary artery atherosclerosis; coronary artery calcium score; coronary artery disease; correlation analysis; female; Friesinger score; Gensini score; high risk population; human; intravascular ultrasound; intravascular ultrasound catheter; Leaman score; major clinical study; male; middle aged; multidetector computed tomography; percutaneous coronary intervention; priority journal; prospective study; receiver operating characteristic; secondary prevention; aged; atherosclerotic plaque; blood vessel calcification; coronary angiography; coronary artery disease; coronary blood vessel; diagnostic imaging; interventional ultrasonography; predictive value; procedures; reproducibility; severity of illness index; validation study; Aged; Area Under Curve; Computed Tomography Angiography; Coronary Angiography; Coronary Artery Disease; Coronary Vessels; Female; Humans; Male; Middle Aged; Plaque, Atherosclerotic; Predictive Value of Tests; Prospective Studies; Reproducibility of Results; ROC Curve; Severity of Illness Index; Ultrasonography, Interventional; Vascular Calcification;Objectives: To evaluate the effects of goal-directed therapy on outcomes in high-risk patients undergoing cardiac surgery. Design: A prospective randomized controlled trial and an updated metaanalysis of randomized trials published from inception up to May 1, 2015. Setting: Surgical ICU within a tertiary referral university-affiliated teaching hospital. Patients: One hundred twenty-six high-risk patients undergoing coronary artery bypass surgery or valve repair. Interventions: Patients were randomized to a cardiac output-guided hemodynamic therapy algorithm (goal-directed therapy group, n = 62) or to usual care (n = 64). In the goal-directed therapy arm, a cardiac index of greater than 3 L/min/m2 was targeted with IV fluids, inotropes, and RBC transfusion starting from cardiopulmonary bypass and ending 8 hours after arrival to the ICU. Measurements and Main Results: The primary outcome was a composite endpoint of 30-day mortality and major postoperative complications. Patients from the goal-directed therapy group received a greater median (interquartile range) volume of IV fluids than the usual care group (1,000 [625-1,500] vs 500 [500-1,000] mL; p &lt; 0.001], with no differences in the administration of either inotropes or RBC transfusions. The primary outcome was reduced in the goal-directed therapy group (27.4% vs 45.3%; p = 0.037). The goal-directed therapy group had a lower occurrence rate of infection (12.9% vs 29.7%; p = 0.002) and low cardiac output syndrome (6.5% vs 26.6%; p = 0.002). We also observed lower ICU cumulative dosage of dobutamine (12 vs 19 mg/kg; p = 0.003) and a shorter ICU (3 [3-4] vs 5 [4-7] d; p &lt; 0.001) and hospital length of stay (9 [8-16] vs 12 [9-22] d; p = 0.049) in the goal-directed therapy compared with the usual care group. There were no differences in 30-day mortality rates (4.8% vs 9.4%, respectively; p = 0.492). The metaanalysis identified six trials and showed that, when compared with standard treatment, goal-directed therapy reduced the overall rate of complications (goal-directed therapy, 47/410 [11%] vs usual care, 92/415 [22%]; odds ratio, 0.40 [95% CI, 0.26-0.63]; p &lt; 0.0001) and decreased the hospital length of stay (mean difference,-5.44 d; 95% CI,-9.28 to-1.60; p = 0.006) with no difference in postoperative mortality: 9 of 410 (2.2%) versus 15 of 415 (3.6%), odds ratio, 0.61 (95% CI, 0.26-1.47), and p = 0.27. Conclusions: Goal-directed therapy using fluids, inotropes, and blood transfusion reduced 30-day major complications in high-risk patients undergoing cardiac surgery. © 2016 by the Society of Critical Care Medicine and Wolters Kluwer Health, Inc. All Rights Reserved. cardiac surgery; goal-directed; hemodynamic; resuscitation dobutamine; infusion fluid; inotropic agent; beta 1 adrenergic receptor stimulating agent; dobutamine; clinical evaluation; confidence interval; coronary artery bypass surgery; erythrocyte transfusion; event free survival; forward heart failure; goal directed therapy; heart output; heart surgery; heart valve surgery; hemodynamic resuscitation therapy; hemodynamics; high risk patient; hospitalization; human; intensive care unit; length of stay; meta analysis; outcome assessment; perioperative period; postoperative complication; postoperative infection; priority journal; prospective study; randomized controlled trial (topic); Review; surgical mortality; systematic review; teaching hospital; treatment outcome; aged; controlled study; female; fluid therapy; heart surgery; hemodynamics; literature; male; meta analysis (topic); middle aged; mortality; physiology; Postoperative Complications; procedures; randomized controlled trial; very elderly; Adrenergic beta-1 Receptor Agonists; Aged; Aged, 80 and over; Cardiac Output; Cardiac Surgical Procedures; Dobutamine; Female; Fluid Therapy; Hemodynamics; Humans; Intensive Care Units; Length of Stay; Male; Meta-Analysis as Topic; Middle Aged; Postoperative Complications; Review Literature as Topic; Treatment Outcome;Objective Evaluate if statin therapy prior to elective coronary stent implantation (CSI) reduces the plasma levels of markers of inflammation and of myocardial necrosis in low-risk stable coronary artery disease patients (CAD). Background The elevation of markers of inflammation and of myocardial necrosis after percutaneous coronary intervention may interfere with clinical outcome. Among acute coronary syndrome patients, statins improve clinical outcomes when used before CSI - mostly due to reduction of CSI-related myocardial infarction. However, little is known concerning preprocedural statin therapy on the reduction of these markers in stable patients at low-risk. Methods In this prospective, observational study, 100 patients (n = 50 on statin therapy vs. n = 50 not on statin) with stable coronary artery disease underwent elective CSI. Inflammatory (C-reactive protein [CRP], interleukin [IL]-6, tumor necrosis factor-α and matrix metalloproteinase-9) and myocardial necrosis markers (troponin I and CK-MB) were determined before and 24 hr after CSI. Results All patients presented a significant increase of CRP and IL-6 after CSI. However, this increase was attenuated in patients on statin therapy prior to CSI than those without statin therapy: 75% vs. 150% (P < 0.001) and 192% vs. 300% (P < 0.01). The other pro-inflammatory markers were similar for both sets of patients. Troponin I and CK-MB did not change after CSI regardless of previous statin therapy or not. Conclusions Pretreatment with statin attenuates procedural inflammation, denoted by markedly lower increases of CRP and IL-6 levels, in elective CSI within low-risk stable CAD patients. Periprocedural myocardial injury was irrelevant and was not affected by preprocedural statin therapy in this population. © 2015 Wiley Periodicals, Inc. coronary stent implantation; inflammatory markers; myocardial damage; stable coronary artery disease; statin acetylsalicylic acid; atorvastatin; C reactive protein; clopidogrel; creatine kinase MB; ezetimibe plus simvastatin; hydroxymethylglutaryl coenzyme A reductase inhibitor; interleukin 6; rosuvastatin; simvastatin; troponin I; antiinflammatory agent; autacoid; biological marker; C reactive protein; creatine kinase MB; hydroxymethylglutaryl coenzyme A reductase inhibitor; IL6 protein, human; interleukin 6; troponin I; adult; Article; blood sampling; clinical article; controlled study; coronary artery disease; coronary stenting; female; heart muscle injury; heart muscle necrosis; human; inflammation; loading drug dose; low risk patient; male; observational study; preoperative treatment; prospective study; adverse effects; aged; blood; cardiac muscle; coronary artery disease; devices; metabolism; middle aged; necrosis; pathology; percutaneous coronary intervention; risk factor; stent; treatment outcome; Aged; Anti-Inflammatory Agents; Biomarkers; C-Reactive Protein; Coronary Artery Disease; Creatine Kinase, MB Form; Female; Humans; Hydroxymethylglutaryl-CoA Reductase Inhibitors; Inflammation Mediators; Interleukin-6; Male; Middle Aged; Myocardium; Necrosis; Percutaneous Coronary Intervention; Prospective Studies; Risk Factors; Stents; Treatment Outcome; Troponin I;Background: Chagas' heart disease is an important public health problem in South America. Several aspects of the pathogenesis are not fully understood, especially in its subclinical phases. On pathology Chagas' heart disease is characterized by chronic myocardial inflammation and extensive myocardial fibrosis. The latter has also been demonstrated by late gadolinium enhancement (LGE) by cardiovascular magnetic resonance (CMR). In three clinical phases of this disease, we sought to investigate the presence of LGE, myocardial increase in signal intensity in T2-weighted images (T2W) and in T1-weighted myocardial early gadolinium enhancement (MEGE), previously described CMR surrogates for myocardial fibrosis, myocardial edema and hyperemia, respectively. Methods: Fifty-four patients were analyzed. Sixteen patients with the indeterminate phase (IND), seventeen patients with the cardiac phase with no left ventricular systolic dysfunction (CPND), and twenty-one patients with the cardiac phase with left ventricular systolic dysfunction (CPD). All patients underwent 1.5 T CMR scan including LGE, T2W and MEGE image sequences to evaluate myocardial abnormalities. Results: Late gadolinium enhancement was present in 72.2 % of all patients, in 12.5 % of IND, 94.1 % of the CPND and 100 % of the CPD patients (p < 0.0001). Myocardial increase in signal intensity in T2-weighted images (T2W) was present in 77.8 % of all patients, in 31.3 % of the IND, 94.1 % of the CPND and 100 % of the CPD patients (p < 0.0001). T1-weighted myocardial early gadolinium enhancement (MEGE) was present in 73.8 % of all patients, in 25.0 % of the IND, 92.3 % of the CPND and 94.1 % of the CPD (p < 0.0001). A good correlation between LGE and T2W was observed (r = 0.72, and p < 0.001). Conclusions: Increase in T2-weighted (T2W) myocardial signal intensity and T1-weighted myocardial early gadolinium enhancement (MEGE) can be detected by CMR in patients throughout all phases of Chagas' heart disease, including its subclinical presentation (IND). Moreover, those findings were parallel to myocardial fibrosis (LGE) in extent and location and also correlated with the degree of Chagas' heart disease clinical severity. These findings contribute to further the knowledge on pathophysiology of Chagas' heart disease, and might have therapeutic and prognostic usefulness in the future. © 2015 Torreão et al. Cardiovascular magnetic resonance; Chagas' heart disease; Fibrosis; Late gadolinium enhancement; Myocarditis; T2 weighted image gadolinium; contrast medium; gadolinium 1,4,7,10-tetraazacyclododecane-N,N',N'',N'''-tetraacetate; heterocyclic compound; organometallic compound; adult; aged; Article; cardiovascular magnetic resonance; Chagas disease; contrast enhancement; cross-sectional study; disease severity; female; heart muscle fibrosis; human; left ventricular systolic dysfunction; major clinical study; male; priority journal; very elderly; cardiac muscle; Chagas cardiomyopathy; fibrosis; heart edema; heart left ventricle function; middle aged; nuclear magnetic resonance imaging; parasitology; pathology; pathophysiology; predictive value; severity of illness index; systole; Adult; Aged; Chagas Cardiomyopathy; Contrast Media; Cross-Sectional Studies; Edema, Cardiac; Female; Fibrosis; Heterocyclic Compounds; Humans; Magnetic Resonance Imaging; Male; Middle Aged; Myocardium; Organometallic Compounds; Predictive Value of Tests; Severity of Illness Index; Systole; Ventricular Dysfunction, Left; Ventricular Function, Left;[No abstract available]  amiodarone; benznidazole; nifurtimox; posaconazole; awareness; cardiovascular magnetic resonance; cardiovascular mortality; Chagas disease; disease course; disease predisposition; drug effect; drug efficacy; drug safety; drug treatment failure; genetic polymorphism; health care cost; heart muscle conduction system; heart transplantation; heart ventricle arrhythmia; human; implantable cardioverter defibrillator; parasite transmission; priority journal; randomized controlled trial (topic); Review; survival; Trypanosoma cruzi; vector control; Chagas Cardiomyopathy; cost of illness; drug effects; economics; female; health; health care cost; international cooperation; male; Amiodarone; Chagas Cardiomyopathy; Cost of Illness; Female; Global Health; Health Expenditures; Humans; Internationality; Male; Trypanosoma cruzi;Background: Familial hypercholesterolemia is characterized by elevated plasma cholesterol and early coronary arterial disease onset. However, few studies investigated the association of heterozygous familial hypercholesterolemia with peripheral arterial disease. Methods: In a cross sectional study 202 heterozygous familial hypercholesterolemia patients (91% confirmed by molecular diagnosis) were compared to 524 normolipidemic controls. Peripheral arterial disease was diagnosed by ankle-brachial index values ≤0.90. Results: Compared with controls, familial hypercholesterolemia patients were older, more often female, with higher rates of hypertension, diabetes, previous coronary disease and higher total cholesterol levels. Smoking (previous and former) was more common among controls. The prevalence of peripheral arterial disease was 17.3 and 2.3% respectively in familial hypercholesterolemia and controls (p < 0.001). Results persisted after matching familial hypercholesterolemia and controls by a propensity score. Regression analyses demonstrated that age (odds ratio- OR = 1.03 95% CI 1.00-1.05, p = 0.033), previous cardiovascular disease (OR = 3.12 CI 95% 1.56-6.25, p = 0.001) and familial hypercholesterolemia diagnosis (OR = 5.55 CI 95% 2.69-11.44, p. < 0.001) were independently associated with peripheral arterial disease. Among familial hypercholesterolemia patients, age (OR 1.05, 95% CI 1.02-1.09, p = 0.005), intermittent claudication (OR 6.32, 95% CI 2.60-15.33, p. < 0.001) and smoking (OR 2.44, 95% CI 1.08-5.52, p = 0.032) were associated with peripheral arterial disease. Conclusions: Peripheral arterial disease is more frequent in familial hypercholesterolemia than in normolipidemic subjects and it should routine screened in these individuals even if asymptomatic. However, its role as predictor of cardiovascular events needs to be ascertained prospectively. © 2015 Elsevier Ireland Ltd. Atherosclerosis; Familial hypercholesterolemia; Peripheral artery disease antilipemic agent; cholesterol; creatinine; glucose; high density lipoprotein cholesterol; low density lipoprotein cholesterol; low density lipoprotein receptor; genetic marker; adult; ankle brachial index; Article; cardiovascular risk; cholesterol blood level; controlled study; cross-sectional study; diastolic blood pressure; disease association; familial hypercholesterolemia; female; glucose blood level; heart muscle revascularization; heterozygosity; human; intermittent claudication; major clinical study; male; middle aged; molecular diagnosis; observational study; peripheral occlusive artery disease; prevalence; priority journal; propensity score; sex difference; smoking; adverse effects; age; Brazil; case control study; chi square distribution; comorbidity; genetic marker; genetic predisposition; genetics; heterozygote; Hyperlipoproteinemia Type II; odds ratio; pathophysiology; Peripheral Arterial Disease; prospective study; register; risk assessment; risk factor; statistical model; Adult; Age Factors; Ankle Brachial Index; Brazil; Case-Control Studies; Chi-Square Distribution; Comorbidity; Cross-Sectional Studies; Female; Genetic Markers; Genetic Predisposition to Disease; Heterozygote; Humans; Hyperlipoproteinemia Type II; Logistic Models; Male; Middle Aged; Odds Ratio; Peripheral Arterial Disease; Prevalence; Propensity Score; Prospective Studies; Registries; Risk Assessment; Risk Factors; Smoking;Background: We previously showed that unesterified-cholesterol transfer to high-density lipoprotein (HDL), a crucial step in cholesterol esterification and role in reverse cholesterol transport, was diminished in non-diabetic patients with coronary artery disease (CAD). The aim was to investigate whether, in patients with type 2 diabetes mellitus (T2DM), the occurrence of CAD was also associated with alterations in lipid transfers and other parameters of plasma lipid metabolism. Methods: Seventy-nine T2DM with CAD and 76 T2DM without CAD, confirmed by cineangiography, paired for sex, age (40-80 years), BMI and without statin use, were studied. In vitro transfer of four lipids to HDL was performed by incubating plasma of each patient with a donor emulsion containing radioactive lipids during 1 h at 37 °C. Lipids transferred to HDL were measured after chemical precipitation of non-HDL fractions and the emulsion. Results are expressed as % of total radioactivity of each lipid in HDL. Results: In T2DM + CAD, LDL-cholesterol and apo B were higher than in T2DM. T2DM + CAD also showed diminished transfer to HDL of unesterified cholesterol (T2DM + CAD = 7.6 ± 1.2; T2DM = 8.2 ± 1.5 %, p < 0.01) and of cholesteryl-esters (4.0 ± 0.6 vs 4.3 ± 0.7, p < 0.01). Unesterified cholesterol in the non-HDL serum fraction was higher in T2DM + CAD (0.93 ± 0.20 vs 0.85 ± 0.15, p = 0.02) and CETP concentration was diminished (2.1 ± 1.0 vs 2.5 ± 1.1, p = 0.02). Lecithin-cholesterol acyltransferase activity, HDL size and lipid composition were equal. Conclusion: Reduction in T2DM + CAD of cholesterol transfer to HDL may impair cholesterol esterification and reverse cholesterol transport and altogether with simultaneous increased plasma unesterified cholesterol may facilitate CAD development in T2DM. © 2015 Sprandel et al. Cholesterol; Coronary artery disease; HDL; High-density lipoprotein; Lipid transfers; Nanoparticles; Type 2 diabetes apolipoprotein A1; apolipoprotein B; cholesterol; cholesterol ester; cholesterol ester transfer protein; high density lipoprotein; high density lipoprotein cholesterol; low density lipoprotein cholesterol; phosphatidylcholine sterol acyltransferase; triacylglycerol; unclassified drug; unesterified cholesterol; APOB protein, human; apolipoprotein B100; biological marker; cholesterol ester; high density lipoprotein; low density lipoprotein cholesterol; nanoparticle; adult; aged; Article; body mass; case control study; cholesterol blood level; cholesterol esterification; cholesterol transport; cineangiocardiography; comparative study; controlled study; coronary artery disease; disease association; enzyme activity; female; human; in vitro study; incubation temperature; incubation time; lipid composition; lipid transport; major clinical study; male; non insulin dependent diabetes mellitus; particle size; precipitation; reverse cholesterol transport; blood; cineangiography; complication; coronary artery disease; Diabetes Mellitus, Type 2; Diabetic Angiopathies; Dyslipidemias; middle aged; risk factor; very elderly; Adult; Aged; Aged, 80 and over; Apolipoprotein B-100; Biomarkers; Case-Control Studies; Cholesterol Esters; Cholesterol, LDL; Cineangiography; Coronary Artery Disease; Diabetes Mellitus, Type 2; Diabetic Angiopathies; Dyslipidemias; Female; Humans; Lipoproteins, HDL; Male; Middle Aged; Nanoparticles; Particle Size; Risk Factors;Background: Heart surgery has developed with increasing patient complexity. Objective: To assess the use of resources and real costs stratified by risk factors of patients submitted to surgical cardiac procedures and to compare them with the values reimbursed by the Brazilian Unified Health System (SUS). Method: All cardiac surgery procedures performed between January and July 2013 in a tertiary referral center were analyzed. Demographic and clinical data allowed the calculation of the value reimbursed by the Brazilian SUS. Patients were stratified as low, intermediate and high-risk categories according to the EuroSCORE. Clinical outcomes, use of resources and costs (real costs versus SUS) were compared between established risk groups. Results: Postoperative mortality rates of low, intermediate and high-risk EuroSCORE risk strata showed a significant linear positive correlation (EuroSCORE: 3.8%, 10%, and 25%; p < 0.0001), as well as occurrence of any postoperative complication (EuroSCORE: 13.7%, 20.7%, and 30.8%, respectively; p = 0.006). Accordingly, length-of-stay increased from 20.9 days to 24.8 and 29.2 days (p < 0.001). The real cost was parallel to increased resource use according to EuroSCORE risk strata (R$ 27.116,00 ± R$ 13.928,00 versus R$ 34.854,00 ± R$ 27.814,00 versus R$ 43.234,00 ± R$ 26.009,00, respectively; p < 0.001). SUS reimbursement also increased (R$ 14.306,00 ± R$ 4.571,00 versus R$ 16.217,00 ± R$ 7.298,00 versus R$ 19.548,00 ± R$935,00; p < 0.001). However, as the EuroSCORE increased, there was significant difference (p < 0.0001) between the real cost increasing slope and the SUS reimbursement elevation per EuroSCORE risk strata. Conclusion: Higher EuroSCORE was related to higher postoperative mortality, complications, length of stay, and costs. Although SUS reimbursement increased according to risk, it was not proportional to real costs. © 2015, Arquivos Brasileiros de Cardiologia. All rights reserved. Cardiac surgical procedures/economics; Hospital costs; Hospital mortality; Morbidity; Preoperative care; Risk groups; Unified health system adult; aged; Article; calculation; cost benefit analysis; EuroSCORE; female; heart surgery; hospital discharge; hospitalization; human; length of stay; logistic regression analysis; major clinical study; male; middle aged; observational study; outcome assessment; preoperative care; prospective study; public health; risk factor; Brazil; comparative study; economics; heart surgery; mortality; nonparametric test; postoperative complication; preoperative period; public health; reference value; reimbursement; risk assessment; severity of illness index; tertiary care center; Aged; Brazil; Cardiac Surgical Procedures; Female; Humans; Length of Stay; Male; Middle Aged; National Health Programs; Postoperative Complications; Preoperative Period; Prospective Studies; Reference Values; Reimbursement Mechanisms; Risk Assessment; Risk Factors; Severity of Illness Index; Statistics, Nonparametric; Tertiary Care Centers;Objective: Investigate the relations of glycemic levels with plasma lipids and in vitro lipid transfers to HDL in patients with type 2 diabetes mellitus. Materials and Methods: 143 patients with type 2 diabetes not taking anti-lipidemic drugs were separated into 2 groups: group A included 62 patients with glycated hemoglobin (HbA<inf>1c</inf>)≤6.5% (48mmol/mol) and group B 81 patients with HbA<inf>1c</inf>>6.5%. In vitro transfer of lipids was determined by 1h incubation of a donor nanoemulsion containing radioactively labeled unesterified and esterified cholesterol, phospholipids and triglycerides with whole plasma followed by chemical precipitation and radioactive counting in the supernatant (HDL). Results: LDL and HDL cholesterol were similar in Group A and B, but group B had higher triglycerides (2.31±1.30 vs. 1.58±0.61mmol/l, P<0.0001) and total and non-HDL unesterified cholesterol (36.3±7.8 vs. 33.9±5.9mmol/l, P<0,05; 30.6±7.9 vs. 27.6±6.2mmol/l, P<0,05; respectively) than group A and a non-significant trend to increased apolipoprotein B (103±20 vs. 97±20mg/dl, P=0.08). 36 patients with the highest, ≥8.0% (64mmol/mol), HbA<inf>1c</inf> also showed non-significant trend of elevated non-esterified fatty acids (NEFA) compared to 37 with lowest, ≤6.0% (42mmol/mol), HbA<inf>1c</inf> (P=0.08). Patients with higher NEFA had higher triglycerides than those with lower NEFA levels (P<0.01).Transfers of all lipids from nanoemulsion to HDL and lipid composition of HDL were equal in both groups. Conclusions: For the first time it was shown that in addition to triglycerides, unesterified cholesterol is also a marker of poor glycemic control. In vitro HDL lipid transfers, an important aspect of HDL metabolism, were not related with the glycemic control. J. A. Barth Verlag in Georg Thieme Verlag KG Stuttgart. diabetic dyslipidemia; glycated hemoglobin; lipoproteins metabolism apolipoprotein B; cholesterol; cholesterol ester; fatty acid; glucose; hemoglobin A1c; high density lipoprotein; high density lipoprotein cholesterol; lipid; low density lipoprotein cholesterol; phospholipid; triacylglycerol; antidiabetic agent; glucose blood level; glycosylated hemoglobin; hemoglobin A1c protein, human; lipid; adult; Article; controlled study; diabetic patient; female; glucose blood level; glycemic control; human; lipid blood level; lipid composition; lipid transport; lipoprotein metabolism; major clinical study; male; middle aged; nanoemulsion; non insulin dependent diabetes mellitus; precipitation; priority journal; supernatant; aged; blood; Diabetes Mellitus, Type 2; lipid metabolism; metabolism; physiology; Aged; Blood Glucose; Diabetes Mellitus, Type 2; Female; Hemoglobin A, Glycosylated; Humans; Hypoglycemic Agents; Lipid Metabolism; Lipids; Male; Middle Aged;Background: Noncompaction cardiomyopathy (NCC) is a rare genetic cardiomyopathy characterized by a thin, compacted epicardial layer and an extensive noncompacted endocardial layer. The clinical manifestations of this disease include ventricular arrhythmia, heart failure, and systemic thromboembolism. Case presentation: A 43-year-old male was anticoagulated by pulmonary thromboembolism for 1 year when he developed progressive dyspnea. Cardiovascular magnetic resonance imaging showed severe biventricular trabeculation with an ejection fraction of 15%, ratio of maximum noncompacted/compacted diastolic myocardial thickness of 3.2 and the presence of exuberant biventricular apical thrombus. Conclusion: Still under discussion is the issue of which patients and when they should be anticoagulated. It is generally recommended to those presenting ventricular systolic dysfunction, antecedent of systemic embolism, presence of cardiac thrombus and atrial fibrillation. In clinical practice the patients with NCC and ventricular dysfunction have been given oral anticoagulation, although there are no clinical trials showing the real safety and benefit of this treatment. © Tavares de Melo et al.; licensee BioMed Central. Cardiomyopathy; Echocardiography; Magnetic resonance; Noncompaction; Thromboembolism anticoagulant agent; gadolinium; adult; Article; cardiovascular magnetic resonance; cardiovascular parameters; case report; computed tomographic angiography; contrast enhancement; coronary artery atherosclerosis; drug indication; dyspnea; echocardiography; gout; heart atrium fibrillation; heart ejection fraction; heart failure; heart ventricle failure; human; intracardiac thrombosis; left anterior descending coronary artery; lung embolism; male; myocardial thickness; obesity; priority journal; sinus rhythm; systolic dysfunction; treatment duration; ventricular noncompaction;BACKGROUND: Noncompaction cardiomyopathy (NCC) is a rare genetic cardiomyopathy characterized by a thin, compacted epicardial layer and an extensive noncompacted endocardial layer. The clinical manifestations of this disease include ventricular arrhythmia, heart failure, and systemic thromboembolism.CASE PRESENTATION: A 43-year-old male was anticoagulated by pulmonary thromboembolism for 1 year when he developed progressive dyspnea. Cardiovascular magnetic resonance imaging showed severe biventricular trabeculation with an ejection fraction of 15%, ratio of maximum noncompacted/compacted diastolic myocardial thickness of 3.2 and the presence of exuberant biventricular apical thrombus.CONCLUSION: Still under discussion is the issue of which patients and when they should be anticoagulated. It is generally recommended to those presenting ventricular systolic dysfunction, antecedent of systemic embolism, presence of cardiac thrombus and atrial fibrillation. In clinical practice the patients with NCC and ventricular dysfunction have been given oral anticoagulation, although there are no clinical trials showing the real safety and benefit of this treatment.  adult; Arrhythmia, Sinus; cardiac muscle; Cardiomyopathies; case report; complication; coronary angiography; Coronary Thrombosis; diagnostic imaging; echocardiography; heart; heart ventricle function; human; magnetic resonance angiography; male; pathology; Pulmonary Embolism; x-ray computed tomography; Adult; Arrhythmia, Sinus; Cardiomyopathies; Coronary Angiography; Coronary Thrombosis; Echocardiography; Heart; Humans; Magnetic Resonance Angiography; Male; Myocardium; Pulmonary Embolism; Tomography, X-Ray Computed; Ventricular Dysfunction;Background: The diagnostic accuracy of 64-slice MDCT in comparison with IVUS has been poorly described and is mainly restricted to reports analyzing segments with documented atherosclerotic plaques. Objectives: We compared 64-slice multidetector computed tomography (MDCT) with gray scale intravascular ultrasound (IVUS) for the evaluation of coronary lumen dimensions in the context of a comprehensive analysis, including segments with absent or mild disease. Methods: The 64-slice MDCT was performed within 72 h before the IVUS imaging, which was obtained for at least one coronary, regardless of the presence of luminal stenosis at angiography. A total of 21 patients were included, with 70 imaged vessels (total length 114.6 ± 38.3 mm per patient). A coronary plaque was diagnosed in segments with plaque burden > 40%. Results: At patient, vessel, and segment levels, average lumen area, minimal lumen area, and minimal lumen diameter were highly correlated between IVUS and 64-slice MDCT (p < 0.01). However, 64-slice MDCT tended to underestimate the lumen size with a relatively wide dispersion of the differences. The comparison between 64-slice MDCT and IVUS lumen measurements was not substantially affected by the presence or absence of an underlying plaque. In addition, 64-slice MDCT showed good global accuracy for the detection of IVUS parameters associated with flow-limiting lesions. Conclusions: In a comprehensive, multi-territory, and whole-artery analysis, the assessment of coronary lumen by 64-slice MDCT compared with coronary IVUS showed a good overall diagnostic ability, regardless of the presence or absence of underlying atherosclerotic plaques. © 2015, Arquivos Brasileiros de Cardiologia. All rights reserved. Atherosclerotic/diagnosis; Coronary Artery Disease; Multidetector Computed Tomography/utilization; Plaque; Ultrasonography/utilization adult; angiocardiography; Article; atherosclerotic plaque; computer assisted tomography; coronary artery disease; diabetes mellitus; diagnostic accuracy; diagnostic test accuracy study; female; heart rate; human; hypertension; image reconstruction; intravascular ultrasound; major clinical study; male; multidetector computed tomography; percutaneous coronary intervention; receiver operating characteristic; aged; atherosclerotic plaque; comparative study; coronary blood vessel; echography; endoscopic ultrasonography; middle aged; multidetector computed tomography; procedures; radiography; Aged; Coronary Artery Disease; Coronary Vessels; Endosonography; Female; Humans; Male; Middle Aged; Multidetector Computed Tomography; Plaque, Atherosclerotic;Background: Vascular remodeling, the dynamic dimensional change in face of stress, can assume different directions as well as magnitudes in atherosclerotic disease. Classical measurements rely on reference to segments at a distance, risking inappropriate comparison between dislike vessel portions. Objective: to explore a new method for quantifying vessel remodeling, based on the comparison between a given target segment and its inferred normal dimensions. Methods: Geometric parameters and plaque composition were determined in 67 patients using three-vessel intravascular ultrasound with virtual histology (IVUS-VH). Coronary vessel remodeling at cross-section (n = 27.639) and lesion (n = 618) levels was assessed using classical metrics and a novel analytic algorithm based on the fractional vessel remodeling index (FVRI), which quantifies the total change in arterial wall dimensions related to the estimated normal dimension of the vessel. A prediction model was built to estimate the normal dimension of the vessel for calculation of FVRI. Results: According to the new algorithm, “Ectatic” remodeling pattern was least common, “Complete compensatory” remodeling was present in approximately half of the instances, and “Negative” and “Incomplete compensatory” remodeling types were detected in the remaining. Compared to a traditional diagnostic scheme, FVRI-based classification seemed to better discriminate plaque composition by IVUS-VH. Conclusions: Quantitative assessment of coronary remodeling using target segment dimensions offers a promising approach to evaluate the vessel response to plaque growth/regression. © 2015 Arquivos Brasileiros de Cardiologia. All rights reserved. Atherosclerosis/physiopathology; Coronary artery diseases; Neovascularization; Pathologic; Ultrasonography; Vascular remodeling adult; Article; atherosclerosis; cardiovascular risk; coronary bifurcation lesion; coronary remodeling; female; heart catheterization; histology; human; human tissue; intravascular ultrasound; major clinical study; male; mathematical parameters; mathematical phenomena; physical parameters; plaque assay; prospective study; quantitative analysis; radiofrequency identification; reproducibility; transluminal coronary angioplasty; vascular remodeling; aged; algorithm; analysis of variance; atherosclerotic plaque; coronary artery disease; coronary blood vessel; diagnostic imaging; interventional ultrasonography; middle aged; pathology; pathophysiology; physiology; predictive value; reference value; vascular remodeling; Aged; Algorithms; Analysis of Variance; Coronary Artery Disease; Coronary Vessels; Female; Humans; Male; Middle Aged; Plaque, Atherosclerotic; Predictive Value of Tests; Prospective Studies; Reference Values; Reproducibility of Results; Ultrasonography, Interventional; Vascular Remodeling;OBJECTIVES: Coronary artery disease is the leading cause of death in women. The proposed treatments for women are similar to those for men. However, in women with multivessel stable coronary artery disease and normal left ventricular function, the best treatment is unknown.METHODS: A post hoc analysis of the MASS II study with 10 years of follow-up, mean (standard deviation) 6.8 (3.7) years, enrolled between May 1995 and May 2000, evaluated 188 women with chronic stable multivessel coronary artery disease who underwent medical treatment, percutaneous coronary intervention or coronary artery bypass graft surgery. Primary end-points were incidence of total mortality, Q-wave myocardial infarction, or refractory angina. Data were analysed according to the intention-to-treat principle.RESULTS: Women treated with percutaneous coronary intervention and medical treatment had more primary events than those treated with coronary artery bypass graft surgery, respectively, of 34, 44 and 22% (P = 0.003). Survival rates at 10 years were 72% for coronary artery bypass graft surgery, 72% for percutaneous coronary intervention and 56% for medical treatment (P = 0.156). For the composite end-point, Cox regression analysis adjusted for age, diabetes, hypertension, treatment allocation, prior myocardial infarction, smoking, number of vessels affected and total cholesterol, had a higher incidence of primary events with medical treatment than with coronary artery bypass graft surgery [hazard ratio (HR) = 2.38 (95% confidence interval (CI): 1.40-4.05); P = 0.001], a lower incidence with percutaneous coronary intervention than with medical treatment [HR = 0.60 (95% CI: 0.38-0.95); P = 0.031] but no differences between coronary artery bypass graft surgery and percutaneous coronary intervention. Regarding death, a protective effect was observed with percutaneous coronary intervention compared with medical treatment [HR = 0.44 (95% CI: 0.21-0.90); P = 0.025].CONCLUSIONS: Percutaneous coronary intervention and coronary artery bypass graft surgery compared with medical treatment had better results after 10 years of follow-up. © The Author 2014. Published by Oxford University Press on behalf of the European Association for Cardio-Thoracic Surgery. All rights reserved. Coronary artery bypass graft; Coronary disease; Percutaneous coronary intervention; Treatment; Women cardiovascular agent; adverse effects; aged; angina pectoris; chi square distribution; chronic disease; comparative study; complication; controlled study; coronary artery bypass graft; coronary artery disease; female; heart left ventricle function; human; Kaplan Meier method; middle aged; mortality; multivariate analysis; Myocardial Infarction; pathophysiology; percutaneous coronary intervention; proportional hazards model; randomized controlled trial; risk factor; sex difference; survival rate; time factor; treatment outcome; Aged; Angina Pectoris; Cardiovascular Agents; Chi-Square Distribution; Chronic Disease; Coronary Artery Bypass; Coronary Artery Disease; Female; Humans; Kaplan-Meier Estimate; Middle Aged; Multivariate Analysis; Myocardial Infarction; Percutaneous Coronary Intervention; Proportional Hazards Models; Risk Factors; Sex Factors; Survival Rate; Time Factors; Treatment Outcome; Ventricular Function, Left;[No abstract available] Cardiopulmonary resuscitation; Electric countershock; Heart massage; Out-of-hospital cardiac arrest; Ventricular fibrillation creatine kinase MB; streptokinase; troponin; acute heart infarction; adult; automated external defibrillator; cardiopulmonary arrest; case report; compression therapy; consciousness; continuous chest compression; dizziness; endotracheal intubation; fibrinolytic therapy; follow up; human; male; middle aged; note; pneumonia; thorax pain; defibrillator; Out-of-Hospital Cardiac Arrest; procedures; railway; resuscitation; time; treatment outcome; Cardiopulmonary Resuscitation; Defibrillators; Humans; Male; Middle Aged; Out-of-Hospital Cardiac Arrest; Railroads; Time Factors; Treatment Outcome;Aims: To perform a comprehensive evaluation of heart rhythm disorders and the influence of disease/therapy factors in a large systemic lupus erythematosus (SLE) cohort. Methods and results: Three hundred and seventeen consecutive patients of an ongoing electronic database protocol were evaluated by resting electrocardiogram and 142 were randomly selected for 24 h Holter monitoring for arrhythmia and conduction disturbances. The mean age was 40.2 ± 12.1 years and disease duration was 11.4 ± 8.1 years. Chloroquine (CQ) therapy was identified in 69.7 with a mean use of 8.5 ± 6.7 years. Electrocardiogram abnormalities were detected in 66 patients (20.8): prolonged QTc/QTd (14.2); bundle-branch block (2.5); and atrioventricular block (AVB) (1.6). Age was associated with AVB (P 0.029) and prolonged QTc/QTd (P 0.039) whereas anti-Ro/SS-A and Systemic Lupus Erythematosus Disease Activity Index (SLEDAI) scores were not (P > 0.05). Chloroquine was negatively associated with AVB (P 0.01) as was its longer use (6.1 ± 6.9 vs. 1.0 ± 2.5 years, P 0.018). Time of CQ use was related with the absence of AVB [odds ratio (OR) 0.103; 95 confidence interval (CI) 0.011-0.934, P 0.043] in multiple logistic regression. Holter monitoring revealed abnormalities in 121 patients (85.2): supraventricular ectopies (63.4) and tachyarrhythmia (18.3); ventricular ectopies (45.8). Atrial tachycardia/fibrillation (AT/AF) were associated with shorter CQ duration (7.05 ± 7.99 vs. 3.63 ± 5.02 years, P 0.043) with a trend to less CQ use (P 0.054), and older age (P < 0.001). Predictors of AT/AF in multiple logistic regression were age (OR 1.115; 95 CI 1.059-1.174, P < 0.001) and anti-Ro/SS-A (OR 0.172; 95 CI 0.047-0.629, P 0.008). Conclusions: Chloroquine seems to play a protective role in the unexpected high rate of cardiac arrhythmias and conduction disturbances observed in SLE. Further studies are necessary to determine if this antiarrhythmic effect is due to the disease control or a direct effect of the drug. All rights reserved. © The Author 2013. Antimalarial; Arrhythmia; Safety; Systemic lupus erythematosus; Treatment antiarrhythmic agent; chloroquine; antiarrhythmic agent; antirheumatic agent; cardiotonic agent; chloroquine; adult; age distribution; antiarrhythmic activity; article; atrioventricular block; clinical protocol; cohort analysis; data base; descriptive research; disease association; disease duration; drug safety; ectopia cordis; electrocardiogram; female; heart atrium fibrillation; heart bundle branch block; heart ventricle extrasystole; Holter monitoring; human; major clinical study; male; observational study; open study; priority journal; QT prolongation; SLEDAI; supraventricular tachycardia; systemic lupus erythematosus; tachycardia; Arrhythmias, Cardiac; Brazil; comorbidity; drug effects; electrocardiography; feasibility study; Lupus Erythematosus, Systemic; off label drug use; prevalence; retrospective study; risk assessment; statistics and numerical data; treatment outcome; Adult; Anti-Arrhythmia Agents; Antirheumatic Agents; Arrhythmias, Cardiac; Brazil; Cardiotonic Agents; Causality; Chloroquine; Comorbidity; Electrocardiography; Feasibility Studies; Female; Humans; Lupus Erythematosus, Systemic; Male; Off-Label Use; Prevalence; Retrospective Studies; Risk Assessment; Treatment Outcome;Background: Data from over 4 decades have reported a higher incidence of silent infarction among patients with diabetes mellitus (DM), but recent publications have shown conflicting results regarding the correlation between DM and presence of pain in patients with acute coronary syndromes (ACS).Objective: Our primary objective was to analyze the association between DM and precordial pain at hospital arrival. Secondary analyses evaluated the association between hyperglycemia and precordial pain at presentation, and the subgroup of patients presenting within 6 hours of symptom onset.Methods: We analyzed a prospectively designed registry of 3,544 patients with ACS admitted to a Coronary Care Unit of a tertiary hospital. We developed multivariable models to adjust for potential confounders.Results: Patients with precordial pain were less likely to have DM (30.3%) than those without pain (34.0%; unadjusted p = 0.029), but this difference was not significant after multivariable adjustment, for the global population (p = 0.84), and for subset of patients that presented within 6 hours from symptom onset (p = 0.51). In contrast, precordial pain was more likely among patients with hyperglycemia (41.2% vs 37.0% without hyperglycemia, p = 0.035) in the overall population and also among those who presented within 6 hours (41.6% vs. 32.3%, p = 0.001). Adjusted models showed an independent association between hyperglycemia and pain at presentation, especially among patients who presented within 6 hours (OR = 1.41, p = 0.008).Conclusion: In this non-selected ACS population, there was no correlation between DM and hospital presentation without precordial pain. Moreover, hyperglycemia correlated significantly with pain at presentation, especially in the population that arrived within 6 hours from symptom onset. © 2014, Arquivos Brasileiros de Cardiologia. All rights reserved. Acute coronary syndrome; Chest pain; Diabetes mellitus; Hyperglycemia creatine kinase; troponin I; acute coronary syndrome; acute heart infarction; adult; aged; angina pectoris; Article; cerebrovascular accident; coronary artery bypass graft; coronary artery disease; correlational study; death; diabetes mellitus; disease association; female; glucose blood level; heart failure; human; hypercholesterolemia; hyperglycemia; hypertension; major clinical study; male; percutaneous coronary intervention; precordial pain; prospective study; risk factor; ST segment elevation myocardial infarction; acute coronary syndrome; Chest Pain; diabetic cardiomyopathy; hospital admission; middle aged; mortality; multivariate analysis; nonparametric test; pain threshold; pathophysiology; physiology; time; Acute Coronary Syndrome; Aged; Chest Pain; Diabetic Cardiomyopathies; Female; Hospital Mortality; Humans; Male; Middle Aged; Multivariate Analysis; Pain Threshold; Patient Admission; Risk Factors; Statistics, Nonparametric; Time Factors;OBJECTIVES The aim of this study was to evaluate the impact of intravascular ultrasound (IVUS) guidance on the final volume of contrast agent used in patients undergoing percutaneous coronary intervention (PCI). BACKGROUND To date, few approaches have been described to reduce the final dose of contrast agent in PCIs. We hypothesized that IVUS might serve as an alternative imaging tool to angiography in many steps during PCI, thereby reducing the use of iodine contrast. METHODS A total of 83 patients were randomized to angiography-guided PCI or IVUS-guided PCI; both groups were treated according to a pre-defined meticulous procedural strategy. The primary endpoint was the total volume contrast agent used during PCI. Patients were followed clinically for an average of 4 months. RESULTS The median total volume of contrast was 64.5 ml (interquartile range [IQR]: 42.8 to 97.0 ml; minimum, 19 ml; maximum, 170 ml) in the angiography-guided group versus 20.0 ml (IQR: 12.5 to 30.0 ml; minimum, 3 ml; maximum, 54 ml) in the IVUS-guided group (p < 0.001). Similarly, the median volume of contrast/creatinine clearance ratio was significantly lower among patients treated with IVUS-guided PCI (1.0 [IQR: 0.6 to 1.9] vs. 0.4 [IQR: 0.2 to 0.6, respectively; p < 0.001). In-hospital and 4-month outcomes were not different between patients randomized to angiography-guided and IVUS-guided PCI. CONCLUSIONS Thoughtful and extensive use of IVUS as the primary imaging tool to guide PCI is safe and markedly reduces the volume of iodine contrast compared with angiography-alone guidance. The use of IVUS should be considered for patients at high risk of contrast-induced acute kidney injury or volume overload undergoing coronary angioplasty. © 2014 By The American College of Cardiology Foundation Published by Elsevier Inc. Contrast; Coronary intravascular ultrasound; Renal failure; Stent creatinine; iodixanol; iopromide; biological marker; contrast medium; creatinine; iodixanol; iodobenzoic acid derivative; iohexol; iopromide; adult; aged; angiography; Article; contrast induced acute kidney injury; contrast induced nephropathy; controlled study; creatinine clearance; female; human; intravascular ultrasound; major clinical study; male; outcome assessment; percutaneous coronary intervention; priority journal; randomized controlled trial; treatment outcome; ultrasound scanner; Acute Kidney Injury; adverse effects; analogs and derivatives; angiocardiography; blood; chemically induced; comparative study; coronary artery disease; diagnostic use; echography; endoscopic echography; middle aged; percutaneous coronary intervention; procedures; radiography; risk factor; time; Acute Kidney Injury; Aged; Biological Markers; Contrast Media; Coronary Angiography; Coronary Artery Disease; Creatinine; Female; Humans; Iohexol; Male; Middle Aged; Percutaneous Coronary Intervention; Risk Factors; Time Factors; Treatment Outcome; Triiodobenzoic Acids; Ultrasonography, Interventional;Introduction: We evaluated the safety and efficacy of protamine administration, guided by activated clotting time, for the immediate femoral arterial sheath removal in patients undergoing percutaneous coronary intervention with unfractionated heparin in order to propose an algorithm for clinical practice. Methods: Prospective study with consecutive patients with stable angina or low-to-moderate risk acute coronary syndrome. We compared patients with an early removal of the arterial sheath to those whose sheath removal was based on a standard protocol. Results: The early removal group (n = 149) had lower access manipulation time than the conventional group (58.3 ± 21.4 minutes vs. 355.0 ± 62.9 minutes; p < 0.01), mainly due to a reduced time to sheath removal (42.3 ± 21.1 minutes vs. 338.6 ± 61.5 minutes; p < 0.01), with no impact on the duration of femoral compression (16.0 ± 3.6 minutes vs. 16.4 ± 5.1 minutes; p = 0.49). There was no stent thrombosis during hospitalization and no significant differences in the incidence of major vascular or bleeding events. The incidence of other bleeding events leading to prolonged hospitalization time was lower in the group with early removal (1.3% vs. 5.1%; p = 0.05). Conclusions: The selective use of an approach for immediate femoral sheath removal, based on activated clotting time guidance and protamine administration, is safe and effective in patients undergoing percutaneous coronary intervention by femoral access. Anticoagulants; Femoral artery; Heparin; Percutaneous coronary intervention; Protamines; Radial artery protamine; acute coronary syndrome; article; bleeding; blood clotting time; controlled study; femoral artery; human; length of stay; major clinical study; nursing protocol; patient safety; percutaneous coronary intervention; prospective study; "
Ludhmila Abrahão Hajjar,Universidade de São Paulo - Faculdade de Medicina,"Objective: Intra-aortic balloon pump (IABP) is commonly used as mechanical support after cardiac surgery or cardiac shock. Although its benefits for cardiac function have been well documented, its effects on cerebral circulation are still controversial. We hypothesized that transfer function analysis (TFA) and continuous estimates of dynamic cerebral autoregulation (CA) provide consistent results in the assessment of cerebral autoregulation in patients with IABP. Approach: Continuous recordings of blood pressure (BP, intra-arterial line), end-tidal CO2, heart rate and cerebral blood flow velocity (CBFV, transcranial Doppler) were obtained (i) 5 min with IABP ratio 1:3, (ii) 5 min, starting 1 min with the IABP-ON, and continuing for another 4 min without pump assistance (IABP-OFF). Autoregulation index (ARI) was estimated from the CBFV response to a step change in BP derived by TFA and as a function of time using an autoregressive moving-average model during removal of the device (ARIt). Critical closing pressure and resistance area-product were also obtained. Main results: ARI with IABP-ON (4.3 1.2) were not different from corresponding values at IABP-OFF (4.7 1.4, p = 0.42). Removal of the balloon had no effect on ARIt, CBFV, BP, cerebral critical closing pressure or resistance area-product. Significance: IABP does not disturb cerebral hemodynamics. TFA and continuous estimates of dynamic CA can be used to assess cerebral hemodynamics in patients with IABP. These findings have important implications for the design of studies of critically ill patients requiring the use of different invasive support devices. © 2017 Institute of Physics and Engineering in Medicine. autoregulation index; cerebral blood flow velocity; dynamic cerebral autoregulation; intra-aortic balloon pump; transcranial doppler ultrasound ;Objective: To assess whether a restrictive strategy of RBC transfusion reduces 28-day mortality when compared with a liberal strategy in cancer patients with septic shock. Design: Single center, randomized, double-blind controlled trial. Setting: Teaching hospital. Patients: Adult cancer patients with septic shock in the first 6 hours of ICU admission. Interventions: Patients were randomized to the liberal (hemoglobin threshold, < 9 g/dL) or to the restrictive strategy (hemoglobin threshold, < 7 g/dL) of RBC transfusion during ICU stay. Measurements and Main Results: Patients were randomized to the liberal (n = 149) or to the restrictive transfusion strategy (n = 151) group. Patients in the liberal group received more RBC units than patients in the restrictive group (1 [0-3] vs 0 [0-2] unit; p < 0.001). At 28 days after randomization, mortality rate in the liberal group (primary endpoint of the study) was 45% (67 patients) versus 56% (84 patients) in the restrictive group (hazard ratio, 0.74; 95% CI, 0.53-1.04; p = 0.08) with no differences in ICU and hospital length of stay. At 90 days after randomization, mortality rate in the liberal group was lower (59% vs 70%) than in the restrictive group (hazard ratio, 0.72; 95% CI, 0.53-0.97; p = 0.03). Conclusions: We observed a survival trend favoring a liberal transfusion strategy in patients with septic shock when compared with the restrictive strategy. These results went in the opposite direction of the a priori hypothesis and of other trials in the field and need to be confirmed. © 2017 by the Society of Critical Care Medicine and Wolters Kluwer Health, Inc. All Rights Reserved. Critically ill oncology; Intensive care; Randomized controlled trial; Transfusion aged; controlled study; critical illness; double blind procedure; erythrocyte transfusion; female; human; intensive care; intensive care unit; length of stay; middle aged; mortality; Neoplasms; procedures; proportional hazards model; randomized controlled trial; severity of illness index; Shock, Septic; statistics and numerical data; time factor; university hospital; Aged; Critical Care; Critical Illness; Double-Blind Method; Erythrocyte Transfusion; Female; Hospitals, University; Humans; Intensive Care Units; Length of Stay; Middle Aged; Neoplasms; Proportional Hazards Models; Severity of Illness Index; Shock, Septic; Time Factors;IMPORTANCE Perioperative lung-protective ventilation has been recommended to reduce pulmonary complications after cardiac surgery. The protective role of a small tidal volume (VT) has been established, whereas the added protection afforded by alveolar recruiting strategies remains controversial. OBJECTIVE To determine whether an intensive alveolar recruitment strategy could reduce postoperative pulmonary complications, when added to a protective ventilation with small VT. DESIGN, SETTING, AND PARTICIPANTS Randomized clinical trial of patients with hypoxemia after cardiac surgery at a single ICU in Brazil (December 2011-2014). INTERVENTIONS Intensive recruitment strategy (n=157) or moderate recruitment strategy (n=163) plus protective ventilation with small VT. MAIN OUTCOMES AND MEASURES Severity of postoperative pulmonary complications computed until hospital discharge, analyzed with a common odds ratio (OR) to detect ordinal shift in distribution of pulmonary complication severity score (0-To-5 scale, 0, no complications; 5, death). Prespecified secondary outcomes were length of stay in the ICU and hospital, incidence of barotrauma, and hospital mortality. RESULTS All 320 patients (median age, 62 years; IQR, 56-69 years; 125 women [39%]) completed the trial. The intensive recruitment strategy group had a mean 1.8 (95%CI, 1.7 to 2.0) and a median 1.7 (IQR, 1.0-2.0) pulmonary complications score vs 2.1 (95%CI, 2.0-2.3) and 2.0 (IQR, 1.5-3.0) for the moderate strategy group. Overall, the distribution of primary outcome scores shifted consistently in favor of the intensive strategy, with a common OR for lower scores of 1.86 (95%CI, 1.22 to 2.83; P = .003). The mean hospital stay for the moderate group was 12.4 days vs 10.9 days in the intensive group (absolute difference, 1.5 days; 95% CI, 3.1 to 0.3; P = .04). The mean ICU stay for the moderate group was 4.8 days vs 3.8 days for the intensive group (absolute difference, 1.0 days; 95%CI, 1.6 to 0.2; P = .01). Hospital mortality (2.5%in the intensive group vs 4.9% in the moderate group; absolute difference, 2.4%, 95%CI, 7.1%to 2.2%) and barotrauma incidence (0% in the intensive group vs 0.6% in the moderate group; absolute difference, 0.6%; 95%CI, 1.8%to 0.6%; P = .51) did not differ significantly between groups. CONCLUSIONS AND RELEVANCE Among patients with hypoxemia after cardiac surgery, the use of an intensive vs a moderate alveolar recruitment strategy resulted in less severe pulmonary complications while in the hospital.  adult; aged; Article; artificial ventilation; barotrauma; Brazil; continuous ventilator; controlled study; coronary artery bypass graft; disease severity; female; heart valve surgery; hospital mortality; human; hypoxemia; intensive alveolar recruitment strategy; intensive care unit; length of stay; lung disease; lung protective ventilation; major clinical study; male; moderate alveolar recruitment strategy; postoperative complication; randomized controlled trial; therapy effect; tidal volume; adverse effects; artificial ventilation; barotrauma; blood pressure; heart rate; heart surgery; hypoxia; incidence; intensive care; lung alveolus; Lung Diseases; middle aged; odds ratio; oxygen therapy; partial pressure; physiology; positive end expiratory pressure; Postoperative Complications; procedures; severity of illness index; statistics and numerical data; Aged; Barotrauma; Blood Pressure; Cardiac Surgical Procedures; Critical Care; Female; Heart Rate; Hospital Mortality; Humans; Hypoxia; Incidence; Length of Stay; Lung Diseases; Male; Middle Aged; Odds Ratio; Oxygen Inhalation Therapy; Partial Pressure; Positive-Pressure Respiration; Postoperative Complications; Pulmonary Alveoli; Respiration, Artificial; Severity of Illness Index; Tidal Volume;Objective Of the 230 million patients undergoing major surgical procedures every year, more than 1 million will die within 30 days. Thus, any nonsurgical interventions that help reduce perioperative mortality might save thousands of lives. The authors have updated a previous consensus process to identify all the nonsurgical interventions, supported by randomized evidence, that may help reduce perioperative mortality. Design and Setting A web-based international consensus conference. Participants The study comprised 500 clinicians from 61 countries. Interventions A systematic literature search was performed to identify published literature about nonsurgical interventions, supported by randomized evidence, showing a statistically significant impact on mortality. A consensus conference of experts discussed eligible papers. The interventions identified by the conference then were submitted to colleagues worldwide through a web-based survey. Measurements and Main Results The authors identified 11 interventions contributing to increased survival (perioperative hemodynamic optimization, neuraxial anesthesia, noninvasive ventilation, tranexamic acid, selective decontamination of the gastrointestinal tract, insulin for tight glycemic control, preoperative intra-aortic balloon pump, leuko-depleted red blood cells transfusion, levosimendan, volatile agents, and remote ischemic preconditioning) and 2 interventions showing increased mortality (beta-blocker therapy and aprotinin). Interventions then were voted on by participating clinicians. Percentages of agreement among clinicians in different countries differed significantly for 6 interventions, and a variable gap between evidence and clinical practice was noted. Conclusions The authors identified 13 nonsurgical interventions that may decrease or increase perioperative mortality, with variable agreement by clinicians. Such interventions may be optimal candidates for investigation in high-quality trials and discussion in international guidelines to reduce perioperative mortality. © 2017 Elsevier Inc. anesthesia; consensus; intensive care; mortality; perioperative care ;Preliminary evidence suggests that statins may prevent major perioperative vascular complications. Methods We randomized 648 statin-naïve patients who were scheduled for noncardiac surgery and were at risk for a major vascular complication. Patients were randomized to a loading dose of atorvastatin or placebo (80 mg anytime within 18 hours before surgery), followed by a maintenance dose of 40 mg (or placebo), started at least 12 hours after the surgery, and then 40 mg/d (or placebo) for 7 days. The primary outcome was a composite of all-cause mortality, nonfatal myocardial injury after noncardiac surgery, and stroke at 30 days. Results The primary outcome was observed in 54 (16.6%) of 326 patients in the atorvastatin group and 59 (18.7%) of 316 patients in the placebo group (hazard ratio [HR] 0.87, 95% CI 0.60-1.26, P = .46). No significant effect was observed on the 30-day secondary outcomes of all-cause mortality (4.3% vs 4.1%, respectively; HR 1.14, 95% CI 0.53-2.47, P = .74), nonfatal myocardial infarction (3.4% vs 4.4%, respectively; HR 0.76, 95% CI 0.35-1.68, P = .50), myocardial injury after noncardiac surgery (13.2% vs 16.5%; HR 0.79, 95% CI 0.53-1.19, P = .26), and stroke (0.9% vs 0%, P = .25). Conclusion In contrast to the prior observational and trial data, the LOAD trial has neutral results and did not demonstrate a reduction in major cardiovascular complications after a short-term perioperative course of statin in statin-naïve patients undergoing noncardiac surgery. We demonstrated, however, that a large multicenter blinded perioperative statin trial for high-risk statin-naïve patients is feasible and should be done to definitely establish the efficacy and safety of statin in this patient population. © 2016  acetylsalicylic acid; atorvastatin; beta adrenergic receptor blocking agent; calcium antagonist; clonidine; clopidogrel; dipeptidyl carboxypeptidase inhibitor; fibric acid derivative; insulin; low molecular weight heparin; nitric acid derivative; placebo; warfarin; atorvastatin; hydroxymethylglutaryl coenzyme A reductase inhibitor; troponin; aged; Article; cerebrovascular accident; controlled study; drug efficacy; drug safety; female; heart muscle injury; high risk population; human; loading drug dose; major clinical study; major surgery; male; mortality; multicenter study; outcome assessment; postoperative complication; priority journal; randomized controlled trial; blood; clinical trial; electrocardiography; middle aged; Myocardial Infarction; Myocardial Ischemia; perioperative period; Postoperative Complications; procedures; proportional hazards model; risk assessment; Stroke; surgery; Aged; Atorvastatin Calcium; Electrocardiography; Female; Humans; Hydroxymethylglutaryl-CoA Reductase Inhibitors; Male; Middle Aged; Myocardial Infarction; Myocardial Ischemia; Perioperative Care; Postoperative Complications; Proportional Hazards Models; Risk Assessment; Stroke; Surgical Procedures, Operative; Troponin;Neuroendocrine tumours are a heterogeneous group of diseases with a significant variety of diagnostic tests and treatment modalities. Guidelines were developed by North American and European groups to recommend their best management. However, local particularities and relativisms found worldwide led us to create Brazilian guidelines. Our consensus considered the best feasible strategies in an environment involving more limited resources. We believe that our recommendations may be extended to other countries with similar economic standards. © 2017 the authors. Cancer; Chemotherapy; Guideline; Neuroendcrine tumours; Radionuclide peptide therapy; Targeted therapy 5 hydroxyindoleacetic acid; alpha interferon; chromogranin A; everolimus; fluorine 18; fluorodeoxyglucose; gallium 68; gastrin; insulin; radioisotope; somatostatin derivative; sunitinib; bone metastasis; Brazilian; cancer chemotherapy; cancer of unknown primary site; carcinoid syndrome; chemoembolization; computer assisted tomography; cytoreductive surgery; diagnostic imaging; duodenum tumor; endoscopic ultrasonography; follow up; gastrointestinal tumor; glycemic control; histopathology; human; inoperable cancer; insulinoma; liver graft; liver metastasis; molecularly targeted therapy; neuroendocrine tumor; pancreas tumor; positron emission tomography; practice guideline; radioembolization; radiofrequency ablation; Review; tumor biopsy; watchful waiting; whole body MRI;Venous thromboembolism (VTE) includes deep vein thrombosis (DVT) and/or pulmonary embolism (PE). Many surgeons and clinicians believe that VTE after coronary artery bypass grafting (CABG) has little clinical significance because it is seldom diagnosed. This study aimed to identify VTE after CABG, independent of clinical suspicion. In this prospective, observational, single-center study, 100 patients underwent computed tomographic pulmonary angiography (multidetector-64) and lower extremity venous compressive ultrasound after elective CABG. Patients with high risk for VTE were excluded. Aspirin was maintained throughout the preoperative and postoperative periods, and early ambulation was encouraged. Postoperatively, no mechanical or heparin prophylaxis was used in any patients. At the discretion of the surgeons, 83 surgeries were on-pump, and 17 were off-pump. On average, tomography and ultrasound were performed 7 ± 3 days after CABG. Isolated PE was observed in 13 of 100 patients (13%), simultaneous PE and DVT in 8 of 100 (8%), and isolated DVT in 4 of 100 (4%), thus totaling 25/100 VTEs (25%). Of the 21 PEs, 3 of 21 (14%) involved subsegmental, 15 of 21 (71%) segmental, 1 of 21 (5%) lobar, and 2 of 21 (10%) central pulmonary arteries. Of the 12 DVTs, all were distal (below the popliteal vein), and 2 of 12 (17%) were also proximal; 5 of 12 (42%) were unilateral, of which 3 of 5 (60%) on the contralateral saphenous vein-harvested leg. No VTE caused hemodynamic instability, and none was clinically suspected. In conclusion, VTEs were frequent, some extensive proximal VTEs occurred, but most were distally localized. Many patients in this series would have been discharged without diagnosis of and treatment for PE and/or DVT. © 2016 Elsevier Inc.  acetylsalicylic acid; adult; aged; Article; body mass; cardiac patient; cardiopulmonary bypass; cardiovascular risk; computed tomographic angiography; computed tomography scanner; coronary artery bypass graft; deep vein thrombosis; female; hospitalization; human; intensive care unit; lung angiography; lung embolism; major clinical study; male; multidetector computed tomography; observational study; off pump coronary surgery; postoperative complication; postoperative period; preoperative period; priority journal; prospective study; surgical patient; thrombosis prevention; adverse effects; coronary artery bypass graft; coronary artery disease; diagnostic imaging; echography; middle aged; Pulmonary Embolism; Venous Thrombosis; Aged; Computed Tomography Angiography; Coronary Artery Bypass; Coronary Artery Disease; Female; Humans; Male; Middle Aged; Prospective Studies; Pulmonary Embolism; Ultrasonography; Venous Thrombosis;Patients with ischemic heart failure (iHF) have a high risk of neurological complications such as cognitive impairment and stroke. We hypothesized that iHF patients have a higher incidence of impaired dynamic cerebral autoregulation (dCA). Adult patients with iHF and healthy volunteers were included. Cerebral blood flow velocity (CBFV, transcranial Doppler, middle cerebral artery), end-tidal CO2 (capnography), and arterial blood pressure (Finometer) were continuously recorded supine for 5 min at rest. Autoregulation index (ARI) was estimated from the CBFV step response derived by transfer function analysis using standard template curves. Fifty-two iHF patients and 54 age-, gender-, and BP-matched healthy volunteers were studied. Echocardiogram ejection fraction was 40 (20–45) % in iHF group. iHF patients compared with control subjects had reduced end-tidal CO2 (34.1 ± 3.7 vs. 38.3 ± 4.0 mmHg, P ± 0.001) and lower ARI values (5.1 ± 1.6 vs. 5.9 ± 1.0, P ± 0.012). ARI &lt; 4, suggestive of impaired CA, was more common in iHF patients (28.8 vs. 7.4%, P ± 0.004). These results confirm that iHF patients are more likely to have impaired dCA compared with age-matched controls. The relationship between impaired dCA and neurological complications in iHF patients deserves further investigation. © 2017 the American Physiological Society. Autoregulation index; Cerebral blood flow; Dynamic cerebral autoregulation; Transcranial Doppler; Transfer function analysis adult; arterial pressure; Article; autoregulation; blood flow velocity; brain blood flow; capnometry; end tidal carbon dioxide tension; female; heart ejection fraction; heart failure; human; ischemic heart disease; ischemic heart failure; major clinical study; male; middle aged; middle cerebral artery; priority journal; transcranial Doppler ultrasonography; brain circulation; Cerebrovascular Disorders; complication; heart failure; heart muscle ischemia; homeostasis; pathophysiology; Blood Flow Velocity; Cerebrovascular Circulation; Cerebrovascular Disorders; Female; Heart Failure; Homeostasis; Humans; Male; Middle Aged; Myocardial Ischemia;Background: Vasoplegic syndrome is a common complication after cardiac surgery and impacts negatively on patient outcomes. The objective of this study was to evaluate whether vasopressin is superior to norepinephrine in reducing postoperative complications in patients with vasoplegic syndrome. Methods: This prospective, randomized, double-blind trial was conducted at the Heart Institute, University of Sao Paulo, Sao Paulo, Brazil, between January 2012 and March 2014. Patients with vasoplegic shock (defined as mean arterial pressure less than 65 mmHg resistant to fluid challenge and cardiac index greater than 2.2 l · min -2 · m -2) after cardiac surgery were randomized to receive vasopressin (0.01 to 0.06 U/min) or norepinephrine (10 to 60 μg/min) to maintain arterial pressure. The primary endpoint was a composite of mortality or severe complications (stroke, requirement for mechanical ventilation for longer than 48 h, deep sternal wound infection, reoperation, or acute renal failure) within 30 days. Results: A total of 330 patients were randomized, and 300 were infused with one of the study drugs (vasopressin, 149; norepinephrine, 151). The primary outcome occurred in 32% of the vasopressin patients and in 49% of the norepinephrine patients (unadjusted hazard ratio, 0.55; 95% CI, 0.38 to 0.80; P = 0.0014). Regarding adverse events, the authors found a lower occurrence of atrial fibrillation in the vasopressin group (63.8% vs. 82.1%; P = 0.0004) and no difference between groups in the rates of digital ischemia, mesenteric ischemia, hyponatremia, and myocardial infarction. Conclusions: The authors' results suggest that vasopressin can be used as a first-line vasopressor agent in postcardiac surgery vasoplegic shock and improves clinical outcomes. © 2016 the American Society of Anesthesiologists, Inc. Wolters Kluwer Health, Inc. All Rights Reserved.  noradrenalin; vasoconstrictor agent; vasopressin derivative; Brazil; comparative study; complication; controlled study; double blind procedure; female; heart surgery; human; male; middle aged; Postoperative Complications; prospective study; randomized controlled trial; shock; treatment outcome; vasoplegia; Brazil; Cardiac Surgical Procedures; Double-Blind Method; Female; Humans; Male; Middle Aged; Norepinephrine; Postoperative Complications; Prospective Studies; Shock; Treatment Outcome; Vasoconstrictor Agents; Vasoplegia; Vasopressins;Introduction: A recently published study raised doubts about the need for percutaneous treatment of nonculprit lesions in patients with acute coronary syndromes (ACS). Methods: Retrospective, unicentric, observational study. Objective: To analyze the long-term outcomes in patients undergoing treatment of the culprit artery, comparing those who remained with significant residual lesions in nonculprit arteries (group I) versus those without residual lesions in other coronary artery beds (group II). The study included 580 patients (284 in group I and 296 in group II) between May 2010 and May 2013. We obtained demographic and clinical data, as well as information regarding the coronary treatment administered to the patients. In the statistical analysis, the primary outcome included combined events (reinfarction/ angina, death, heart failure, and need for reintervention). The comparison between groups was performed using the chisquare test and ANOVA. The long-term analysis was conducted with the Kaplan-Meier method, with a mean follow-up of 9.86 months. Results: The mean ages were 63 years in group I and 62 years in group II. On long-term follow-up, there was no significant difference in combined events in groups I and II (31.9% versus 35.6%, respectively, p = 0.76). Conclusion: The strategy of treating the culprit artery alone seems safe. In this study, no long-term differences in combined endpoints were observed between patients who remained with significant lesions compared with those without other obstructions. © 2016, Arquivos Brasileiros de Cardiologia. All rights reserved. Acute coronary syndrome; Clinical evolution; Long term; Memory; Myocardial infarction; Treatment acetylsalicylic acid; beta adrenergic receptor blocking agent; clopidogrel; dipeptidyl carboxypeptidase inhibitor; enoxaparin; hydroxymethylglutaryl coenzyme A reductase inhibitor; tirofiban; troponin; acute coronary syndrome; adult; analysis of variance; Article; comparative study; controlled study; coronary artery bypass graft; creatinine blood level; demography; evaluation and follow up; evolution; female; heart left ventricle ejection fraction; heart reinfarction; hospitalization; human; Kaplan Meier method; long term care; major clinical study; male; middle aged; observational study; outcome assessment; retrospective study; systolic blood pressure; thorax pain; acute coronary syndrome; aged; disease course; heart muscle revascularization; mortality; nonparametric test; percutaneous coronary intervention; procedures; risk factor; time factor; treatment outcome; Acute Coronary Syndrome; Aged; Analysis of Variance; Disease Progression; Female; Humans; Male; Middle Aged; Myocardial Revascularization; Percutaneous Coronary Intervention; Retrospective Studies; Risk Factors; Statistics, Nonparametric; Time Factors; Treatment Outcome;[No abstract available]  anesthesia; anesthesiology; Brazil; consensus; fluid therapy; hemodynamics; human; meta analysis; motivation; practice guideline; procedures; standards; surgery; Anesthesia; Anesthesiology; Brazil; Consensus; Fluid Therapy; Goals; Guidelines as Topic; Hemodynamics; Humans; Surgical Procedures, Operative;Objectives Democracy-based medicine is a combination of evidence-based medicine (systematic review), expert assessment, and worldwide voting by physicians to express their opinions and self-reported practice via the Internet. The authors applied democracy-based medicine to key trials in critical care medicine. Design and Setting A systematic review of literature followed by web-based voting on findings of a consensus conference. Participants A total of 555 clinicians from 61 countries. Interventions The authors performed a systematic literature review (via searching MEDLINE/PubMed, Scopus, and Embase) and selected all multicenter randomized clinical trials in critical care that reported a significant effect on survival and were endorsed by expert clinicians. Then they solicited voting and self-reported practice on such evidence via an interactive Internet questionnaire. Relationships among trial sample size, design, and respondents’ agreement were investigated. The gap between agreement and use/avoidance and the influence of country origin on physicians’ approach to interventions also were investigated. Measurements and Main Results According to 24 multicenter randomized controlled trials, 15 interventions affecting mortality were identified. Wide variabilities in both the level of agreement and reported practice among different interventions and countries were found. Moreover, agreement and reported practice often did not coincide. Finally, a positive correlation among agreement, trial sample size, and number of included centers was found. On the contrary, trial design did not influence clinicians’ agreement. Conclusions Physicians’ clinical practice and agreement with the literature vary among different interventions and countries. The role of these interventions in affecting survival should be further investigated to reduce both the gap between evidence and clinical practice and transnational differences. © 2016 Elsevier Inc. anesthesia; consensus conference; critically ill; intensive care; mortality; survival ;[No abstract available]  Article; assisted circulation; Brazilian Society of Cardiology; medical society; practice guideline; assisted circulation; Brazil; devices; extracorporeal oxygenation; heart assist device; heart failure; human; medical society; practice guideline; procedures; risk factor; standards; Assisted Circulation; Brazil; Extracorporeal Membrane Oxygenation; Heart Failure; Heart-Assist Devices; Humans; Risk Factors; Societies, Medical;Objectives: To assess the long-term survival, health-related quality of life, and quality-adjusted life years of cancer patients admitted to ICUs. Design: Prospective cohort. Setting: Two cancer specialized ICUs in Brazil. Patients: A total of 792 participants. Interventions: None. Measurements and Main Results: The health-related quality of life before ICU admission; at 15 days; and at 3, 6, 12, and 18 months was assessed with the EQ-5D-3L. In addition, the vital status was assessed at 24 months. The mean age of the subjects was 61.6 ± 14.3 years, 42.5% were female subjects and half were admitted after elective surgery. The mean Simplified Acute Physiology Score 3 was 47.4 ± 15.6. Survival at 12 and 18 months was 42.4% and 38.1%, respectively. The mean EQ-5D-3L utility measure before admission to the ICU was 0.47 ± 0.43, at 15 days it was 0.41 ± 0.44, at 90 days 0.56 ± 0.42, at 6 months 0.60 ± 0.41, at 12 months 0.67 ± 0.35, and at 18 months 0.67 ± 0.35. The probabilities for attaining 12 and 18 months of quality-adjusted survival were 30.1% and 19.1%, respectively. There were statistically significant differences in survival time and quality-adjusted life years according to all assessed baseline characteristics (ICU admission after elective surgery, emergency surgery, or medical admission; Simplified Acute Physiology Score 3; cancer extension; cancer status; previous surgery; previous chemotherapy; previous radiotherapy; performance status; and previous health-related quality of life). Only the previous health-related quality of life and performance status were associated with the health-related quality of life during the 18-month follow-up. Conclusions: Long-term survival, health-related quality of life, and quality-adjusted life year expectancy of cancer patients admitted to the ICU are limited. Nevertheless, these clinical outcomes exhibit wide variability among patients and are associated with simple characteristics present at the time of ICU admission, which may help healthcare professionals estimate patients' prognoses. © 2016 by the Society of Critical Care Medicine and Wolters Kluwer Health, Inc. cancer; cohort study; critically ill patients; health-related quality of life; intensive care unit; quality-adjusted life year antineoplastic agent; adult; advanced cancer; Article; brain cancer; breast cancer; cancer chemotherapy; cancer patient; cancer radiotherapy; cancer surgery; cancer survival; central nervous system tumor; cohort analysis; controlled clinical trial; controlled study; critically ill patient; digestive system cancer; elective surgery; emergency surgery; female; genital tract cancer; hospital admission; human; length of stay; leukemia; long term survival; lymphoma; major clinical study; male; malignant neoplastic disease; metastasis; mouth cancer; multicenter study; myeloma; patient history of chemotherapy; patient history of radiotherapy; patient history of tumorectomy; pharynx cancer; primary tumor; priority journal; prospective study; quality adjusted life year; respiratory tract cancer; Simplified Acute Physiology Score; surgical patient; survival time; urinary tract cancer; aged; Brazil; critical illness; epidemiology; health status; intensive care unit; middle aged; mortality; neoplasm; quality of life; survival analysis; Aged; Brazil; Critical Illness; Female; Health Status; Humans; Intensive Care Units; Male; Middle Aged; Neoplasms; Patient Admission; Prospective Studies; Quality of Life; Quality-Adjusted Life Years; Survival Analysis;This study aimed to describe severe infections with extensively drug-resistant Acinetobacter baumannii-calcoaceticus complex (XDR-ABC), as well as to investigate risk factors for mortality, in cancer patients. It was a retrospective study including all patients diagnosed with XDR-ABC bacteraemia during hospitalization in the intensive care unit of a cancer hospital between July 2009 and July 2013. Surveillance cultures were collected weekly during the study period, and clonality was analysed using pulsed field gel electrophoresis (PFGE). We analysed underlying diseases, oncology therapy, neutrophil counts, infection site and management of infection, in terms of their correlation with 30-day mortality. During the study period, 92 patients with XDR-ABC bacteraemia were identified, of whom 35 (38.0%) were patients with haematological malignancy. We identified XDR-ABC strains with four different profile patterns, 91.3% of patients harbouring the predominant PFGE type. Of the 92 patients with XDR-ABC bacteraemia, 66 (71.7%) had central line-associated bloodstream infections; infection occurred during neutropenia in 22 (23.9%); and 58 (63.0%) died before receiving the appropriate therapy. All patients were treated with polymyxin, which was used in combination therapy in 30 of them (32.4%). The 30-day mortality rate was 83.7%. Multivariate analysis revealed that septic shock at diagnosis of XDR-ABC infection was a risk factor for 30-day mortality; protective factors were receiving appropriate therapy and invasive device removal within the first 48 h. Among cancer patients, ineffective management of such infection increases the risk of death, more so than do features such as neutropenia and infection at the tumour site. © 2015 European Society of Clinical Microbiology and Infectious Diseases. Acinetobacter baumannii; Bacterial infection; Bloodstream infection; Cancer; Carbapenem resistance; Intensive care; Mortality; Multidrug resistance; Neutropenia; Tumour infection amikacin; aminoglycoside; colistin; gentamicin; imipenem; meropenem; polymyxin; sultamicillin; tigecycline; antiinfective agent; Acinetobacter baumannii; Acinetobacter infection; acute leukemia; adult; aged; antibiotic sensitivity; Article; bacteremia; bloodstream infection; cancer center; cancer mortality; cancer patient; catheter infection; female; Hodgkin disease; hospitalization; human; intensive care unit; major clinical study; male; mortality; mortality rate; multiple myeloma; neutropenia; neutrophil count; nonhodgkin lymphoma; priority journal; retrospective study; septic shock; solid tumor; Acinetobacter baumannii; Acinetobacter Infections; adolescent; complication; drug effects; isolation and purification; microbiology; middle aged; multidrug resistance; neoplasm; risk factor; sepsis; survival analysis; treatment outcome; very elderly; young adult; Acinetobacter baumannii; Acinetobacter Infections; Adolescent; Adult; Aged; Aged, 80 and over; Anti-Bacterial Agents; Drug Resistance, Multiple, Bacterial; Humans; Intensive Care Units; Male; Middle Aged; Neoplasms; Neutropenia; Retrospective Studies; Risk Factors; Sepsis; Survival Analysis; Treatment Outcome; Young Adult;Objectives: To evaluate the effects of goal-directed therapy on outcomes in high-risk patients undergoing cardiac surgery. Design: A prospective randomized controlled trial and an updated metaanalysis of randomized trials published from inception up to May 1, 2015. Setting: Surgical ICU within a tertiary referral university-affiliated teaching hospital. Patients: One hundred twenty-six high-risk patients undergoing coronary artery bypass surgery or valve repair. Interventions: Patients were randomized to a cardiac output-guided hemodynamic therapy algorithm (goal-directed therapy group, n = 62) or to usual care (n = 64). In the goal-directed therapy arm, a cardiac index of greater than 3 L/min/m2 was targeted with IV fluids, inotropes, and RBC transfusion starting from cardiopulmonary bypass and ending 8 hours after arrival to the ICU. Measurements and Main Results: The primary outcome was a composite endpoint of 30-day mortality and major postoperative complications. Patients from the goal-directed therapy group received a greater median (interquartile range) volume of IV fluids than the usual care group (1,000 [625-1,500] vs 500 [500-1,000] mL; p &lt; 0.001], with no differences in the administration of either inotropes or RBC transfusions. The primary outcome was reduced in the goal-directed therapy group (27.4% vs 45.3%; p = 0.037). The goal-directed therapy group had a lower occurrence rate of infection (12.9% vs 29.7%; p = 0.002) and low cardiac output syndrome (6.5% vs 26.6%; p = 0.002). We also observed lower ICU cumulative dosage of dobutamine (12 vs 19 mg/kg; p = 0.003) and a shorter ICU (3 [3-4] vs 5 [4-7] d; p &lt; 0.001) and hospital length of stay (9 [8-16] vs 12 [9-22] d; p = 0.049) in the goal-directed therapy compared with the usual care group. There were no differences in 30-day mortality rates (4.8% vs 9.4%, respectively; p = 0.492). The metaanalysis identified six trials and showed that, when compared with standard treatment, goal-directed therapy reduced the overall rate of complications (goal-directed therapy, 47/410 [11%] vs usual care, 92/415 [22%]; odds ratio, 0.40 [95% CI, 0.26-0.63]; p &lt; 0.0001) and decreased the hospital length of stay (mean difference,-5.44 d; 95% CI,-9.28 to-1.60; p = 0.006) with no difference in postoperative mortality: 9 of 410 (2.2%) versus 15 of 415 (3.6%), odds ratio, 0.61 (95% CI, 0.26-1.47), and p = 0.27. Conclusions: Goal-directed therapy using fluids, inotropes, and blood transfusion reduced 30-day major complications in high-risk patients undergoing cardiac surgery. © 2016 by the Society of Critical Care Medicine and Wolters Kluwer Health, Inc. All Rights Reserved. cardiac surgery; goal-directed; hemodynamic; resuscitation dobutamine; infusion fluid; inotropic agent; beta 1 adrenergic receptor stimulating agent; dobutamine; clinical evaluation; confidence interval; coronary artery bypass surgery; erythrocyte transfusion; event free survival; forward heart failure; goal directed therapy; heart output; heart surgery; heart valve surgery; hemodynamic resuscitation therapy; hemodynamics; high risk patient; hospitalization; human; intensive care unit; length of stay; meta analysis; outcome assessment; perioperative period; postoperative complication; postoperative infection; priority journal; prospective study; randomized controlled trial (topic); Review; surgical mortality; systematic review; teaching hospital; treatment outcome; aged; controlled study; female; fluid therapy; heart surgery; hemodynamics; literature; male; meta analysis (topic); middle aged; mortality; physiology; Postoperative Complications; procedures; randomized controlled trial; very elderly; Adrenergic beta-1 Receptor Agonists; Aged; Aged, 80 and over; Cardiac Output; Cardiac Surgical Procedures; Dobutamine; Female; Fluid Therapy; Hemodynamics; Humans; Intensive Care Units; Length of Stay; Male; Meta-Analysis as Topic; Middle Aged; Postoperative Complications; Review Literature as Topic; Treatment Outcome;Background Data on renal replacement therapy (RRT) in cancer patients with acute kidney injury (AKI) in the intensive care unit (ICU) is scarce. The aim of this study was to assess the safety and the adequacy of intermittent hemodialysis (IHD) in critically ill cancer patients with AKI. Methods and Findings In this observational prospective cohort study, 149 ICU cancer patients with AKI were treated with 448 single-pass batch IHD procedures and evaluated from June 2010 to June 2012. Primary outcomes were IHD complications (hypotension and clotting) and adequacy. A multiple logistic regression was performed in order to identify factors associated with IHD complications (hypotension and clotting). Patients were 62.2 ± 14.3 years old, 86.6% had a solid cancer, sepsis was the main AKI cause (51%) and in-hospital mortality was 59.7%. RRT session time was 240 (180-300) min, blood/dialysate flow was 250 (200-300) mL/min and UF was 1000 (0-2000) ml. Hypotension occurred in 25% of the sessions. Independent risk factors (RF) for hypotension were dialysate conductivity (each ms/cm, OR 0.81, CI 0.69-0.95), initial mean arterial pressure (each 10 mmHg, OR 0.49, CI 0.40-0.61) and SOFA score (OR 1.16, CI 1.03-1.30). Clotting and malfunctioning catheters (MC) occurred in 23.8% and 29.2% of the procedures, respectively. Independent RF for clotting were heparin use (OR 0.57, CI 0.33-0.99), MC (OR 3.59, CI 2.24-5.77) and RRT system pressure increase over 25% (OR 2.15, CI 1.61-4.17). Post RRT blood tests were urea 71 (49-104) mg/dL, creatinine 2.71 (2.10-3.8) mg/dL, bicarbonate 24.1 (22.5-25.5) mEq/Land K3.8 (3.5-4.1) mEq/L Conclusion IHD for critically ill patients with cancer and AKI offered acceptable hemodynamic stability and provided adequate metabolic control. © 2016 Torres da Costa e Silva et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.  bicarbonate; creatinine; heparin; urea; acute kidney failure; adult; aged; Article; bicarbonate blood level; blood clotting; blood flow; Brazil; cancer patient; catheter; cohort analysis; controlled study; creatinine blood level; critically ill patient; dialysate; female; hemodialysis; hemodynamic stability; hemodynamics; human; hypotension; intensive care unit; length of stay; major clinical study; male; mean arterial pressure; metabolic regulation; mortality; observational study; outcome assessment; prospective study; risk factor; safety; sepsis; solid tumor; terminal care; urea blood level; Acute Kidney Injury; complication; critical illness; hospital mortality; middle aged; Neoplasms; pathophysiology; renal replacement therapy; Acute Kidney Injury; Adult; Aged; Cohort Studies; Critical Illness; Female; Hospital Mortality; Humans; Intensive Care Units; Male; Middle Aged; Neoplasms; Prospective Studies; Renal Dialysis; Renal Replacement Therapy; Risk Factors;Objective The aim of this study was to compare outcomes in patients undergoing cardiac surgery who are aged 60 years or more or less than 60 years after implementation of a restrictive or a liberal transfusion strategy. Methods This is a substudy of the Transfusion Requirements After Cardiac Surgery (TRACS) randomized controlled trial. In this subgroup analysis, we separated patients into those aged 60 years or more (elderly) and those aged less than 60 years randomized to a restrictive or a liberal strategy of red blood cell transfusion. The primary outcome was a composite defined as a combination of 30-day all-cause mortality and severe morbidity. Results Of the 502 patients included in the Transfusion Requirements After Cardiac Surgery study, 260 (51.8%) were aged 60 years or more and 242 (48.2%) were aged less than 60 years and were included in this study. The primary end point occurred in 11.9% of patients in the liberal strategy group and 16.8% of patients in the restrictive strategy group (P =.254) for those aged 60 years or more and in 6.8% of patients in the liberal strategy group and 5.6% of patients in the restrictive strategy group for those aged less than 60 years (P =.714). However, in the older patients, cardiogenic shock was more frequent in patients in the restrictive transfusion group (12.8% vs 5.2%, P =.031). Thirty-day mortality, acute respiratory distress syndrome, and acute renal injury were similar in the restrictive and liberal transfusion groups in both age groups. Conclusions Although there was no difference between groups regarding the primary outcome, a restrictive transfusion strategy may result in an increased rate of cardiogenic shock in elderly patients undergoing cardiac surgery compared with a more liberal strategy. Cardiovascular risk of anemia may be more harmful than the risk of blood transfusion in older patients. © 2015 The American Association for Thoracic Surgery. cardiac surgery; cardiogenic shock; elderly; red blood cell transfusion adult; aged; anemia; Article; blood transfusion; cardiogenic shock; cardiovascular risk; controlled study; disease severity; erythrocyte transfusion; female; groups by age; heart surgery; human; kidney injury; major clinical study; male; morbidity; mortality; priority journal; randomized controlled trial; adverse effects; age; anemia; blood; Brazil; comparative study; erythrocyte transfusion; heart surgery; hematocrit; metabolism; middle aged; procedures; prospective study; risk factor; Shock, Cardiogenic; time; treatment outcome; biological marker; hemoglobin; Adult; Age Factors; Aged; Anemia; Biomarkers; Brazil; Cardiac Surgical Procedures; Erythrocyte Transfusion; Hematocrit; Hemoglobins; Humans; Middle Aged; Prospective Studies; Risk Factors; Shock, Cardiogenic; Time Factors; Treatment Outcome;Introduction: Hypotensive state is frequently observed in several critical conditions. If an adequate mean arterial pressure is not promptly restored, insufficient tissue perfusion and organ dysfunction may develop. Fluids and catecholamines are the cornerstone of critical hypotensive states management. Catecholamines side effects such as increased myocardial oxygen consumption and development of arrhythmias are well known. Thus, in recent years, interest in catecholamine-sparing agents such as vasopressin, terlipressin and methylene blue has increased; however, few randomized trials, mostly with small sample sizes, have been performed. We therefore conducted a meta-analysis of randomized trials to investigate the effect of non-catecholaminergic vasopressors on mortality. Methods: PubMed, BioMed Central and Embase were searched (update December 31st, 2014) by two independent investigators. Inclusion criteria were: random allocation to treatment, at least one group receiving a non-catecholaminergic vasopressor, patients with or at risk for vasodilatory shock. Exclusion criteria were: crossover studies, pediatric population, nonhuman studies, studies published as abstract only, lack of data on mortality. Studied drugs were vasopressin, terlipressin and methylene blue. Primary endpoint was mortality at the longest follow-up available. Results: A total of 1,608 patients from 20 studies were included in our analysis. The studied settings were sepsis (10/20 studies [50%]), cardiac surgery (7/20 [35%]), vasodilatory shock due to any cause (2/20 [19%]), and acute traumatic injury (1/20 [5%]). Overall, pooled estimates showed that treatment with non-catecholaminergic agents improves survival (278/810 [34.3%] versus 309/798 [38.7%], risk ratio = 0.88, 95%confidence interval = 0.79 to 0.98, p = 0.02). None of the drugs was associated with significant reduction inmortality when analyzed independently. Results were not confirmed when analyzing studies with a low risk of bias. Conclusions: Catecholamine-sparing agents in patients with or at risk for vasodilatory shock may improve survival. Further researches on this topic are needed to confirm the finding. © 2015 Belletti et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.  hypertensive factor; methylene blue; terlipressin; vasopressin; lypressin; methylene blue; vasoconstrictor agent; vasopressin derivative; acute disease; Article; drug use; follow up; heart surgery; human; injury; mortality; randomized controlled trial (topic); risk factor; sepsis; shock; survival rate; systematic review; vasodilatory shock; analogs and derivatives; factual database; meta analysis; sepsis; shock; Databases, Factual; Humans; Lypressin; Methylene Blue; Randomized Controlled Trials as Topic; Sepsis; Shock; Vasoconstrictor Agents; Vasopressins;[No abstract available]  hemoglobin; serum albumin; abdominal surgery; blood storage; blood transfusion; cancer patient; comorbidity; critically ill patient; death; erythrocyte transfusion; hazard ratio; heart surgery; hemoglobin determination; human; intensive care; Letter; mortality; postoperative complication; priority journal; septic shock; Sequential Organ Failure Assessment Score; surgical patient; Abdominal Neoplasms; female; male; procedures; statistics and numerical data; Abdominal Neoplasms; Erythrocyte Transfusion; Female; Humans; Male;Background Guidelines support the use of a restrictive strategy in blood transfusion management in a variety of clinical settings. However, recent randomized controlled trials (RCTs) performed in the perioperative setting suggest a beneficial effect on survival of a liberal strategy. We aimed to assess the effect of liberal and restrictive blood transfusion strategies on mortality in perioperative and critically ill adult patients through a meta-analysis of RCTs. Methods We searched PubMed/Medline, Embase, Cochrane Central Register of Controlled Trials, Transfusion Evidence Library, and Google Scholar up to 27 March 2015, for RCTs performed in perioperative or critically ill adult patients, receiving a restrictive or liberal transfusion strategy, and reporting all-cause mortality. We used a fixed or random-effects model to calculate the odds ratio (OR) and 95% confidence interval (CI) for pooled data. We assessed heterogeneity using Cochrane's Q and I2 tests. The primary outcome was all-cause mortality within 90-day follow-up. Results Patients in the perioperative period receiving a liberal transfusion strategy had lower all-cause mortality when compared with patients allocated to receive a restrictive transfusion strategy (OR 0.81; 95% CI 0.66 1.00; P=0.05; I2=25%; Number needed to treat=97) with 7552 patients randomized in 17 trials. There was no difference in mortality among critically ill patients receiving a liberal transfusion strategy when compared with the restrictive transfusion strategy (OR 1.10; 95% CI 0.991.23; P=0.07; I2=34%) with 3469 patients randomized in 10 trials. Conclusion According to randomized published evidence, perioperative adult patients have an improved survival when receiving a liberal blood transfusion strategy. © 2015 The Author 2015. anesthesia; blood transfusion; critical illness; mortality; perioperative care hemoglobin; Article; blood transfusion; cause of death; critically ill patient; hematocrit; hemoglobin blood level; human; liberal blood transfusion; meta analysis; perioperative period; postoperative period; priority journal; prognosis; randomized controlled trial (topic); restrictive blood transfusion; surgical mortality; surgical patient; survival; systematic review; blood transfusion; critical illness; intensive care; mortality; perioperative period; procedures; randomized controlled trial (topic); statistics and numerical data; survival; treatment outcome; Blood Transfusion; Critical Care; Critical Illness; Humans; Perioperative Care; Randomized Controlled Trials as Topic; Survival Analysis; Treatment Outcome;[No abstract available] Cardiovascular disease; Diabetes; Insulin resistance; Insulin therapy; Mortality hemoglobin A1c; insulin; oral antidiabetic agent; antidiabetic agent; insulin; blood glucose monitoring; cardiovascular disease; case control study; diabetes mellitus; disease duration; human; insulin resistance; insulin treatment; metabolic parameters; Note; priority journal; Cardiovascular Diseases; Diabetes Mellitus, Type 2; female; male; Cardiovascular Diseases; Diabetes Mellitus, Type 2; Female; Humans; Hypoglycemic Agents; Insulin; Male;Purpose of Review: Anemia has been demonstrated to be detrimental in several populations such as high-surgical-risk patients, critically ill elderly, and cardiac patients. Red blood cell transfusion is the most commonly prescribed therapy for anemia. Despite being life-saving, it carries a risk that ranges from mild complications to death. The aim of this review is to discuss the risks of anemia and blood transfusion, and to describe recent developments in the strategies to reduce allogeneic blood transfusion. Recent Findings: In the past decades, clinical studies comparing transfusion strategies in different populations were conducted. Despite the challenges imposed by the development of such studies, evidence-based medicine on transfusion medicine in critically ill patients is being created. Different results arising from these studies reflect population heterogeneity, specific circumstances, and difficulties in measuring the impact of anemia and transfusion in a clinical trial. Summary: An adequate judgment of a clinical condition associated with proper application of the available literature is the cornerstone in the management of transfusion in critical care. Apart from this individualized strategy, the institution of a patient blood management program allows goal-directed approach through preoperative recognition of anemia, surgical efforts to minimize blood loss, and continuous assessment of the coagulation status. © 2015 Wolters Kluwer Health, Inc. All rights reserved. anemia; blood transfusion; individualized approach; patient blood management anemia; blood transfusion; clinical trial (topic); critically ill patient; evidence based medicine; human; Latin American medicine; population research; priority journal; Review; risk assessment; transfusion medicine;Background: Several studies have indicated that a restrictive erythrocyte transfusion strategy is as safe as a liberal one in critically ill patients, but there is no clear evidence to support the superiority of any perioperative transfusion strategy in patients with cancer. Methods: In a randomized, controlled, parallel-group, double-blind (patients and outcome assessors) superiority trial in the intensive care unit of a tertiary oncology hospital, the authors evaluated whether a restrictive strategy of erythrocyte transfusion (transfusion when hemoglobin concentration <7 g/dl) was superior to a liberal one (transfusion when hemoglobin concentration <9 g/dl) for reducing mortality and severe clinical complications among patients having major cancer surgery. All adult patients with cancer having major abdominal surgery who required postoperative intensive care were included and randomly allocated to treatment with the liberal or the restrictive erythrocyte transfusion strategy. The primary outcome was a composite endpoint of mortality and morbidity. Results: A total of 198 patients were included as follows: 101 in the restrictive group and 97 in the liberal group. The primary composite endpoint occurred in 19.6% (95% CI, 12.9 to 28.6%) of patients in the liberal-strategy group and in 35.6% (27.0 to 45.4%) of patients in the restrictive-strategy group (P = 0.012). Compared with the restrictive strategy, the liberal transfusion strategy was associated with an absolute risk reduction for the composite outcome of 16% (3.8 to 28.2%) and a number needed to treat of 6.2 (3.5 to 26.5). Conclusion: A liberal erythrocyte transfusion strategy with a hemoglobin trigger of 9 g/dl was associated with fewer major postoperative complications in patients having major cancer surgery compared with a restrictive strategy. Copyright © 2014, the American Society of Anesthesiologists, Inc.  abdominal infection; abdominal surgery; acute kidney failure; adult; adult respiratory distress syndrome; Article; cancer surgery; controlled study; double blind procedure; erythrocyte transfusion; female; hemoglobin determination; human; intensive care; liberal erythrocyte transfusion; major clinical study; major surgery; male; morbidity; mortality; multiple organ failure; postoperative care; priority journal; prospective study; randomized controlled trial; reoperation; respiratory failure; restrictive erythrocyte transfusion; risk reduction; septic shock; Abdominal Neoplasms; Brazil; erythrocyte transfusion; follow up; intensive care unit; middle aged; outcome assessment; Postoperative Complications; procedures; risk; statistics and numerical data; hemoglobin; Abdominal Neoplasms; Brazil; Double-Blind Method; Erythrocyte Transfusion; Female; Follow-Up Studies; Hemoglobins; Humans; Intensive Care Units; Male; Middle Aged; Outcome Assessment (Health Care); Postoperative Complications; Prospective Studies; Risk;[No abstract available]  ;[No abstract available]  ;[No abstract available]  ;[No abstract available]  ;[No abstract available]  ;Heart transplantation is an option for children with complex congenital heart disease and cardiomyopathies. A patient's quality of life and long-term survival depend on successful management of the surgical complications and adverse side effects of immunosuppression. The purpose of this review was to summarize the practical management of postoperative care in this patient population and to make recommendations for the future. © 2014 CLINICS. Children; Heart transplantation; Immunosuppression; Postoperative care; Rejection adverse effects; Cardiomyopathies; child; graft rejection; Heart Defects, Congenital; heart transplantation; human; postoperative care; procedures; quality of life; Cardiomyopathies; Child; Graft Rejection; Heart Defects, Congenital; Heart Transplantation; Humans; Postoperative Care; Quality of Life;Results: A total of 63 patients were included in the study, 30 in the fibrinogen concentrate group and 33 in the cryoprecipitate group. The median 48-hour blood loss was not significantly different between the 2 groups (320 mL [interquartile range, 157-750] vs 410 mL [interquartile range, 215-510], respectively; P = .672). After treatment, plasma fibrinogen concentration increased similarly following administration of both products. There were no differences in allogeneic blood transfusion after intervention treatment.Conclusions: A large trial comparing fibrinogen concentrate and cryoprecipitate in the management of children with acute acquired hypofibrinogenemia during heart surgery is feasible. The preliminary results of our study showed that the use of fibrinogen concentrate was as efficient and safe as cryoprecipitate in the management of bleeding children undergoing cardiac surgery.Objectives: Acute acquired hypofibrinogenemia in children undergoing cardiac surgery is a major concern because it often results in perioperative bleeding and high rates of allogeneic blood transfusion. Fibrinogen concentrate has been proposed as an alternative to cryoprecipitate (the gold standard therapy), with minimal infectious and immunologic risks. Our objective was to investigate the efficacy and safety of fibrinogen concentrate in children undergoing cardiac surgery.Methods: In this randomized pilot study, patients were allocated to receive fibrinogen concentrate (60 mg/kg) or cryoprecipitate (10 mL/kg) if bleeding was associated with fibrinogen levels < 1 g/dL after cardiopulmonary bypass weaning. The primary outcome was postoperative blood losses during the 48 hours after surgery. © 2014 The American Association for Thoracic Surgery.  blood clotting factor 10; blood clotting factor 13; blood clotting factor 7; blood clotting factor 9; fibrinogen; fibrinogen concentrate; prothrombin; blood clotting factor 8; cryoprecipitate coagulum; fibrinogen; acute heart infarction; acute kidney failure; adult; anaphylaxis; artery thrombosis; Article; artificial ventilation; bleeding; blood clotting test; blood transfusion; cardiopulmonary bypass; cause of death; cerebrovascular accident; controlled study; cryoprecipitate; deep vein thrombosis; drug effect; drug eruption; drug fever; drug hypersensitivity; drug induced headache; drug safety; dyspnea; feasibility study; female; fibrinogen blood level; follow up; heart infarction; heart surgery; human; lung embolism; major clinical study; male; outcome assessment; patient safety; pediatric surgery; peripheral occlusive artery disease; pilot study; postoperative hemorrhage; postoperative period; prospective study; randomized controlled trial; reoperation; septic shock; thromboembolism; vomiting; Brazil; child; erythrocyte transfusion; infant; operative blood loss; preschool child; statistics and numerical data; treatment outcome; Blood Coagulation Tests; Blood Loss, Surgical; Brazil; Cardiac Surgical Procedures; Cardiopulmonary Bypass; Child; Child, Preschool; Erythrocyte Transfusion; Factor VIII; Female; Fibrinogen; Humans; Infant; Male; Pilot Projects; Prospective Studies; Treatment Outcome;Background The renoprotective effect of N-acetylcystein in patients undergoing coronary artery bypass graft surgery is controversial. Methods We assessed the renoprotective effect of the highest dose of N-acetylcystein sanctioned for clinical use in a prospective, double-blind, placebo-controlled study including 70 chronic kidney disease patients, stage 3 or 4, who underwent coronary artery bypass graft surgery, on cardiopulmonary bypass (CPB) and off CPB, and were randomly allocated to receive either N-acetylcystein 150 mg/kg followed by 50 mg/kg for 6 hours in 0.9% saline or only 0.9% saline. Acute kidney injury was defined by the Acute Kidney Injury Network classification. Results The incidence of kidney injury was reduced in the N-acetylcystein group (57.1% versus 28.6%, p = 0.016). Nonuse of N-acetylcystein (relative risk 3.58, 95% confidence interval: 1.04 to 12.33, p = 0.04) and cardiopulmonary bypass (relative risk 4.55, 95% confidence interval: 1.28 to 16.15, p = 0.02) were independent predictors of kidney injury. In patients treated with CPB, N-acetylcystein reduced the incidence of kidney injury from 63% to 46%. Oxidative stress was increased in control subjects (p = 0.01) and abolished in patients receiving N-acetylcystein. Conclusions Maximum intravenous doses of N-acetylcystein reduce the incidence of acute kidney injury in patients with kidney disease undergoing coronary artery bypass graft surgery, abolish oxidative stress, and mitigate the negative effect of CPB on renal function. © 2014 by The Society of Thoracic Surgeons.  acetylcysteine; angiotensin receptor antagonist; anticoagulant agent; antidiabetic agent; beta adrenergic receptor blocking agent; calcium channel blocking agent; digitalis; dipeptidyl carboxypeptidase inhibitor; diuretic agent; fibric acid derivative; hydroxymethylglutaryl coenzyme A reductase inhibitor; insulin; placebo; sodium chloride; vasoactive agent; vasodilator agent; acute heart infarction; acute kidney failure; adult; aged; anticoagulant therapy; article; blood transfusion; cardiopulmonary bypass; cardiovascular disease; chronic kidney disease; clinical assessment; controlled study; coronary artery bypass graft; diabetes mellitus; disease classification; disease severity; diuretic therapy; double blind procedure; drug efficacy; drug megadose; dyslipidemia; female; heart left ventricle ejection fraction; heart muscle revascularization; human; hypertension; major clinical study; male; middle aged; oxidative stress; postoperative hemorrhage; predictor variable; priority journal; prospective study; randomized controlled trial; risk factor; risk reduction; surgical patient; surgical risk; treatment outcome; Acetylcysteine; Acute Kidney Injury; Aged; Cardiopulmonary Bypass; Confidence Intervals; Coronary Artery Bypass; Coronary Stenosis; Dose-Response Relationship, Drug; Double-Blind Method; Female; Hospital Mortality; Humans; Kidney Failure, Chronic; Male; Middle Aged; Postoperative Complications; Prognosis; Prospective Studies; Pulse Therapy, Drug; Reference Values; Risk Assessment; Statistics, Nonparametric; Survival Rate; Treatment Outcome;Perspectives on invasive and noninvasive ventilatory support for critically ill patients are evolving, as much evidence indicates that ventilation may have positive effects on patient survival and the quality of the care provided in intensive care units in Brazil. For those reasons, the Brazilian Association of Intensive Care Medicine (Associação de Medicina Intensiva Brasileira - AMIB) and the Brazilian Thoracic Society (Sociedade Brasileira de Pneumologia e Tisiologia - SBPT), represented by the Mechanical Ventilation Committee and the Commission of Intensive Therapy, respectively, decided to review the literature and draft recommendations for mechanical ventilation with the goal of creating a document for bedside guidance as to the best practices on mechanical ventilation available to their members. The document was based on the available evidence regarding 29 subtopics selected as the most relevant for the subject of interest. The project was developed in several stages, during which the selected topics were distributed among experts recommended by both societies with recent publications on the subject of interest and/or significant teaching and research activity in the field of mechanical ventilation in Brazil. The experts were divided into pairs that were charged with performing a thorough review of the international literature on each topic. All the experts met at the Forum on Mechanical Ventilation, which was held at the headquarters of AMIB in São Paulo on August 3 and 4, 2013, to collaboratively draft the final text corresponding to each sub-topic, which was presented to, appraised, discussed and approved in a plenary session that included all 58 participants and aimed to create the final document.  artificial ventilation; Brazil; critical illness; health care quality; human; intensive care; intensive care unit; practice guideline; procedures; standards; Brazil; Critical Care; Critical Illness; Humans; Intensive Care Units; Practice Guidelines as Topic; Quality of Health Care; Respiration, Artificial;Perspectives on invasive and noninvasive ventilatory support for critically ill patients are evolving, as much evidence indicates that ventilation may have positive effects on patient survival and the quality of the care provided in intensive care units in Brazil. For those reasons, the Brazilian Association of Intensive Care Medicine (Associação de Medicina Intensiva Brasileira - AMIB) and the Brazilian Thoracic Society (Sociedade Brasileira de Pneumologia e Tisiologia - SBPT), represented by the Mechanical Ventilation Committee and the Commission of Intensive Therapy, respectively, decided to review the literature and draft recommendations for mechanical ventilation with the goal of creating a document for beds"
Márcio Sommer Bittencourt,Universidade de São Paulo - Faculdade de Medicina,"[No abstract available]  ;Objectives The aim of this study was to assess the odds of initiation or continuation of pharmacological and lifestyle preventive therapies in patients with nonzero versus zero coronary artery calcium (CAC) score detected on cardiac computed tomography. Background Detection of calcified coronary plaque could serve as a motivational tool for physicians and patients to intensify preventive therapies. Methods We searched PubMed, EMBASE (Excerpta Medica database), Web of Science, Cochrane CENTRAL (Cochrane central register of controlled trials), ClinicalTrials.gov, and the International Clinical Trials Registry Platform for studies evaluating the association of CAC scores with downstream pharmacological or lifestyle interventions for prevention of cardiovascular disease. Pooled odds ratios (ORs) of downstream interventions were obtained using the DerSimonian and Laird random effects model. Results After a review of 6,256 citations and 54 full-text papers, 6 studies (11,256 participants, mean follow-up time: 1.6 to 6.0 years) were included. Pooled estimates of the odds of aspirin initiation (OR: 2.6; 95% confidence interval [CI]: 1.8 to 3.8), lipid-lowering medication initiation (OR: 2.9; 95% CI: 1.9 to 4.4), blood pressure–lowering medication initiation (OR: 1.9; 95% CI: 1.6 to 2.3), lipid-lowering medication continuation (OR: 2.3; 95% CI: 1.6 to 3.3), increase in exercise (OR: 1.8; 95% CI: 1.4 to 2.4), and dietary change (OR: 1.9; 95% CI: 1.5 to 2.5) were higher in individuals with nonzero CAC versus zero CAC scores, but not for aspirin or blood pressure–lowering medication continuation. When assessed within individual studies, these findings remained significant after adjustment for baseline patient characteristics and cardiovascular risk factors. Conclusions This systematic review and meta-analysis suggests that nonzero CAC score, identifying calcified coronary plaque, significantly increases the likelihood of initiation or continuation of pharmacological and lifestyle therapies for the prevention of cardiovascular disease. © 2017 American College of Cardiology Foundation cardiovascular prevention; coronary calcium score; meta-analysis ;Background: There is controversy whether management of blood cholesterol should be based or not on LDL-cholesterol (LDL-c) target concentrations. Objectives: To compare the estimated impact of different lipid-lowering strategies, based or not on LDL-c targets, on the risk of major cardiovascular events in a population with higher cardiovascular risk. Methods: We included consecutive individuals undergoing a routine health screening in a single center who had a 10-year risk for atherosclerotic cardiovascular disease (ASCVD) ≥ 7.5% (pooled cohort equations, ACC/AHA, 2013). For each individual, we simulated two strategies based on LDL-c target (≤ 100 mg/dL [Starget-100] or ≤ 70 mg/dL [Starget-70]) and two strategies based on percent LDL-c reduction (30% [S30%] or 50% [S50%]). Results: In 1,897 subjects (57 ± 7 years, 96% men, 10-year ASCVD risk 13.7 ± 7.1%), LDL-c would be lowered from 141 ± 33 mg/dL to 99 ± 23 mg/dL in S30%, 71 ± 16 mg/dL in S50%, 98 ± 9 mg/dL in Starget-100, and 70 ± 2 mg/dL in Starget-70. Ten-year ASCVD risk would be reduced to 8.8 ± 4.8% in S50% and 8.9 ± 5.2 in Starget-70. The number of major cardiovascular events prevented in 10 years per 1,000 individuals would be 32 in S30%, 31 in Starget-100, 49 in S50%, and 48 in Starget-70. Compared with Starget-70, S50% would prevent more events in the lower LDL-c tertile and fewer events in the higher LDL-c tertile. Conclusions: The more aggressive lipid-lowering approaches simulated in this study, based on LDL-c target or percent reduction, may potentially prevent approximately 50% more hard cardiovascular events in the population compared with the less intensive treatments. Baseline LDL-c determines which strategy (based or not on LDL-c target) is more appropriate at the individual level. © 2017, Arquivos Brasileiros de Cardiologia. All rights reserved. Cholesterol; Cholesterol; Coronary artery disease; HDL/ blood; Hypercholesterolemia/ blood; LDL/ blood; Risk factors ;Background: The best way to select individuals for lipid-lowering treatment in the population is controversial. Objective: In healthy individuals in primary prevention: (1) to assess the relationship between cardiovascular risk categorized according to the V Brazilian Guideline on Dyslipidemia and the risk calculated by the pooled cohort equations (PCE); (2) to compare the proportion of individuals eligible for statins, according to different criteria. Methods: In individuals aged 40-75 years consecutively submitted to routine health assessment at one single center, four criteria of eligibility for statin were defined: BR-1, BR-2 (LDL-c above or at least 30 mg/dL above the goal recommended by the Brazilian Guideline, respectively), USA-1 and USA-2 (10-year risk estimated by the PCE ≥ 5.0% or ≥ 7.5%, respectively). Results: The final sample consisted of 13,947 individuals (48 ± 6 years, 71% men). Most individuals at intermediate or high risk based on the V Brazilian Guideline had a low risk calculated by the PCE, and more than 70% of those who were considered at high risk had this categorization because of the presence of aggravating factors. Among women, 24%, 17%, 4% and 2% were eligible for statin use according to the BR-1, BR-2, USA-1 and USA-2 criteria, respectively (p < 0.01). The respective figures for men were 75%, 58%, 31% and 17% (p < 0.01). Eighty-five percent of women and 60% of men who were eligible for statin based on the BR-1 criterion would not be candidates for statin based on the USA-1 criterion. Conclusions: As compared to the North American Guideline, the V Brazilian Guideline considers a substantially higher proportion of the population as eligible for statin use in primary prevention. This results from discrepancies between the risk stratified by the Brazilian Guideline and that calculated by the PCE, particularly because of the risk reclassification based on aggravating factors. © 2017, Arquivos Brasileiros de Cardiologia. All rights reserved. Anticholesterelemic agents; Cardiovascular diseases; Cholesterol; Hydroxymethylglutaryl-CoA reductases; Practice guidelines as topic; Risk assessment ;Background and aims Epidemiological studies have analyzed the association between carotid intima-media thickness (CIMT) and insulin resistance, glucose levels or glycated hemoglobin with mixed results. We aimed to evaluate the association between CIMT and homeostasis model assessment – insulin resistance (HOMA-IR), fasting and post-load plasma glucose and glycated hemoglobin in the Brazilian Longitudinal Study of Adult Health (ELSA-Brasil) baseline. Methods We included 8028 participants (aged 35–74 years) without diabetes or overt cardiovascular disease who had complete CIMT data at baseline. We built crude and adjusted linear and binary logistic models to evaluate the association between CIMT and (a) HOMA-IR; (b) fasting plasma glucose; (c) post-load plasma glucose; and (d) glycated hemoglobin. We also built post-hoc models, stratified by sex. Results In the fully-adjusted linear models, only the association between CIMT (in mm) and HOMA-IR remained significant (β = 0.004; 95% confidence interval [95%CI]:0.001 to 0.006). Consistent with these results, only the association between the highest age- sex- and race-specific CIMT quartile and HOMA-IR was significant in the adjusted logistic model (odds ratio [OR]:1.10; 95% CI:1.04–1.17). The association between HOMA-IR and the highest CIMT quartile remained significant in sex-specific analyses (OR:1.10; 95% CI:1.02–1.20 for men and OR:1.10; 95% CI:1.02–1.20 for women). We did not find an independent association between CIMT and glucose or glycated hemoglobin. Conclusions We found a direct association between HOMA-IR and CIMT in a large sample of non-diabetic participants. Mechanisms unrelated to glucose homeostasis, as a direct effect of insulin on atherosclerosis, or medial hypertrophy, may be involved. © 2017 Elsevier B.V. Glycemic; Hyperinsulinemia; Insulin; Medial hypertrophy; Metabolism; Subclinical atherosclerosis ;Although low-density lipoprotein cholesterol (LDL-C) is widely accepted as the principal lipid fraction associated with atherosclerosis, emerging evidence suggests a causal relation between lifelong elevations in triglyceride-rich lipoprotein cholesterol (TRL-C) and cardiovascular disease (CVD) in genetic studies. To provide further evidence for the potential relevance of TRL-C and atherosclerosis, we have evaluated the relation between TRL-C and coronary artery calcium (CAC) score. We included 3,845 subjects (49.9 ± 8.4 years, 54% women) who had no history of CVD, were not using lipid-lowering medications, and underwent CAC evaluation. We assessed the relation between increasing fasting TRL-C and the graded increase in CAC and to what extent TRL-C were independently associated with CAC over and above LDL-C using logistic regression models. Overall, 973 (25%) of the participants had a CAC >0 and 308 (8%) had a CAC >100. The median TRL-C level was 22 mg/dL (IQR 16 to 32). Subjects with CAC >0 had higher TRL-C levels than those with CAC = 0 (p <0.001). Similarly, subjects with CAC >0 had higher levels of LDL-C, non–high-density lipoprotein cholesterol, and lower high-density lipoprotein cholesterol (all p <0.001). After multivariate adjustment, log-transformed TRL-C remained associated with the presence and severity of CAC (all p <0.05). When TRL-C was added to models that contained demographic factors and conventional lipids, it significantly improved the model to predict the presence of CAC >0 (p = 0.01). In conclusion, in a large cohort of asymptomatic subjects, TRL-C was associated with subclinical atherosclerosis supporting a potentially causal role in CVD. © 2017 Elsevier Inc.  C reactive protein; cholesterol; high density lipoprotein cholesterol; lipoprotein; lipoprotein cholesterol; lipoprotein triglyceride; low density lipoprotein cholesterol; triacylglycerol; adult; blood; blood vessel calcification; Brazil; coronary artery disease; diagnostic imaging; diet restriction; female; human; male; metabolism; middle aged; multivariate analysis; statistical model; x-ray computed tomography; Adult; Brazil; C-Reactive Protein; Cholesterol; Cholesterol, HDL; Cholesterol, LDL; Coronary Artery Disease; Fasting; Female; Humans; Lipoproteins; Logistic Models; Male; Middle Aged; Multivariate Analysis; Tomography, X-Ray Computed; Triglycerides; Vascular Calcification;[No abstract available] coronary CTA; myocardial ischemia; prevention; revascularization ;Background: Few studies have evaluated a possible relationship between thyrotropin levels and glomerular filtration rate (GFR) and albumin/creatinine ratio in euthyroid subjects. We aimed to analyze this association using baseline data from the Brazilian Longitudinal Study of Adult Health (ELSA-Brasil). Methods: Cross-sectionally, we included subjects with normal thyroid function and with subclinical hypothyroidism (SCH). We excluded individuals using medications that affect thyroid function. Linear and logistic regression models evaluated GFR estimated by Chronic Kidney Disease Epidemiology Collaboration (CKD-Epi) and albuminuria/creatinine ratio as dependent variables and thyrotropin quartiles in individuals with euthyroidism and SCH as independent variables, adjusted for demographical characteristics and diseases related to CKD. Results: We included 13,193 subjects with a median age of 51 years [interquartile range, (IQR): 45–58], 6840 (51.8%) women, 12,416 (94.1%) euthyroid, and 777 (5.9%) with SCH. SCH subjects were characterized by higher age, triglycerides, frequency of white race, cardiovascular disease, CKD, and former smokers. In adjusted models, log-transformed TSH in euthyroid subjects was inversely and strongly associated with CKD (β = −2.181, 95% CI −2.714 to −1.648), P < 0.0001 for glomerular filtration rate and 4.528 (1.190–7.865) for albuminuria/creatinine ratio. Multivariate logistic models for euthyroid subjects showed an OR of 1.45 (95% CI 1.15–1.83) for GFR and of 1.95 (95% CI 1.08–3.54) for albuminuria/creatinine ratio in the fourth quartile of TSH using the first as the reference. Conclusions: Thyrotropin levels are independently associated with CKD in euthyroid subjects. © 2017 Japanese Society of Nephrology Chronic kidney disease; CKD-Epi; Glomerular filtration rate; Subclinical hypothyroidism; Thyroid dysfunction ;Purpose: Obstructive sleep apnea (OSA) is associated with coronary disease among men. However, this association is not clear for women. In this study, we evaluate the association between OSA and presence of subclinical atherosclerosis assessed by tomographic coronary calcium score in middle-aged women. Methods: We evaluated consecutive women aged between 45 and 65 years in perimenopause or postmenopause period (with menstrual irregularity—amenorrhea &gt; 60 days), without manifest cardiovascular disease (heart failure, coronary disease, and stroke), from two gynecologic clinics. All patients underwent clinical evaluation, computed tomographic examination for coronary artery calcium (CAC &gt; 100 Agatston units), and portable sleep study. Multiple logistic regression models were used to evaluate the association between OSA and CAC, controlling for traditional risk factors including Framingham Risk Score (FRS), body mass index (BMI), and diabetes. Results: We studied 214 women (age 56 years (52–61); BMI 28 kg/m2 (25–31), 25 % diabetes, 62 % hypertension). OSA (apnea-hypopnea index (AHI) ≥5 events/h) was diagnosed in 82 women (38.3 %). CAC was more prevalent in patients with moderate/severe OSA (AHI ≥15 events/h) than in patients without or with mild OSA, 19 % vs 4.5 and 1.6 %, respectively (p &lt; 0.01). Moderate to severe OSA was associated with CAC in unadjusted (odds ratio = 6.25, 95 % CI 1.66–23.52; p &lt; 0.01) and adjusted (odds ratio = 8.19, 95 % CI 1.66–40.32; p = 0.01) logistic regression analysis. Conclusion: Moderate to severe OSA is independently associated with the presence of CAC in middle-aged women. These results reinforce the concept that women are also susceptible to the cardiovascular consequences of OSA. © 2016, Springer-Verlag Berlin Heidelberg. Atherosclerosis; Coronary heart disease; Obstructive sleep apnea; Women ;Background Proper treatment of patients with diffuse, severe coronary artery disease (CAD) is a challenge due to its complexity. Thus, data on the outcomes after coronary artery bypass graft (CABG) in this population is scarce. In this study, we aimed to determine the impact of CABG on the clinical and functional status, as well as graft patency in those individuals. Methods Patients with severe and diffuse CAD who underwent incomplete CABG due to complex anatomy or extensive distal coronary involvement were evaluated preoperatively and 1 year after surgery. Postoperative coronary angiography was performed to evaluate graft patency. Graft occlusion was defined as the complete absence of opacification of the target vessel. Stratified analysis of graft occlusion was performed by graft type and territories, defined as left anterior descending artery (LAD), the left circumflex branch, and the right coronary artery territories; the latter two, grouped, were further classified as non-LAD territory. Results A total of 57 patients were included, in whom 131 grafts were placed. There was a significant improvement in Canadian Cardiovascular Society angina symptom severity (Z = –6.1; p < 0.001) and maximum oxygen uptake (p < 0.001), with a corresponding decrease in the use of long-acting nitrates (p < 0.001). The overall graft occlusion rate was 19.1%, with no significant difference between LAD and non-LAD territories (p = 0.08). However, a significantly lower occlusion rate was noted for the internal mammary artery (IMA) grafts when compared with saphenous vein grafts (p = 0.01), though this difference was only significant in the LAD territory (p = 0.04). Overall, the use of venous graft was the only predictor occlusion at 1 year (odds ratio: 4.03; p = 0.016). Conclusion In patients with diffuse CAD, incomplete CABG surgery resulted in a significant clinical improvement, with acceptable graft occlusion rates at 1 year, particularly for IMA grafts to the LAD territory. Copyright © 2017, Georg Thieme Verlag KG. All rights reserved. coronary artery bypass graft surgery; coronary artery disease; graft occlusion ;Background and aims: Achilles tendon xanthomas (ATX) are a sign of long-term exposure to high blood cholesterol in familial hypercholesterolemia (FH) patients, which have been associated with cardiovascular disease. We evaluated the ATX association with the presence and extent of subclinical coronary atherosclerosis in heterozygous FH patients. Methods: 102 FH patients diagnosed by US-MEDPED criteria (67% with genetically proven FH), with median LDL-C 279 mg/dL (interquartile range: 240; 313), asymptomatic for cardiovascular disease, underwent computed tomography angiography and coronary artery calcium (CAC) quantification. Subclinical coronary atherosclerosis was quantified by CAC, segment-stenosis (SSS) and segment-involvement (SIS) scores. Adjusted Poisson regression was used to assess the association of ATX with subclinical atherosclerosis burden as continuous variables. Results: Patients with ATX (n = 21, 21%) had higher LDL-C and lipoprotein(a) [Lp(a)] concentrations as well as greater CAC scores, SIS and SSS (p < 0.05). After adjusting for age, sex, smoking, hypertension, previous statin use, HDL-C, LDL-C and Lp(a) concentrations, there was an independent positive association of ATX presence with CAC scores (β = 1.017, p < 0.001), SSS (β = 0.809, p < 0.001) and SIS (β = 0.640, p < 0.001). Conclusions: ATX are independently associated with the extension of subclinical coronary atherosclerosis quantified by tomographic scores in FH patients. © 2017 Elsevier B.V. Atherosclerosis; Calcium score; Computed tomography; Familial hypercholesterolemia; Xanthomas ;Background and aims The impact of tobacco use and cessation on atherogenesis remains unclear. We aimed to study the association of tobacco use and prior cessation with the presence, extent and severity of atherosclerosis on coronary computed tomographic angiography (CTA). Methods We examined 1798 consecutive symptomatic patients without known coronary artery disease (CAD) referred for CTA, stratified by smoking status (never, current [within 30 days], or former [>30 days before CTA]). Plaque severity (none, <50%, ≥50% stenosis), composition (non-calcified [NCP], partially calcified [PCP], or calcified plaque [CP]), and segment involvement score (SIS) were visually graded. Multivariate analysis was performed, adjusting for CAD risk factors and cholesterol lowering medication use. Results The median age of patients was 50 years [IQR:42–58] (61% male), with 74% never smokers, 12% current smokers, and 14% former smokers (median quit duration = 12 years [IQR:3–26]). Smoking exposure in former versus current smokers was 11 [IQR:5–25] and 10 [IQR:2–20] pack-years, respectively (p = 0.01). Compared to never smokers, current smokers demonstrated an increased odds ratio of all plaque types (adjusted OR: any NCP = 1.55 [95% CI 1.04–2.32], p = 0.03; any PCP = 1.61 [1.10–2.37], p = 0.02; any CP = 1.93 [1.32–2.81], p = 0.001), non-obstructive CAD (aOR = 1.47 [1.04, 2.07], p = 0.03), obstructive CAD (aOR = 1.81 [1.01–3.24], p = 0.047), and SIS > 4 (aOR = 1.60 [1.04–2.46], p = 0.03). Compared to current smoking, prior smoking cessation (≥12 years) was associated with a decreased odds ratio of any NCP (aOR = 0.42 [0.19–0.90], p = 0.03), CP (aOR = 0.43 [0.22–0.84], p = 0.02), and obstructive CAD (aOR = 0.40, [0.15–0.98], p = 0.048). Conclusions Current smoking is independently associated with the presence and extent of coronary plaque, and a higher risk of non-obstructive and obstructive CAD compared to never smoking. Prior smoking cessation correlated with improvements in CTA-identified plaque measures. © 2016 Atherosclerosis; Coronary artery disease; Coronary computed tomographic angiography; Smoking; Tobacco ;Objective: To investigate the relationship among body mass index (BMI), cardiometabolic risk and coronary artery disease (CAD) among patients undergoing coronary computed tomography angiography (CTA). Methods: Retrospective cohort study of 1118 patients, who underwent coronary CTA at two centers from September 2004 to October 2011. Coronary CTA were categorized as normal, nonobstructive CAD (&lt;50%), or obstructive CAD (≥50%) in addition to segment involvement (SIS) and stenosis scores. Extensive CAD was defined as SIS &gt; 4. Association of BMI with cardiovascular prognosis was evaluated using multivariable fractional polynomial models. Results: Mean age of the cohort was 57 ± 13 years with median follow-up of 3.2 years. Increasing BMI was associated with MetS (OR 1.28 per 1 kg/m2, p &lt; 0.001) and burden of CAD on a univariable basis, but not after multivariable adjustment. Prognosis demonstrated a J-shaped relationship with BMI. For BMI from 20-39.9 kg/m2, after adjustment for age, gender, and smoking, MetS (HR 2.23, p = 0.009) was more strongly associated with adverse events. Conclusions: Compared to normal BMI, there was an increased burden of CAD for BMI &gt; 25 kg/m2. Within each BMI category, metabolically unhealthy patients had greater extent of CAD, as measured by CCTA, compared to metabolically healthy patients. © 2017 The Author(s). Cohort; Coronary artery disease; Coronary computed tomography angiography; Metabolic syndrome; Obesity; Prognosis adult; age; Article; body mass; cardiometabolic risk; cohort analysis; computed tomographic angiography; computed tomography scanner; controlled study; coronary angiography; coronary artery disease; disease association; disease severity; female; gender; human; major clinical study; male; metabolic syndrome X; middle aged; obesity; prevalence; prognosis; retrospective study; smoking;[No abstract available]  cardiovascular disease; cardiovascular magnetic resonance; clinical evaluation; coronary angiography; coronary artery disease; error; human; intermethod comparison; ischemic heart disease; Letter; practice guideline; scoring system; type 1 error; coronary artery disease; heart; Coronary Angiography; Coronary Artery Disease; Coronary Disease; Heart; Humans;[No abstract available]  computed tomographic angiography; coronary angiography; coronary artery calcium score; coronary artery disease; heart infarction; heart muscle revascularization; human; Letter; practice guideline; priority journal; thorax pain;[No abstract available]  ;Objectives: We examined the relationship between coronary artery calcification (CAC) score and performance in cognitive tests in a large Brazilian sample. Methods: In this cross-sectional study, 4104 participants (mean age = 50.9 ± 8.8 years old, 54% female) from the Brazilian Longitudinal Study of Adult Health had complete information for CAC and cognitive tests. We used linear regression models adjusted for sociodemographics, cardiovascular risk factors (hypertension, diabetes, smoking, alcohol use, physical activity, and body mass index), depression, and thyroid function. To investigate potential different associations for middle-aged and older adults, we stratified the analysis by age groups. Results: Participants with CAC ≥ 100 Agatston score had poorer performance in the trail making test compared to those with CAC < 100 Agatston score (β = -0.101, 95% CI = -0.194; -0.010, p = 0.03). We did not find any other association between CAC and cognitive tests. When we investigated the effect modification between CAC and age on cognitive tests, only the effect modification on global cognition (p = 0.02) and trail making test was significant (p = 0.0003). Conclusions: Higher CAC was weakly associated with poorer performance in an executive function test in a large sample from the Brazilian Longitudinal Study of Adult Health. © 2017 John Wiley & Sons, Ltd. Atherosclerosis; Calcinosis; Cognition; Coronary artery; Dementia ;Background and aims While studies have described the importance of higher physical activity levels (PAL) in weight loss, the impact of self-initiated PAL on health status warrants further study. We aimed to prospectively examine the effects of self-initiated longitudinal PAL changes on body mass index (BMI) and cardiometabolic parameters in normal weight, overweight and obese adults. Methods and results We included 4840 adults (mean age 41.6 ± 7.9 years, 79% male) undergoing routine health screening examinations. Self-reported PAL, height, weight, blood pressure and blood samples were collected at baseline and after a mean (95% confidence interval) follow up of 536 (531–541) days. Subjects were stratified according to BMI [39.8% normal weight (&lt;25 kg/m2), 45.1% overweight (25.0–29.9 kg/m2), and 19.1% obese (≥30 kg/m2)]. In normal weight individuals, BMI increased from baseline to follow-up, irrespective of PAL changes. On the other hand, overweight and obese individuals that increased PAL experienced a decrease in BMI by −0.9% and −3.1%, respectively (p &lt; 0.05). Overweight and obese individuals that increased PAL also experienced a decrease in −5.8% -4.6% in non-HDL concentrations from baseline to follow-up (p &lt; 0.05). Finally, in overweight individuals, LDL cholesterol concentrations decreased from baseline to follow-up, irrespective of PAL changes whereas in obese individuals, a maintenance or increased PAL were associated with a decrease in −4.7% and −6.1% (p &lt; 0.05), respectively. Conclusions In a large cohort of screening patients, longitudinal self-initiated PAL is associated with improved BMI and cardiometabolic profile in overweight and obese individuals. © 2016 The Italian Society of Diabetology, the Italian Society for the Study of Atherosclerosis, the Italian Society of Human Nutrition, and the Department of Clinical Medicine and Surgery, Federico II University Body mass index; Cardiovascular risk; Exercise; Triglycerides ;BACKGROUND There is little available data on carotid-femoral pulse wave velocity (cf-PWV) in subjects with subclinical hypothyroidism (SCH). We aimed to analyze the association between SCH and cf-PWV using baseline data from the Brazilian Longitudinal Study of Adult Health (ELSA-Brasil). METHODS We included subjects with normal thyroid function (thyrotropin (TSH): 0.4-4.0 mIU/l, and normal free thyroxine (FT4: 0.8-1.9 ng/dl) and SCH (TSH > 4.0 mIU/l and normal FT4) evaluated for cf-PWV in a crosssectional analysis. We excluded individuals using medications that interfere in thyroid function, antihypertensives, or diuretics, and subjects with chronic kidney disease or previous cardiovascular disease. Generalized linear and logistic regression models evaluated cf-PWV as a dependent variable and SCH as an independent variable, adjusted for cardiovascular risk factors. RESULTS Of 8,341 subjects (52.3% women), 7,878 (94.4%) were euthyroid and 463 (5.6%) showed SCH. The median age was 50 years (interquartile range: 44-56). The groups differed by age, sex, body mass index, glomerular filtration rate, and C-reactive protein. SCH was not associated with cf-PWV in the full-adjusted linear model (β = -0.039; P = 0.562) and with cf-PWV >75th percentile in the full-adjusted logistic model (odds ratio = 0.94; 95% confidence interval = 0.72-1.22). CONCLUSION In a large sample, SCH was not associated with increased cf-PWV. Artery stiffness; Blood pressure; Cardiovascular risk factors; Hypertension; Pulse wave velocity; Subclinical atherosclerosis; Subclinical hypothyroidism; Thyroid dysfunction antihypertensive agent; C reactive protein; diuretic agent; thyrotropin; adult; Article; body mass; Brazil; Brazilian; cardiovascular disease; cardiovascular risk; carotid artery pulse; carotid femoral pulse wave velocity; chronic kidney failure; cross-sectional study; disease association; female; femoral artery; free thyroxine index; glomerulus filtration rate; human; hypertension; longitudinal study; major clinical study; male; priority journal; pulse wave; subclinical hypothyroidism; thyroid function;Objective: There is little information about the association between thyrotrophin (TSH) levels and coronary artery calcification (CAC). Our aim was to analyse the association between TSH quintiles and subclinical atherosclerosis measured by CAC, using baseline data from the Brazilian Longitudinal Study of Adult Health (ELSA-Brasil). Design: Cross-sectional study. Patients: We excluded individuals using medications that affect thyroid function and who self-reported cardiovascular disease. We included euthyroid subjects and individuals with subclinical hypothyroidism (SCHypo) and subclinical hyperthyroidism (SCHyper). Logistic regression models evaluated CAC >100 Agatston units as the dependent variable, and increasing quintiles of TSH as the independent variable, adjusted for demographic and cardiovascular risk factors. Results: Our sample included 3836 subjects, mean age 49 years (interquartile range 44-56); 1999 (52.1%) were female, 3551 (92.6%) were euthyroid, 239 (6.2%) had SCHypo and 46 (1.2%) had SCHyper. The frequency of women, White people and never smokers as well as body mass index and insulin resistance increased according to quintiles. The 1st quintile for TSH (0-0.99 mIU/L) was associated with CAC >100, using the 3rd quintile (1.39-1.85 mIU/L) as reference (adjusted OR=1.57, 95% CI: 1.05-2.35, P=.027), but no association was shown for the 5th quintile (2.68-35.5 mIU/L) compared to the 3rd. Restricting the analysis to euthyroid subjects did not change the results. For women, but not for men, we observed a U-shaped curve with 1st and 5th TSH quintiles associated with CAC>100. Conclusion: Low and low-normal (1st quintile) TSH levels were associated with CAC>100 Agatston units in a sample with subclinical thyroid disorders and euthyroid subjects. © 2017 John Wiley & Sons Ltd. Cardiovascular disease; Coronary artery calcification; Subclinical atherosclerosis; Subclinical hyperthyroidism; Subclinical hypothyroidism; Subclinical thyroid disease; Thyroid disorders; Thyrotrophin levels ;[No abstract available] atherosclerosis; Editorials; prognosis; risk factors; sample size; vascular calcification ;Background The use of non-contrast cardiac computed tomography measurements to predict heart failure (HF) has not been studied. In the present study we evaluated the prognostic value of left ventricular area adjusted for the body surface area (LVA-BSA) measured by non-contrast cardiac CT to predict incident HF and cardiovascular disease (CVD) events. Methods We studied left ventricular dimensions and calculated LVA-BSA in 6781 participants of the MESA study (mean age: was 62 ± 10 years, 53% females; 62% non-white) free from prior HF who underwent non-contrast cardiac CT to evaluate the coronary artery calcium score (CAC) at baseline and were followed up for a median of 10.2 years. Results During follow up, 237 (3.5%) incident HF and 475 (7.0%) CVD events occurred. After adjustment for clinical variables and CAC, LVA-BSA was significantly associated with incident HF (hazard ratio [HR]: 1.10 per 100 mm2/m2, p &lt; 0.001) and CVD events (HR: 1.07 per 100 mm2/m2, p &lt; 0.001). The area under the ROC curve for the prediction of incident HF improved from 0.787 on a model including only risk factors to 0.798 when CAC was added (p = 0.02), and to 0.816 with the additional inclusion of LVA-BSA (p = 0.007). Similar improvements for the prediction of CVD events were noted. Conclusion In an ethnically diverse population of asymptomatic individuals free from baseline CVD or HF, the left ventricular area measured by non-contrast cardiac CT is a strong predictor of incident HF events beyond traditional risk factors and CAC score. © 2016 Society of Cardiovascular Computed Tomography Heart failure; Left ventricle size; Non-contrast cardiac computed tomography; Prognosis antihypertensive agent; antilipemic agent; adult; antihypertensive therapy; Article; body mass; cardiovascular disease; cardiovascular risk; cerebrovascular disease; computed tomography scanner; computer assisted tomography; coronary artery calcium score; diabetes mellitus; diastolic blood pressure; female; follow up; Han Chinese; heart failure; heart left ventricle; human; hypertension; left anterior descending coronary artery; major clinical study; male; prediction; priority journal; prospective study; receiver operating characteristic; systolic blood pressure; aged; area under the curve; blood vessel calcification; chi square distribution; clinical trial; computed tomographic angiography; coronary angiography; coronary artery disease; diagnostic imaging; epidemiology; ethnology; heart failure; heart left ventricle hypertrophy; heart ventricle; incidence; middle aged; multicenter study; multivariate analysis; pathophysiology; predictive value; procedures; proportional hazards model; risk assessment; risk factor; time factor; United States; very elderly; Aged; Aged, 80 and over; Area Under Curve; Chi-Square Distribution; Computed Tomography Angiography; Coronary Angiography; Coronary Artery Disease; Female; Heart Failure; Heart Ventricles; Humans; Hypertrophy, Left Ventricular; Incidence; Male; Middle Aged; Multivariate Analysis; Predictive Value of Tests; Proportional Hazards Models; Prospective Studies; Risk Assessment; Risk Factors; ROC Curve; Time Factors; United States; Vascular Calcification;Background and aims Familial hypercholesterolemia (FH) is a common genetic disorder characterized by elevated blood cholesterol, increased prevalence of subclinical atherosclerosis and high risk of premature coronary heart disease. However, this risk is not explained solely by elevated LDL-cholesterol concentrations, and other factors may influence atherosclerosis development. There is evidence that increased adiposity may predispose to atherosclerosis in FH. Epicardial fat has been associated with subclinical coronary atherosclerosis in the general population. This study evaluated the association of epicardial fat (EFV) volume with the presence and extent of subclinical coronary atherosclerosis detected by computed tomography angiography in FH patients. Methods Ninety-seven FH subjects (35% male, mean age 45 ± 13 years, LDL-C 281 ± 56 mg/dL, 67% with proven molecular defects) underwent computed tomography angiography and coronary artery calcium (CAC) scoring. EFV was measured in non-contrast images using a semi-automated method. Segment-stenosis score (SSS) and segment-involvement score (SIS) were calculated. Multivariate Poisson regression was utilized to assess an independent association of EFV with coronary atherosclerotic burden. Results EFV was positively associated with age, body mass index, waist circumference, blood glucose, the presence of the metabolic syndrome components, but not with LDL-C. After adjusting for confounders and abdominal circumference, an independent association (shown as β coefficients and 95% confidence intervals) of EVF with CAC scores [β = 0.263 (0.234; 0.292), p=0.000], SIS [β = 0.304 (0.141; 0.465) p=0.000] and SSS [β = 0.296 (0.121; 0.471), p=0.001] was found. Conclusions In FH, EFV was independently associated with coronary atherosclerotic presence and severity. © 2016 Elsevier Ireland Ltd Atherosclerosis; Calcium score; Computed tomography; Epicardial fat; Familial hypercholesterolemia; Obesity ;Background and aim Although subclinical hypothyroidism (SCH) is associated with cardiovascular risk, there is scarce data about subclinical atherosclerosis in subjects with SCH. We aimed to analyze the association between SCH and carotid intima-media thickness (IMT) using baseline data from the Brazilian Longitudinal Study of Adult Health (ELSA-Brasil). Methods and Results We included subjects with normal thyroid function (TSH: 0.4–4.0 mIU/l, and normal free thyroxine (FT4): 0.8–1.9 ng/dl) and SCH (TSH ≥ 4.0 mIU/l and normal FT4) evaluated for IMT in a cross-sectional analysis. We excluded individuals using medications that affect thyroid function and those with a history of cardiovascular disease. We performed logistic and linear regression models to evaluate the association with IMT (mean values and categorized at the 75th percentile) as a dependent variable and SCH as an independent variable, adjusted for other cardiovascular risk factors. From 8623 subjects (median age of 50 years; interquartile range: 44–57), 4624 (53.6%) were women, 8095 (93.9%) were euthyroid, and 528 (6.1%) had SCH. Groups varied in age, body mass index, Framingham risk score, Homeostasis Model Assessment for Insulin Resistance (HOMA-IR), C-reactive protein, as well as, IMT, that were all higher in SCH compared to euthyroid participants. SCH is associated with IMT as a continuous variable (β = 0.010, P = 0.036) and IMT >75th percentile: OR = 1.30 (95% CI = 1.06–1.59) in logistic models. Conclusion Individuals with SCH presented higher IMT compared with euthyroid subjects, even after adjustment for potential confounders. IMT was independently associated with SCH in the baseline of the ELSA-Brasil study. © 2016 The Italian Society of Diabetology, the Italian Society for the Study of Atherosclerosis, the Italian Society of Human Nutrition, and the Department of Clinical Medicine and Surgery, Federico II University Cardiovascular risk factors; Carotid intima-media thickness; Subclinical atherosclerosis; Subclinical hypothyroidism; Thyroid dysfunction C reactive protein; cholesterol; creatinine; glucose; glycosylated hemoglobin; high density lipoprotein cholesterol; insulin; low density lipoprotein cholesterol; thyrotropin; thyroxine; triacylglycerol; biological marker; thyroid hormone; adult; arterial wall thickness; Article; blood pressure; body mass; cardiovascular disease; cardiovascular risk; colorimetry; cross-sectional study; diabetes mellitus; enzyme immunoassay; euthyroidism; female; Framingham risk score; high performance liquid chromatography; human; incidence; insulin resistance; longitudinal study; major clinical study; male; nephelometry; priority journal; questionnaire; subclinical hypothyroidism; thyroid function; aged; asymptomatic disease; blood; Brazil; Carotid Artery Diseases; chi square distribution; clinical trial; complication; diagnostic imaging; hypothyroidism; middle aged; multicenter study; multivariate analysis; odds ratio; predictive value; prospective study; risk factor; statistical model; thyroid function test; Adult; Aged; Asymptomatic Diseases; Biomarkers; Brazil; Carotid Artery Diseases; Carotid Intima-Media Thickness; Chi-Square Distribution; Cross-Sectional Studies; Female; Humans; Hypothyroidism; Linear Models; Logistic Models; Longitudinal Studies; Male; Middle Aged; Multivariate Analysis; Odds Ratio; Predictive Value of Tests; Prospective Studies; Risk Factors; Thyroid Function Tests; Thyroid Hormones;Background: To the best of our knowledge, there are no studies evaluating the influence of the unit of the first contact on the frequency and time of pharmacological treatment during an acute coronary syndrome (ACS) event. Objectives: The main objective was to investigate if the unit of first contact influenced the frequency and time of aspirin treatment in the Strategy of Registry of Acute Coronary Syndrome (ERICO) study. Methods: We analyzed the pharmacological treatment time in 830 ERICO participants - 700 individuals for whom the hospital was the unit of first contact and 130 who initially sought primary care units. We built logistic regression models to study whether the unit of first contact was associated with a treatment time of less than three hours. Results: Individuals who went to primary care units received the first aspirin dose in those units in 75.6% of the cases. The remaining 24.4% received aspirin at the hospital. Despite this finding, individuals from primary care still had aspirin administered within three hours more frequently than those who went to the hospital (76.8% vs 52.6%; p<0.001 and 100% vs. 70.7%; p=0.001 for non ST-elevation ACS and ST-elevation myocardial infarction, respectively). In adjusted models, individuals coming from primary care were more likely to receive aspirin more quickly (odds ratio: 3.66; 95% confidence interval: 2.06-6.51). Conclusions: In our setting, individuals from primary care were more likely to receive aspirin earlier. Enhancing the ability of primary care units to provide early treatment and safe transportation may be beneficial in similar settings. © 2016, Arquivos Brasileiros de Cardiologia. All rights reserved. Acute coronary syndrome / mortality; Anticoagulants; Aspirin / administration & dosage; Cohort studies; Primary health care; Time-to-treatment acetylsalicylic acid; clopidogrel; heparin; troponin I; acetylsalicylic acid; anticoagulant agent; antithrombocytic agent; fibrinolytic agent; heparin; ticlopidine; acute coronary syndrome; adult; aged; Article; controlled study; female; fibrinolytic therapy; human; length of stay; major clinical study; male; non ST segment elevation myocardial infarction; primary medical care; ST segment elevation myocardial infarction; study design; time to treatment; unstable angina pectoris; acute coronary syndrome; analogs and derivatives; Brazil; educational status; middle aged; nonparametric test; primary health care; prospective study; statistics and numerical data; time factor; time to treatment; Acute Coronary Syndrome; Aged; Anticoagulants; Aspirin; Brazil; Educational Status; Female; Fibrinolytic Agents; Heparin; Humans; Male; Middle Aged; Platelet Aggregation Inhibitors; Primary Health Care; Prospective Studies; Statistics, Nonparametric; Ticlopidine; Time Factors; Time-to-Treatment;Objectives: African Americans are characterized by higher heart rate variability (HRV), a finding ostensibly associated with beneficial health outcomes. However, these findings are at odds with other evidence that blacks have worse cardiovascular outcomes. Here, we examine associations in a large cohort from the ELSA-Brasil study and determined whether these effects are mediated by discrimination. Methods: Three groups were compared on the basis of self-declared race: ""black"" (n = 2,020), ""brown"" (n = 3,502), and ""white"" (n = 6,467). Perceived discrimination was measured using a modified version of the Everyday Discrimination Scale. Resting-state HRV was extracted from 10-minute resting-state electrocardiograms. Racial differences in HRV were determined by regression analyses weighted by propensity scores, which controlled for potentially confounding variables including age, sex, education, and other health-related information. Nonlinear mediation analysis quantified the average total effect, comprising direct (race-HRV) and indirect (race-discrimination-HRV) pathways. Results: Black participants displayed higher HRV relative to brown (Cohen's d = 0.20) and white participants (Cohen's d = 0.31). Brown relative to white participants also displayed a small but significantly higher HRV (Cohen's d = 0.14).Discrimination indirectly contributed to the effects of race on HRV. Conclusions: This large cohort from the Brazilian population shows that HRV is greatest in black, followed by brown, relative to white participants. The presence of higher HRVin these groups may reflect a sustained compensatory psychophysiological response to the adverse effects of discrimination. Additional research is needed to determine the health consequences of these differences in HRVacross racial and ethnic groups. © Copyright 2016 by the American Psychosomatic Society. discrimination; heart rate variability; HRV; mediation analysis; propensity score weighting; race adult; Article; Brazilian; cohort analysis; confounding variable; controlled study; electrocardiogram; female; genotype environment interaction; heart rate variability; human; major clinical study; male; priority journal; propensity score; race difference; racism; regression analysis; resting heart rate; aged; ancestry group; Black person; Brazil; Caucasian; ethnology; government employee; heart rate; middle aged; physiology; statistics and numerical data; Adult; African Continental Ancestry Group; Aged; Brazil; Continental Population Groups; European Continental Ancestry Group; Government Employees; Heart Rate; Humans; Middle Aged; Racism;Background - Individuals with cardiac sarcoidosis have an increased risk of ventricular arrhythmia and death. Several small cohort studies have evaluated the ability of late gadolinium enhancement (LGE) by cardiac magnetic resonance imaging (MRI) to predict adverse cardiovascular events. However, studies have yielded inconsistent results, and some analyses were underpowered. Therefore, we sought to systematically review and perform meta-analysis of the prognostic value of cardiac MRI for patients with known or suspected cardiac sarcoidosis. Methods and Results - We systematically searched for cohort studies of patients with known sarcoidosis with suspected cardiac involvement who underwent cardiac MRI with LGE with at least 12 months of either prospective or retrospective follow-up data regarding post-MRI adverse cardiovascular outcomes. We identified 7 studies of 694 subjects (mean age 53; 42% men).One hundred and ninety-nine patients (29%) were LGE positive. All-cause mortality occurred in 19 LGE-positive versus 17 LGE-negative subjects (annualized incidence, 3.1% versus 0.6%). The pooled relative risk was 3.38 (95% confidence interval, 1.07-10.7; P=0.04). Cardiovascular mortality occurred in 10 LGE-positive versus 2 LGE-negative subjects (annualized incidence, 1.9% versus 0.3%; relative risk 10.7 [95% confidence interval, 1.34-86.3]; P=0.03). Ventricular arrhythmia occurred in 41 LGE-positive versus 0 LGE-negative subjects (annualized incidence, 5.9% versus 0%; relative risk 19.5 [95% confidence interval, 2.68-143]; P=0.003). A combined end point of death or ventricular arrhythmia occurred in 64 LGE-positive versus 18 LGE-negative subjects (annualized incidence, 8.8% versus 0.6%; relative risk 6.20 [95% confidence interval, 2.47-15.6]; P<0.001). There was no significant heterogeneity for any outcomes. Conclusions - LGE is associated with future cardiovascular death and ventricular arrhythmia among patients referred to MRI for known or suspected cardiac sarcoidosis. © 2016 American Heart Association, Inc. cardiac arrhythmia; cardiomyopathies; heart conduction system; magnetic resonance imaging; sarcoidosis contrast medium; gadolinium; Arrhythmias, Cardiac; cardiac muscle; cardiomyopathy; chi square distribution; Death, Sudden, Cardiac; diagnostic imaging; disease course; female; heart ventricle function; human; male; meta analysis; middle aged; mortality; nuclear magnetic resonance imaging; odds ratio; pathology; pathophysiology; predictive value; prognosis; risk factor; sarcoidosis; Arrhythmias, Cardiac; Cardiomyopathies; Chi-Square Distribution; Contrast Media; Death, Sudden, Cardiac; Disease Progression; Female; Gadolinium; Humans; Magnetic Resonance Imaging; Male; Middle Aged; Myocardium; Odds Ratio; Predictive Value of Tests; Prognosis; Risk Factors; Sarcoidosis; Ventricular Function;Introduction The relationship between migraine and coronary heart disease (CHD) remains controversial. We aimed to investigate the association of subclinical atherosclerosis and migraine with or without aura compared to a non-migraine subgroup (reference) in a large Brazilian multicentric cohort study, the Brazilian Longitudinal Study of Adult Health (ELSA-Brasil). Methods Migraine diagnostic was based on International Headache Society criteria, and aura symptoms were validated by a medical doctor in a sub-sample of the ELSA-Brasil, who also underwent coronary artery calcium score (CAC) and carotid intima-media thickness (C-IMT) evaluations. Subclinical atherosclerosis indexes (CAC and C-IMT) were analyzed as dependent variables and migraine (all, with aura, without aura) as an independent variable in the linear and multinomial logistic regression models adjusted for possible confounders. Results Of 3217 ELSA participants free from CVD at baseline, we found a migraine frequency of 11.9% (5.1% with aura and 6.8% without aura). Overall, migraineurs were mostly women, younger and had lower frequency of CV risk factors, such as hypertension, diabetes and low HDL-cholesterol, compared to non-migraineurs. The strongest inverse correlation between migraine and subclinical atherosclerosis was verified with CAC score. However, all associations lost their significance after multivariate adjustment. Conclusions In this cross-sectional evaluation of the ELSA study, migraine was not associated with subclinical atherosclerosis, regardless of aura symptoms. © International Headache Society. Migraine with aura; observational study; subclinical atherosclerosis adult; arterial wall thickness; Brazil; coronary artery disease; cross-sectional study; female; human; longitudinal study; male; middle aged; Migraine Disorders; Adult; Brazil; Carotid Intima-Media Thickness; Coronary Artery Disease; Cross-Sectional Studies; Female; Humans; Longitudinal Studies; Male; Middle Aged; Migraine Disorders;Aims: Non-alcoholic hepatic steatosis (HS) is associated with hypertension and increased cardiovascular risk. While Blood pressure hyper-reactive response (HRR) during peak exercise indicates an increased risk of incident hypertension and increased cardiovascular risk, no data on the association of non-alcoholic HS and HRR exists. In this study, we have evaluated the association of HS with HRR. Methods: We included 13 410 consecutive individuals with a mean age: 42.4 6 8.9 years, 3561 (26.6%) female with normal resting blood pressure and without a previous diagnosis of hypertension, who underwent symptom limited exercise treadmill test, abdominal ultrasonography and clinical and laboratory evaluation. HS was detected by abdominal ultrasonography. HRR was defined by a peak exercise systolic blood pressure > 220mmHg and/or elevation of 15mmHg or more in diastolic blood pressure from rest to peak exercise. Results: The prevalence of HS was 29.5% (n = 3956). Overall, 4.6% (n = 619) of the study population presented a HRR. Subjects with HS had a higher prevalence of HRR (8.1 vs. 3.1%, odds ratio 2.8, 95% CI 2.4-3.3, P < 0.001). After adjustment for body mass index, waist circumference, fasting plasma glucose and low density lipoprotein cholesterol, HS (odds ratio 1.4, 95% CI 1.1-1.6, P = 0.002) remained independently associated with HRR. HS was additive to obesity markers in predicting exercise HRR. Conclusions: Non-alcoholic HS is independently associated with hyper-reactive exercise blood pressure response. © The Author 2016.  adult; Article; clinical evaluation; controlled study; disease association; echography; female; human; hypertension; laboratory test; major clinical study; male; middle aged; nonalcoholic fatty liver; obesity; priority journal; prospective study; treadmill exercise; treadmill test;Background: The most appropriate score for evaluating the pretest probability of obstructive coronary artery disease (CAD) is unknown. We sought to compare the Diamond-Forrester (DF) score with the 2 CAD consortium scores recently recommended by the European Society of Cardiology. Methods: We included 2274 consecutive patients (age, 56±13 years; 57% male) without prior CAD referred for coronary computed tomographic angiography. Computed tomographic angiography findings were used to determine the presence or absence of obstructive CAD (≥50% stenosis). We compared the DF score with the 2 CAD consortium scores with respect to their ability to predict obstructive CAD and the potential implications of these scores on the downstream use of testing for CAD, as recommended by current guidelines. Results: The DF score did not satisfactorily fit the data and resulted in a significant overestimation of the prevalence of obstructive CAD (P<0.001); the CAD consortium basic score had no significant lack of fitness; and the CAD consortium clinical provided adequate goodness of fit (P=0.39). The DF score had a lower discrimination for obstructive CAD, with an area under the receiver-operating characteristics curve of 0.713 versus 0.752 and 0.791 for the CAD consortium models (P<0.001 for both). Consequently, the use of the DF score was associated with fewer individuals being categorized as requiring no additional testing (8.3%) compared with the CAD consortium models (24.6% and 30.0%; P<0.001). The proportion of individuals with a high pretest probability was 18% with the DF and only 1.1% with the CAD consortium scores (P<0.001) Conclusions: Among contemporary patients referred for noninvasive testing, the DF risk score overestimates the risk of obstructive CAD. On the other hand, the CAD consortium scores offered improved goodness of fit and discrimination; thus, their use could decrease the need for noninvasive or invasive testing while increasing the yield of such tests. © 2016 American Heart Association, Inc. chest pain; coronary artery disease; prognosis; risk assessment adult; Article; cardiovascular disease assessment; cardiovascular mortality; cardiovascular risk; computed tomographic angiography; Coronary Artery Disease Consortium Pretest Probability score; coronary artery obstruction; Diamond Forrester score; Europe; female; follow up; human; major clinical study; male; medical society; middle aged; practice guideline; predictive value; Pretest Probability score; prevalence; priority journal; unstable angina pectoris; aged; area under the curve; cardiology; Coronary Disease; diagnostic imaging; heart function test; receiver operating characteristic; register; risk assessment; severity of illness index; spiral computer assisted tomography; treatment outcome; Adult; Aged; Area Under Curve; Cardiology; Coronary Disease; Heart Function Tests; Humans; Middle Aged; Registries; Risk Assessment; ROC Curve; Severity of Illness Index; Societies, Medical; Tomography, Spiral Computed; Treatment Outcome;Previous studies of the association between symptoms of anxiety or depression and coronary artery calcium (CAC) have produced heterogeneous results. Our aim was to investigate whether psychopathological symptoms were associated with CAC in a cross-sectional analysis of the Brazilian Longitudinal Study of Adult Health (ELSA-Brasil) baseline. We analyzed data from 4,279 ELSA-Brasil subjects (aged 35 to 74 years) from the São Paulo site without previous cardiovascular disease who underwent CAC score assessment at baseline. Prevalent CAC was defined as a CAC score >0. Anxiety and depressive symptoms were assessed using the Clinical Interview Schedule-Revised (CIS-R). We built binary logistic regression models to determine whether CIS-R scores, anxiety, or depression were associated with prevalent CAC. Prevalent CAC was found in 1,211 subjects (28.3%). After adjustment for age and gender, a direct association between CIS-R scores and prevalent CAC was revealed (odds ratio for 1-SD increase: 1.12; 95% confidence interval [CI] 1.04 to 1.22). This association persisted after multivariate adjustment (odds ratio for 1-SD increase 1.11; 95% CI 1.02 to 1.20). No independent associations were found for specific diagnoses of anxiety or depression and prevalent CAC. In post hoc models, a significant interaction term (p = 0.019) suggested a stronger association in older subjects. In conclusion, psychopathological symptoms were directly associated with coronary atherosclerosis in the ELSA-Brasil baseline in adjusted models, and this association seems to be stronger in older subjects. © 2016 Elsevier Inc.  calcium; adult; age distribution; anxiety; Article; blood pressure; cohort analysis; coronary artery atherosclerosis; coronary artery calcium score; cross-sectional study; depression; diabetes mellitus; disease association; dyslipidemia; female; follow up; generalized anxiety disorder; human; hypertension; income; major clinical study; major depression; male; mental disease; middle aged; multicenter study; priority journal; sex difference; smoking; aged; anxiety; Brazil; calcinosis; coronary artery disease; coronary blood vessel; depression; diagnostic imaging; longitudinal study; odds ratio; statistical model; x-ray computed tomography; Adult; Aged; Anxiety; Brazil; Calcinosis; Coronary Artery Disease; Coronary Vessels; Cross-Sectional Studies; Depression; Female; Humans; Logistic Models; Longitudinal Studies; Male; Middle Aged; Odds Ratio; Tomography, X-Ray Computed;[No abstract available]  ;Background: Coronary artery calcium (CAC) has been demonstrated to independently predict the risk of cardiovascular events and all-cause mortality, especially among White populations. Although the population distribution of CAC has been determined for several White populations, the distribution in ethnically admixed groups has not been well established. Hypothesis: The CAC distribution, stratified for age, gender and race, is similar to the previously described distribution in the MESA study. Methods: The Brazilian Longitudinal Study of Adult Health (ELSA-Brasil) is a prospective cohort study designed to investigate subclinical cardiovascular disease in 6 different centers of Brazil. Similar to previous studies, individuals with self-reported coronary or cerebrovascular disease and those treated for diabetes mellitus were excluded from analysis. Results: Percentiles of CAC distribution were estimated with nonparametric techniques. The analysis included 3616 individuals (54% female; mean age, 50 years). As expected, CAC prevalence and burden were steadily higher with increasing age, as well as increased in men and in White individuals. Our results revealed that for a given CAC score, the ELSA-derived CAC percentile would be lower in men compared with the Multi-Ethnic Study of Atherosclerosis (MESA) and would be higher in women compared with MESA. Conclusions: In our sample of the Brazilian population, we observed significant differences in CAC by sex, age, and race. Adjusted for age and sex, low-risk individuals from the Brazilian population present with significantly lower CAC prevalence and burden compared with other low-risk individuals from other worldwide populations. Using US-derived percentiles in Brazilian individuals may lead to overestimating relative CAC burden in men and underestimating relative CAC burden in women. © 2016 Wiley Periodicals, Inc.  ;Background: Chagas disease (CD) is a frequent cause of dilated cardiomyopathy (CMP) in developing countries, leading to clinical heart failure and worse prognosis. Therefore, the development and evolution of this CMP has always been a major topic in numbers of previous studies. A comprehensive echocardiographic study of left ventricular (LV) mechanics, fully assessing myocardial contraction, has never been done before. This could help characterize and improve the understanding of the evolution of this prevalent CMP. Methods: A total of 47 chagasic and 84 control patients were included in this study and allocated in groups according to LV ejection fraction. 2D-Echocardiogram was acquired for LV mechanics analysis by speckle tracking echocardiography. Results: Mean age of chagasic individuals was 55y and 16 (34 %) were men. Significant difference was found in global longitudinal velocity analysis, with lower values in indeterminate form. In the group with severe systolic dysfunction, a paradoxical increase in longitudinal and apical radial displacements were demonstrated. In parallel, segmental analyzes highlighted lower values of radial displacement, strain and strain rate into inferior and inferolateral walls, with increase of these values in septal and anterior walls. Conclusion: Chagasic CMP has a vicarious pattern of contraction in the course of its evolution, defined by reduced displacement and strain into inferior and posterior walls with paradoxical increase in septal and anterior segments. Also, lower longitudinal velocities were demonstrated in CD indeterminate form, which may indicate an incipient myocardial injury. © 2016 Lima et al. Cardiac mechanics; Cardiomyopathies; Speckle tracking echocardiography; Strain adult; Article; Chagas disease; controlled study; female; heart left ventricle contraction; heart left ventricle ejection fraction; human; left ventricular systolic dysfunction; major clinical study; male; prospective study; radiological parameters; region of interest; speckle tracking echocardiography; two dimensional echocardiography; Chagas cardiomyopathy; diagnostic imaging; echocardiography; follow up; heart left ventricle function; heart ventricle; middle aged; pathophysiology; physiology; procedures; prognosis; randomized controlled trial; Adult; Chagas Cardiomyopathy; Echocardiography; Female; Follow-Up Studies; Heart Ventricles; Humans; Male; Middle Aged; Prognosis; Prospective Studies; Ventricular Function, Left;[No abstract available] cardiac imaging techniques; cardiovascular diseases; diagnostic techniques cardiovascular; prevention & control adult; Article; case report; computed tomographic angiography; coronary artery disease; diagnostic accuracy; diagnostic value; follow up; heart infarction; human; male; middle aged; primary prevention; priority journal; sensitivity and specificity; single photon emission computer tomography; thorax pain; treadmill exercise; unstable angina pectoris; Chest Pain; complication; coronary angiography; coronary artery disease; diagnostic imaging; procedures; risk reduction; x-ray computed tomography; hydroxymethylglutaryl coenzyme A reductase inhibitor; Chest Pain; Coronary Angiography; Coronary Artery Disease; Humans; Hydroxymethylglutaryl-CoA Reductase Inhibitors; Male; Middle Aged; Risk Reduction Behavior; Tomography, X-Ray Computed;Background: Research has linked high-frequency heart rate variability (HF-HRV) to cognitive function. The present study adopts a modern path modelling approach to understand potential causal pathways that may underpin this relationship. Methods: Here we examine the association between resting-state HF-HRV and executive function in a large sample of civil servants from Brazil (N = 8114) recruited for the Brazilian Longitudinal Study of Adult Health (ELSA-Brasil). HF-HRV was calculated from 10-min resting-state electrocardiograms. Executive function was assessed using the trail-making test (version B). Results and conclusions: Insulin resistance (a marker of type 2 diabetes mellitus) and carotid intima-media thickness (subclinical atherosclerosis) mediated the relationship between HRV and executive function in seriatim. A limitation of the present study is its cross-sectional design; therefore, conclusions must be confirmed in longitudinal study. Nevertheless, findings support that possibility that HRV provides a 'spark' that initiates a cascade of adverse downstream effects that subsequently leads to cognitive impairment. © 2016. Carotid intima-media thickness; Cognition; Executive function; Heart rate variability; HOMA-IR; Homeostasis model assessment index for insulin resistance; HRV; IMT; Mediation analyses; Path modelling; TMT; Trail making test adult; aged; arterial wall thickness; Article; Brazilian; cross-sectional study; executive function; female; heart rate variability; human; human experiment; insulin resistance; male; normal human; priority journal; trail making test;Limited data exist on how noninvasive testing options compare for evaluating patients with suspected stable coronary artery disease. In this study, we have performed a meta-analysis of randomized controlled trials comparing the use of coronary computed tomographic angiography (CTA) with usual care. Methods and Results-We systematically searched databases for randomized clinical trials comparing coronary CTA with usual care for the evaluation of stable chest pain with follow-up for cardiovascular outcomes. The primary outcomes were myocardial infarction and all-cause mortality. We identified 4 randomized clinical trials, including a total of 7403 patients undergoing coronary CTA and 7414 patients undergoing usual care with various functional testing approaches. When compared with usual care, the use of coronary CTA was associated with a significant reduction in the annual rate of myocardial infarction (rate ratio, 0.69; 95% confidence interval, 0.49-0.98; P=0.038), but no difference was found in all-cause mortality. There was a trend toward more invasive coronary angiographies among patients undergoing coronary CTA (odds ratio, 1.33; 95% confidence interval, 0.95-1.84; P=0.09) and higher use of coronary revascularizations (odds ratio, 1.77; 95% confidence interval, 1.14-2.75). Significant heterogeneity for invasive coronary angiography and revascularization was noted, which was attributable to the Scottish Computed Tomography of the HEART (SCOT-HEART) study. We found no difference in the rate of admission for cardiac chest pain (rate ratio, 1.21; 95% confidence interval, 0.95-1.54). Conclusions-In comparison to usual care, an initial investigation of suspected stable coronary artery disease using coronary CTA resulted in a significant reduction in myocardial infarction, an increased incidence of c"
Adriano M. Alencar,Universidade de São Paulo - Instituto de Física,"We investigate the formation and evolution of silver nanoparticles in the non-irradiated thermally treated glass system Ag:(37–x)Na2O–10CaO–xAl2O3–53SiO2 mol%, where x = 0.0, 0.5, 1, 2 and 5, doped with 0.2 mol% AgNO3. Using optical absorption and transmission electron microscopy we studied the effect of the gradual addition of Al2O3 to the chemical composition of the soda aluminosilicate glasses. A quantitative method for the evaluation of the relative nanoparticles concentration, proportional to the optical absorption intensity of the silver Surface Plasmon Resonance band, was developed and applied to the study of the behavior of the number of particles precipitated in samples thermally treated at temperatures of 400, 500, and 600 °C. Three mechanisms were found, as consequence of the substitution of SiO4 by AlO4 glass forming local tetrahedral structural units, resulting in the gradual suppression of the negatively charged NBOs−, which are the only reducing sources available in the present non-irradiated glass system. The effects produced by the glass chemical and structural changes on the Ag nanoparticles production are discussed. © 2016 Elsevier B.V. Optical absorption; Silver nanoparticles; Soda-lime silicate glasses; Surface plasmon resonance; Thermal treatment; Transmission electron microscopy Aluminum; Electromagnetic wave absorption; Electron microscopy; Glass; Heat treatment; High resolution transmission electron microscopy; Light absorption; Light transmission; Lime; Metal nanoparticles; Nanoparticles; Plasmons; Silicates; Surface plasmon resonance; Transmission electron microscopy; Absorption intensity; Aluminosilicate glass; Chemical compositions; Formation and evolutions; Quantitative method; Silver nanoparticles; Soda lime silicate glass; Tetrahedral structural units; Silver;Diesel exhaust particles (DEPs) from diesel engines produce adverse alterations in cells of the airways by activating intracellular signaling pathways and apoptotic gene overexpression, and also by influencing metabolism and cytoskeleton changes. This study used human bronchial epithelium cells (BEAS-2B) in culture and evaluates their exposure to DEPs (15ug/mL for 1 and 2 h) in order to determine changes to cell rheology (viscoelasticity) and gene expression of the enzymes involved in oxidative stress, apoptosis, and cytotoxicity. BEAS-2B cells exposed to DEPs were found to have a significant loss in stiffness, membrane stability, and mitochondrial activity. The genes involved in apoptosis [B cell lymphoma 2 (BCL-2 and caspase-3)] presented inversely proportional expressions (p = 0.05, p = 0.01, respectively), low expression of the genes involved in antioxidant responses [SOD1 (superoxide dismutase 1); SOD2 (superoxide dismutase 2), and GPx (glutathione peroxidase) (p = 0.01)], along with an increase in cytochrome P450, family 1, subfamily A, polypeptide 1 (CYP1A1) (p = 0.01). These results suggest that alterations in cell rheology and cytotoxicity could be associated with oxidative stress and imbalance between pro- and anti-apoptotic genes. © 2016, Springer-Verlag Berlin Heidelberg. Air pollution; Cell rheology; Cytotoxicity; Gene expression; Oxidative stress; Ultrafine particles antioxidant; BCL2 protein, human; caspase 3; CYP1A1 protein, human; cytochrome P450 1A1; exhaust gas; glutathione peroxidase; particulate matter; protein bcl 2; apoptosis; bronchus; cell line; cell survival; drug effects; epithelium cell; exhaust gas; flow kinetics; gene expression; genetics; human; metabolism; oxidative stress; particle size; particulate matter; pathology; toxicity; Antioxidants; Apoptosis; Bronchi; Caspase 3; Cell Line; Cell Survival; Cytochrome P-450 CYP1A1; Epithelial Cells; Gene Expression; Glutathione Peroxidase; Humans; Oxidative Stress; Particle Size; Particulate Matter; Proto-Oncogene Proteins c-bcl-2; Rheology; Vehicle Emissions;Capillarity is the study of interfaces between two immiscible liquids or between a liquid and a vapor. The theory of capillarity was created in the early 1800s, and it is applicable to mesoscopic and macroscopic (&gt;1 μm) systems. In general, macroscopic theories are expected to fail at the &lt;10 nm scales where molecular details may become relevant. In this work, we show that, surprisingly, capillarity theory (CT) provides satisfactory predictions at 2-10 nm scales. Specifically, we perform atomistic molecular dynamics (MD) simulations of water droplets and capillary bridges of different symmetry in contact with various surfaces. The underlying structure of the surfaces corresponds to hydroxilated (crystalline) silica which is modified to cover a wide range of hydrophobicity/hydrophilicity. In agreement with CT, it is found that water contact angle is independent of the droplet/bridge geometry and depends only on the hydrophobicity/hydrophilicity of the surface employed. In addition, CT provides the correct droplet/bridge profile for all (hydrophobic/hydrophilic) surfaces considered. Remarkably, CT works even for the very small droplets/bridges studied, for which the smallest dimension is 2 nm. It follows that the concepts of surface tension and contact angle are indeed meaningful at 2-10 nm scales even when, macroscopically, such concepts are not justified. In order to confirm the self-consistency of CT at 2-10 nm scales, we also calculate the capillary forces between different surfaces induced by water capillary bridges. These forces depend on the liquid-vapor surface tension of water, . Using CT, the calculated forces indicate that = 0.054 ± 0.001 N/m2. This is in agreement with the value = 0.056 ± 0.001 N/m2 obtained independently using the Kirkwood-Buff method, and it is consistent with values of reported in the literature for the present water model. Confirming the validity of CT at 2-10 nm scales has relevant implications in scientific applications, such as in our understanding of self-assembly processes at interfaces. We discuss briefly this and other consequences of the present results. © 2015 American Chemical Society.  Capillarity; Contact angle; Drop formation; Drops; Hydrophobicity; Liquids; Molecular dynamics; Self assembly; Surface tension; Atomistic computer simulation; Atomistic molecular dynamics; Hydrophobic and hydrophilic; Hydrophobic/hydrophilic; Liquid-vapor surfaces; Satisfactory predictions; Scientific applications; Self assembly process; Phase interfaces;The cytoskeleton (CSK) is a tensed fiber framework that supports, shapes and stabilizes the cell. The CSK is in a constant state of remodeling, moreover, which is an active non-equilibrium thermodynamic process. We report here that cytoskeletal remodeling involves reconfigurations that are not only sudden but also are transmitted to great distances within the cell in a fashion reminiscent of quakes in the Earth's crust. Remarkably, these events in the cell conform both qualitatively and quantitatively to empirical laws typical of earthquakes, including hierarchical fault structures, cumulative energy distributions following the Gutenberg-Richter law, and rate of after-shocks following Omori's law. While it is well-established that remodeling and stabilization of the cytoskeleton are non-equilibrium process, these new unanticipated observations establish that these processes are also remarkably non-local and strongly cooperative. © The Royal Society of Chemistry 2016.  Earthquakes; Structural geology; Cumulative energy; Earth's crust; Empirical laws; Fault structure; Gutenberg-Richter law; Non equilibrium; Non equilibrium thermodynamics; Non-equilibrium process; Stabilization;Aims Recent evidence shows the rigidity of vascular smooth muscle cells (VSMC) contributes to vascular mechanics. Arterial rigidity is an independent cardiovascular risk factor whose associated modifications in VSMC viscoelasticity have never been investigated. This study's objective was to evaluate if the arterial rigidity risk factors aging, African ancestry, female sex, smoking and diabetes mellitus are associated with VMSC stiffening in an experimental model using a human derived vascular smooth muscle primary cell line repository. Methods Eighty patients subjected to coronary artery bypass surgery were enrolled. VSMCs were extracted from internal thoracic artery fragments and mechanically evaluated using Optical Magnetic Twisting Cytometry assay. The obtained mechanical variables were correlated with the clinical variables: age, gender, African ancestry, smoking and diabetes mellitus. Results The mechanical variables Gr, G'r and G""r had a normal distribution, demonstrating an interindividual variability of VSMC viscoelasticity, which has never been reported before. Female sex and smoking were independently associated with VSMC stiffening: Gr (apparent cell stiffness) p = 0.022 and p = 0.018, R2 0.164; G'r (elastic modulus) p = 0.019 and p = 0.009, R2 0.184 and G''r (dissipative modulus) p = 0.011 and p = 0.66, R2 0.141. Conclusion Female sex and smoking are independent predictors of VSMC stiffening. This pro-rigidity effect represents an important element for understanding the vascular rigidity observed in post-menopausal females and smokers, as well as a potential therapeutic target to be explored in the future. There is a significant inter-individual variation of VSMC viscoelasticity, which is slightly modulated by clinical variables and probably relies on molecular factors. © 2015 Dinardo et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in anymedium, provided the original author and source are credited.  adult; aging; arterial stiffness; Article; Black person; coronary artery bypass surgery; diabetes mellitus; female; human; human cell; ischemic heart disease; major clinical study; male; predictive value; risk assessment; risk factor; sex difference; smoking; vascular smooth muscle cell; viscoelasticity; Young modulus; age; aged; Cardiovascular Diseases; cell culture; coronary artery bypass graft; cytology; female; middle aged; non insulin dependent diabetes mellitus; pathology; physiology; postmenopause; sex difference; vascular smooth muscle; African Continental Ancestry Group; Age Factors; Aged; Cardiovascular Diseases; Cells, Cultured; Coronary Artery Bypass; Diabetes Mellitus, Type 2; Elastic Modulus; Female; Humans; Male; Middle Aged; Muscle, Smooth, Vascular; Postmenopause; Risk Factors; Sex Factors; Smoking;Particulate matter from diesel exhaust (DEP) has toxic properties and can activate intracellular signaling pathways and induce metabolic changes. This study was conducted to evaluate the activation of extracellular signal-regulated kinase (ERK) and c-Jun N-terminal kinase (JNK) and to analyze the mucin profile (acid (AB+), neutral (PAS+), or mixed (AB/PAS+) mucus) and vacuolization (V) of tracheal explants after treatment with 50 or 100 μg/mL DEP for 30 or 60 min. Western blot analyses showed small increases in ERK1/2 and JNK phosphorylation after 30 min of 100 μg/mL DEP treatment compared with the control. An increase in JNK phosphorylation was observed after 60 min of treatment with 50 μg/mL DEP compared with the control. We did not observe any change in the level of ERK1/2 phosphorylation after treatment with 50 μg/mL DEP. Other groups of tracheas were subjected to histological sectioning and stained with periodic acid-Schiff (PAS) reagent and Alcian Blue (AB). The stained tissue sections were then subjected to morphometric analysis. The results obtained were compared using ANOVA. Treatment with 50 μg/mL DEP for 30 min or 60 min showed a significant increase (p&lt;0.001) in the amount of acid mucus, a reduction in neutral mucus, a significant reduction in mixed mucus, and greater vacuolization. Our results suggest that compounds found in DEPs are able to activate acid mucus production and enhance vacuolization and cell signaling pathways, which can lead to airway diseases. © 2014 Wiley Periodicals, Inc. Biomarkers; DEP; Explants; Histopathology; MAPK; Signal transduction Biomarkers; Body fluids; Cell death; Enzyme activity; Enzymes; Phosphorylation; Signal transduction; Signaling; C-jun N-terminal kinase; DEP; Diesel exhaust particulates; Explants; Extracellular signal-regulated kinase; Histopathology; Intracellular signaling pathways; MAPK; Cell signaling; diesel fuel; mitogen activated protein kinase; mucin; stress activated protein kinase; air pollutant; exhaust gas; mitogen activated protein kinase; mucin; particulate matter; stress activated protein kinase; animal experiment; animal tissue; apoptosis; Article; Bagg albino mouse; cell vacuole; chemical composition; controlled study; exhaust gas; explant; exposure; intracellular signaling; male; mouse; nonhuman; organ culture; particulate matter; priority journal; protein phosphorylation; scanning electron microscopy; tissue section; toxicity; trachea; Western blotting; air pollutant; animal; apoptosis; dose response; drug effects; epithelium cell; exhaust gas; metabolism; mucus; particulate matter; pathology; phosphorylation; signal transduction; toxicity; trachea; Mus; Air Pollutants; Animals; Apoptosis; Dose-Response Relationship, Drug; Epithelial Cells; Extracellular Signal-Regulated MAP Kinases; JNK Mitogen-Activated Protein Kinases; MAP Kinase Signaling System; Mice; Mice, Inbred BALB C; Mucins; Mucus; Particulate Matter; Phosphorylation; Signal Transduction; Trachea; Vehicle Emissions;Introduction: One of the fundamental structural elements of the cell is the cytoskeleton. Along with myosin, actin microfilaments are responsible for cellular contractions, and their organization may be related to pathological changes in myocardial tissue. Due to the complexity of factors involved, numerical modeling of the cytoskeleton has the potential to contribute to a better understanding of mechanical cues in cellular activities. In this work, a systematic method was developed for the reconstruction of an actomyosin topology based on the displacement exerted by the cell on a flexible substrate. It is an inverse problem which could be considered a phenomenological approach to traction force microscopy (TFM). Methods: An actomyosin distribution was found with a topology optimization method (TOM), varying the material density and angle of contraction of each element of the actomyosin domain. The routine was implemented with a linear material model for the bidimensional actomyosin elements and tridimensional substrate. The topology generated minimizes the nodal displacement squared differences between the generated topology and experimental displacement fields obtained by TFM. The structure resulting from TOM was compared to the actin structures observed experimentally with a GFP-attached actin marker. Results: The optimized topology reproduced the main features of the experimental actin and its squared displacement differences were 11.24 µm2, 27.5% of the sum of experimental squared nodal displacements (40.87 µm2). Conclusion: This approach extends the literature with a model for the actomyosin structure capable of distributing anisotropic material freely, allowing heterogeneous contraction over the cell extension. © 2015, Sociedade Brasileira de Engenharia Biomedica. All rights reserved. Actin; Cell mechanics; Finite element method; Topology optimization method; Traction force microscopy Cells; Complex networks; Cytology; Inverse problems; Proteins; Topology; Traction (friction); Finite element method; Actin; Anisotropic material; Cell mechanics; Cellular contraction; Pathological changes; Phenomenological approach; Topology Optimization Method; Traction force microscopies; Actin microfilaments; Finite element method; Proteins;[No abstract available]  Erratum; error;A cell mechanical stimulation equipment, based on cell substrate deformation, and a more sensitive method for measuring adhesion of cells were developed. A probe, precisely positioned close to the cell, was capable of a vertical localized mechanical stimulation with a temporal frequency of 207 Hz, and strain magnitude of 50%. This setup was characterized and used to probe the response of Human Umbilical Endothelial Vein Cells (HUVECs) in terms of calcium signaling. The intracellular calcium ion concentration was measured by the genetically encoded Cameleon biosensor, with the Transient Receptor Potential cation channel, subfamily M, member 7 (TRPM7) expression inhibited. As TRPM7 expression also regulates adhesion, a relatively simple method for measuring adhesion of cells was also developed, tested and used to study the effect of adhesion alone. Three adhesion conditions of HUVECs on polyacrylamide gel dishes were compared. In the first condition, the substrate is fully treated with Sulfo-SANPAH crosslinking and fibronectin. The other two conditions had increasingly reduced adhesion: partially treated (only coated with fibronectin, with no use of Sulfo-SANPAH, at 5% of the normal amount) and non-treated polyacrylamide gels. The cells showed adhesion and calcium response to the mechanical stimulation correlated to the degree of gel treatment: highest for fully treated gels and lowest for non-treated ones. TRPM7 inhibition by siRNA on HUVECs caused an increase in adhesion relative to control (no siRNA treatment) and non-targeting siRNA, but a decrease to 80% of calcium response relative to non-targeting siRNA which confirms the important role of TRPM7 in mechanotransduction despite the increase in adhesion. © 2015 Nishitani et al.  calcium ion; fibronectin; polyacrylamide gel; small interfering RNA; transient receptor potential channel M7; calcium; protein serine threonine kinase; transient receptor potential channel M; TRPM7 protein, human; Article; biosensor; calcium cell level; calcium signaling; cell adhesion; cell adhesion assay; controlled study; human; human cell; mechanical stimulation; mechanotransduction; protein expression; umbilical vein endothelial cell; calcium signaling; cell adhesion; cell culture; genetic procedures; genetics; mechanics; metabolism; physiology; RNA interference; Biosensing Techniques; Calcium; Calcium Signaling; Cell Adhesion; Cells, Cultured; Human Umbilical Vein Endothelial Cells; Humans; Mechanical Phenomena; Mechanotransduction, Cellular; Protein-Serine-Threonine Kinases; RNA Interference; RNA, Small Interfering; TRPM Cation Channels;This study assessed the effects of the diesel exhaust particles on ERK and JNK MAPKs activation, cell rheology (viscoelasticity), and cytotoxicity in bronchial epithelial airway cells (BEAS-2B). Crude DEP and DEP after extraction with hexane (DEP/HEX) were utilized. The partial reduction of some DEP/HEX organics increased the biodisponibility of many metallic elements. JNK and ERK were activated simultaneously by crude DEP with no alterations in viscoelasticity of the cells. Mitochondrial activity, however, revealed a decrease through the MTT assay. DEP/HEX treatment increased viscoelasticity and cytotoxicity (membrane damage), and also activated JNK. Our data suggest that the greater bioavailability of metals could be involved in JNK activation and, consequently, in the reduction of fiber coherence and increase in the viscoelasticity and cytotoxicity of BEAS cells. The adverse findings detected after exposure to crude DEP and to DEP/HEX reflect the toxic potential of diesel compounds. Considering the fact that the cells of the respiratory epithelium are the first line of defense between the body and the environment, our data contribute to a better understanding of the pathways leading to respiratory cell injury and provide evidence for the onset of or worsening of respiratory diseases caused by inorganic compounds present in DEP. © 2015 Elsevier GmbH. Cell viscoelasticity; Cytotoxicity; Diesel exhaust particles; MAPKs; Metals inorganic compound; Janus kinase; lactate dehydrogenase; mitogen activated protein kinase; exhaust gas; inorganic compound; mitogen activated protein kinase; particulate matter; Article; cell disruption; cell size; cell viability; colorimetry; controlled study; cytoskeleton; cytotoxicity; enzyme activation; enzyme phosphorylation; exhaust gas; human; human cell; particle size; protein expression; viscoelasticity; bronchus; cell culture; cytoskeleton; drug effects; enzyme activation; epithelium cell; exhaust gas; metabolism; particulate matter; respiratory tract mucosa; toxicity; Bronchi; Cells, Cultured; Cytoskeleton; Enzyme Activation; Epithelial Cells; Humans; Inorganic Chemicals; Mitogen-Activated Protein Kinases; Particulate Matter; Respiratory Mucosa; Vehicle Emissions;BACKGROUND: Snoring is extremely common in the general population and may indicate OSA. However, snoring is not objectively measured during polysomnography, and no standard treatment is available for primary snoring or when snoring is associated with mild forms of OSA. This study determined the effects of oropharyngeal exercises on snoring in minimally symptomatic patients with a primary complaint of snoring and diagnosis of primary snoring or mild to moderate OSA. METHODS: Patients were randomized for 3 months of treatment with nasal dilator strips plus respiratory exercises (control) or daily oropharyngeal exercises (therapy). Patients were evaluated at study entry and end by sleep questionnaires (Epworth Sleepiness Scale, Pittsburgh Sleep Quality Index) and full polysomnography with objective measurements of snoring. RESULTS: We studied 39 patients (age, 46 ± 13 years; BMI, 28.2 ± 3.1 kg/m2; apnea-hypopnea index (AHI), 15.3 ± 9.3 events/h; Epworth Sleepiness Scale, 9.2 ± 4.9; Pittsburgh Sleep Quality Index, 6.4 ± 3.3). Control (n = 20) and therapy (n = 19) groups were similar at study entry. One patient from each group dropped out. Intention-to-treat analysis was used. No significant changes occurred in the control group. In contrast, patients randomized to therapy experienced a significant decrease in the snore index (snores > 36 dB/h), 99.5 (49.6-221.3) vs 48.2 (25.5-219.2); P = .017 and total snore index (total power of snore/h), 60.4 (21.8-220.6) vs 31.0 (10.1-146.5); P = .033. CONCLUSIONS: Oropharyngeal exercises are effective in reducing objectively measured snoring and are a possible treatment of a large population suffering from snoring. TRIAL REGISTRY: ClinicalTrials.gov; No.: NCT01636856; URL: www.clinicaltrials.gov. © 2015 American College of Chest Physicians.  apnea hypopnea index; Article; breathing exercise; clinical effectiveness; controlled study; disease association; Epworth sleepiness scale; exercise; human; intention to treat analysis; oropharyngeal exercise; outcome assessment; Pittsburgh Sleep Quality Index; priority journal; randomized controlled trial; snoring; therapy effect; treatment response; complication; female; kinesiotherapy; male; middle aged; pathophysiology; polysomnography; procedures; questionnaire; sleep disordered breathing; snoring; treatment outcome; Exercise Therapy; Female; Humans; Male; Middle Aged; Polysomnography; Sleep Apnea, Obstructive; Snoring; Surveys and Questionnaires; Treatment Outcome;Vascular smooth muscle cells (VSMCs) are thought to assume a quiescent and homogeneous mechanical behavior after arterial tree development phase. However, VSMCs are known to be molecularly heterogeneous in other aspects and their mechanics may play a role in pathological situations. Our aim was to evaluate VSMCs from different arterial beds in terms of mechanics and proteomics, as well as investigate factors that may influence this phenotype. VSMCs obtained from seven arteries were studied using optical magnetic twisting cytometry (both in static state and after stretching) and shotgun proteomics. VSMC mechanical data were correlated with anatomical parameters and ultrastructural images of their vessels of origin. Femoral, renal, abdominal aorta, carotid, mammary, and thoracic aorta exhibited descending order of stiffness (G, P < 0.001). VSMC mechanical data correlated with the vessel percentage of elastin and amount of surrounding extracellular matrix (ECM), which decreased with the distance from the heart. After 48 h of stretching simulating regional blood flow of elastic arteries, VSMCs exhibited a reduction in basal rigidity. VSMCs from the thoracic aorta expressed a significantly higher amount of proteins related to cytoskeleton structure and organization vs. VSMCs from the femoral artery. VSMCs are heterogeneous in terms of mechanical properties and expression/organization of cytoskeleton proteins along the arterial tree. The mechanical phenotype correlates with the composition of ECM and can be modulated by cyclic stretching imposed on VSMCs by blood flow circumferential stress. © 2014 the American Physiological Society. Aorta; Extracellular matrix; Smooth muscle cells; Vascular smooth muscle aorta; extracellular matrix; smooth muscle cells; vascular smooth muscle; Animals; Arteries; Cell Cycle; Collagen; Elastin; Female; Muscle, Smooth, Vascular; Myocytes, Smooth Muscle; Proteomics; Sus scrofa;Natural myocardial markers, or speckles, originated from constructive and destructive interference of ultrasound in the tissues may provide early diagnosis of myocardial changes and be used in the prediction of some cardiac events. Due to its relatively temporal stability, speckles can be tracked by dedicated software along the cardiac cycle, enabling the analysis of the systolic and diastolic function. They are identified by either conventional 2D grey scale and by 3D echo, conferring independence of the insonation angle, thus allowing assessment of cardiac mechanics in the three spatial planes: longitudinal, circumferential, and radial. The purposes of the present paper are: to discuss the role and the meaning of cardiac strain obtained by speckle tracking during the evaluation of cardiac physiology and to discuss clinical applications of this novel echocardiographic technology. Echocardiography/methods; Heart Diseases; Strain, torsion, speckle tracking aorta valve stenosis; cardiovascular function; cardiovascular magnetic resonance; clinical trial (topic); congestive cardiomyopathy; coronary artery disease; echocardiography; Fabry disease; heart cycle; heart infarction; heart left ventricle ejection fraction; heart left ventricle volume; heart muscle fiber membrane potential; heart muscle relaxation; heart right ventricle function; heart ventricle pressure; human; hypertension; hypertrophic cardiomyopathy; mechanical torsion; mitral valve regurgitation; nonhuman; pathophysiology; pericardial disease; restrictive cardiomyopathy; review; speckle tracking echocardiography; systolic time interval; three dimensional echocardiography; tissue Doppler imaging; Echocardiography; Heart; Heart Diseases; Humans; Medical Illustration; Predictive Value of Tests; Reference Values; Reproducibility of Results; Ventricular Function;Pulmonary surfactant is a very important product in the medical treatment of the syndrome of insufficiency respiratory in neonates. The synthesis of this surfactant in labs need to optimize the rate of spreading in the alveolar interstitial liquid obtaining a monolayer of the phospholipids membrane base capable to maintains several of the dynamical properties of the respiratory system during breathing. The recover of theses mechanical properties has to be archived using the minimal quantities of product and with the optimal proteins composition (SP-B in special). In this paper we show our results of obtaining and process speckle pattern images of the spreading of phospholipids membrane composed the matrix of this product (DPPC) when physiologic interstitial liquid are presented. © 2013 SPIE. DPPC structure; Insufficiency respiratory syndrome; Interfacial film; Pulmonary surfactant; Speckle pattern; Surfactant rate of spreading Dynamical properties; Insufficiency respiratory syndrome; Interfacial film; Interstitial liquid; Medical treatment; Phospholipids membranes; Pulmonary surfactants; Speckle patterns; Liquids; Mechanical properties; Optics; Optimization; Phospholipids; Respiratory system; Surface active agents; Speckle;Due to abnormal mechanical instabilities, liquid bridges may form in the small airways blocking airflow. Liquid bridge ruptures during inhalation are the major cause of the crackling adventitious lung sound, which can be heard using a simple stethoscope. Recently, Vyshedskiy and colleagues (2009) [1] described and characterized a crackle sound originated during expiration. However, the mechanism and origin of the expiratory crackle are still controversial. Thus, in this paper, we propose a mechanism for expiratory crackles. We hypothesize that the expiratory crackle sound is a result of the energy released in the form of acoustic waves during the formation of the liquid bridge. The magnitude of the energy released is proportional to the difference in free energy prior and after the bridge formation. We use a lattice gas model to describe the liquid bridge formation between two parallel planes. Specifically, we determine the surface free energy and the conditions of the liquid bridge formation between two parallel planes separated by a distance 2h by a liquid droplet of volume Ω and contact angle Θ, using both Monte Carlo simulation of a lattice gas model and variational calculus based on minimization of the surface area with the volume and the contact angle constrained. We numerically and analytically determine the phase diagram of the system as a function of the dimensionless parameter hΩ-1/3 and Θ. We can distinguish two different phases: one droplet and one liquid bridge. We observe a hysteresis curve for the energy changes between these two states, and a finite size effect in the bridge formation. We compute the release of free energy during the formation of the liquid bridge and discuss the results in terms of system size. We also calculate the force exerted from liquid bridge on the planes by studying the dependence of the free energy on the separation between the planes 2h. The simulation results are in agreement with the analytical solution. © 2013 Elsevier B.V. All rights reserved. Crackle; Instabilities; Pots; Sound Crackle; Dimensionless parameters; Finite size effect; Hysteresis curve; Lattice gas model; Mechanical instabilities; Pots; Surface free energy; Acoustic waves; Acoustics; Contact angle; Cracks; Crystal lattices; Free energy; Gas dynamics; Monte Carlo methods; Phase diagrams; Stability; Variational techniques; Liquids;Low level laser therapy is used as a treatment of several conditions, including inflammatory processes and wound healing. Possible changes in mechanical properties of cells, caused by illumination, are investigated with optical magnetic twisting cytometry (OMTC), which is a technique used to evaluate mechanical properties in cell culture. Ferromagnetic micro beads are bound to cell cytoskeleton, the beads are magnetized vertically and a horizontal twisting magnetic field is applied causing a torque that moves the beads and deforms the cell, the beads rotate and displace. Based on the lateral displacement of the beads, elastic shear and loss moduli are obtained. Samples of human bronchial epithelial cell culture were divided in two groups: one was illuminated with a 660 nm red laser, 30 mW power, 0.75 W/cm2 irradiance, during different time intervals, and the other one, the control group, was not illuminated. The values of the mechanical constants of the cells of the control group showed a tendency of increasing with the time out of the incubator. On the other hand, the illuminated group showed constancy on the behavior of both moduli, keeping the normal conditions of the cell culture. Those results indicate that illumination can induce cells to homeostasis, and OMTC is sensitive to observe departures from the steady conditions. Hence, OMTC is an important technique which can be used to aggregate knowledge on the light effect in cell cytoskeleton and even on the low level laser therapy mechanisms in inflammatory processes and/or wound healing. © 2013 SPIE. Cell mechanical properties; Cytoskeleton; Human bronchial epithelial cell culture; LLLT; Low level laser therapy; Low level light therapy; OMTC; Optical magnetic twisting cytometry Cytoskeletons; Human bronchial epithelial cells; LLLT; Low level laser therapy; Low-level-light; OMTC; Optical magnetic twisting cytometry; Cell culture; Cells; Laser surgery; Mechanical properties; Biomechanics;The spectral analysis of crackles sounds has been carried out based on the assumption that they are stationary signals, and the majority of the work on the crackles was accomplished before the publication of the Computerized Respiratory Sound Analysis (CORSA) guidelines. This works characterizes crackles acquired from patients with fibrosis, heart failure, and pneumonia, breathing at a constant rate, with a system developed according to the CORSA guidelines. Their maximum frequency was obtained by applying discrete pseudo Wigner-Ville distribution, suitable for non-stationary signals, and an objective method to estimate the maximum frequency, the modified geometric method. The effects of the breathing rate and the tidal volume on the spectra of the crackles were also investigated. The role of the high-pass filter cutoff frequency of the acquisition system on the characteristics of the acquired crackles was also assessed in this present study. Higher high-pass filter cutoff frequency allows for higher amplification which modifies the maximum frequency and the 2CD index. It is shown that the crackles acquired according to the CORSA guidelines have higher frequencies and shorter 2CD indexes than those previously reported, highlighting the need for the standardization and detailed report of the acquisition setup when quantifying lung sounds. The results pointed out that the maximum frequency and the 2CD indexes may allow crackles generated by fibrosis to be distinguished from the ones generated by the heart failure and pneumonia. It is not possible, however, by means of these two indexes, to differentiate between pneumonia and heart failure crackles. © 2012 IPEM. 2CD; CORSA; Lung sounds; Maximum frequency; Wigner Ville Distribution 2CD; Acquisition systems; CORSA; Higher frequencies; Lung sounds; Maximum frequency; Nonstationary signals; Respiratory sounds; Biological organs; Cardiology; Cutoff frequency; High pass filters; Spectrum analysis; Wigner-Ville distribution; Cracks; abnormal respiratory sound; adult; aged; analog digital converter; article; breathing rate; clinical article; computer program; expiratory flow; filter; frequency analysis; heart failure; human; inspiratory capacity; lung fibrosis; mathematical analysis; microphone; pneumonia; pressure transducer; priority journal; quantitative analysis; rale; signal processing; sound transmission; standardization; tidal volume; waveform; Aged; Fibrosis; Heart Failure; Humans; Lung; Middle Aged; Pneumonia; Respiration; Respiratory Sounds; Signal Processing, Computer-Assisted;Acute lung injury, whose most severe expression is Respiratory Distress Syndrome, was described 40 years ago. Despite all research in this field, mortality rate still remains around 35%. In this study we used a model of acute lung injury with paraquat, an herbicide that causes alveolar instability through destruction of surfactant, to obtain pressure-volume curves of lungs in open-chest rats. The objectives were to study acute lung injury through measurements of lung elastance during inspiratory and expiratory maneuvers. We observed a rise in elastance during the initial part of the inspiratory maneuver (0-5mL approx.) in animals exposed to paraquat possibly due to increased alveolar collapse. We suppose that alveolar recruitment is responsible for the absence of significant elastance changes in other phases of the inspiratory maneuver. © 2013 Springer. Acute lung injury; elastance; experimental model; paraquat; pressure-volume curve Acute lung injury; Elastance; Experimental models; paraquat; Pressure-volume curves; Biological organs; Biomedical engineering; Herbicides;Snoring is extremely common in the general population and when irregular may indicate the presence of obstructive sleep apnea. We analyze the overnight sequence of wave packets - the snore sound - recorded during full polysomnography in patients referred to the Sleep Laboratory due to suspected obstructive sleep apnea. We hypothesize that irregular snore, with duration in the range between 10 and 100 s, correlates with respiratory obstructive events. We find that the number of irregular snores - easily accessible, and quantified by what we call the snore time interval index (STII) - is in good agreement with the well-known apnea-hypopnea index, which expresses the severity of obstructive sleep apnea and is extracted only from polysomnography. In addition, the Hurst analysis of the snore sound itself, which calculates the fluctuations in the signal as a function of time interval, is used to build a classifier that is able to distinguish between patients with no or mild apnea and patients with moderate or severe apnea. © 2012 Elsevier B.V. All rights reserved. Hurst; OSA; Snore; Time interval Apnea-hypopnea indices; Function of time; General population; Hurst; Hurst analysis; Obstructive sleep apnea; OSA; Polysomnography; Sleep laboratories; Snore; Time interval; Physics;One drawback of in vitro cell culturing is the dedifferentiation process that cells experience. Smooth muscle cells (SMC) also change molecularly and morphologically with long term culture. The main objective of this study was to evaluate if culture passages interfere in vascular SMC mechanical behavior. SMC were obtained from five different porcine arterial beds. Optical magnetic twisting cytometry (OMTC) was used to characterize mechanically vascular SMC from different cultures in distinct passages and confocal microscopy/western blotting, to evaluate cytoskeleton and extracellular matrix proteins. We found that vascular SMC rigidity or viscoelastic complex modulus (G) decreases with progression of passages. A statistically significant negative correlation between G and passage was found in four of our five cultures studied. Phalloidin-stained SMC from higher passages exhibited lower mean signal intensity per cell (confocal microscopy) and quantitative western blotting analysis showed a decrease in collagen I content throughout passages. We concluded that vascular SMC progressively lose their stiffness with serial culture passaging. Thus, limiting the number of passages is essential for any experiment measuring viscoelastic properties of SMC in culture. © 2012 - IOS Press and the authors. All rights reserved. Cell culture; dedifferentiation; optical magnetic twisting cytometry; smooth muscle cells collagen type 1; phalloidin; scleroprotein; animal cell; article; confocal microscopy; controlled study; cytoskeleton; in vitro study; nonhuman; rigidity; smooth muscle fiber; swine; vascular smooth muscle; viscoelasticity; Western blotting; Young modulus; Animals; Cells, Cultured; Collagen Type I; Elastic Modulus; Magnetics; Microscopy, Confocal; Muscle, Smooth, Vascular; Myocytes, Smooth Muscle; Swine;Snoring is highly prevalent, increases with age and is greater in men. The prevalence rates of snoring in the general population are variable, ranging from 20 to 60%. However, all population-based studies used the subjective analysis of snoring evaluated by questionnaires. Snoring is the major reported signal that may indicate obstructive sleep apnea (OSA). The polysomnography study, which is the golden standard test for the diagnosis of OSA, frequently uses the vibration recording by a sensor located on the patient's neck. However, this signal is not calibrated and has no correlation with sound intensity. To date, there is no consensus on the methodology of the acquisition and analysis of the snore signal. This review article evaluates methodological studies that report on snoring recording equipment and correlate the snoring signal with OSA. Using the keywords ""Snoring"" and ""Obstructive Sleep Apnea"" in PubMed, we found 602 articles, however there were only 11 studies (1.8%) that reported on technical aspects regarding snoring recording and analysis. The selected articles showed a wide variability in the method of recording, as well as analysis of the snoring signal. The lack of technical studies on snoring reflects a major gap in Sleep Medicine. Recent data obtained by objective measurements showed that snoring intensity correlates with OSA severity. In addition, snoring recordings represent a promising new noninvasive tool for the diagnosis of OSA. We conclude that snoring is a poorly monitored signal and that more studies are warranted in this fundamental area of Sleep Medicine. Body mass index; Obstructive/diagnosis; Palate; Polysomnography; Respiratory sounds; Sleep apnea; Snoring; Soft/physiopathology ;In this paper a method for measuring the inner and outer diameter of isolated airways in vitro is introduced. This method relies on images to measure outer diameter and on pressure-flow systemic response to measure inner diameter. The hardware/software environment used is introduced and initial experiments are performed as a proof of concept. The authors believe this method can be used to perform experiments in isolated airways that better mimic in-vivo conditions. © 2011 Springer-Verlag Berlin Heidelberg. airway responsiveness; airway wall mechanics; image processing airway responsiveness; Airway walls; Hardware/software; In-vitro; In-vivo; Inner diameters; Noninvasive methods; Outer diameters; Proof of concept; Systemic response; Experiments; Image processing; Noninvasive medical procedures;Auscultation of breathing sounds is a common practice since the antiquity. In 1819, Laënnec invented the stethoscope and published the first work on pulmonary disorders and their associated sounds. Since then, the auscultation was incorporated into medicine. The first electronic device to record and analyze physiological sounds was built in 1955, being followed by many other developments. In 2000, a task force of the European Respiratory Society established guidelines for computerized respiratory sound analysis (CORSA). Our work describes a low cost microcomputerized system, based on the CORSA guidelines, developed to acquire and record breathing sounds as well as respiratory flow waveforms. It consists of a four channel micro-controlled device that can simultaneously record sounds from three different sources and flow waveform. These signals are transmitted to a microcomputer running dedicated software that shows the waveforms on the screen and stores them into the hard disk. The developed device was tested in patients with heart failure, idiopathic pulmonary fibrosis, pneumonia and asthma. Examples of the registered signals and results of a qualitative assessment of the developed system are presented. CORSA; Flow waveform; Respiratory diseases; Respiratory sounds Breathing sounds; CORSA; Electronic device; Flow waveform; Flow waveforms; Four-channel; Hard disks; Heart failure; Idiopathic pulmonary fibrosis; Low costs; Low-cost devices; Qualitative assessments; Respiratory disease; Respiratory flow; Respiratory sounds; Task force; Wave forms; Pulmonary diseases; Electron devices;The past decade has seen significant increases in combustion-generated ambient particles, which contain a nanosized fraction (less than 100 nm), and even greater increases have occurred in engineered nanoparticles (NPs) propelled by the booming nanotechnology industry. Although inhalation of these particulates has become a public health concern, human health effects and mechanisms of action for NPs are not well understood. Focusing on the human airway smooth muscle cell, here we show that the cellular mechanical function is altered by particulate exposure in a manner that is dependent upon particle material, size and dose. We used Alamar Blue assay to measure cell viability and optical magnetic twisting cytometry to measure cell stiffness and agonist-induced contractility. The eight particle species fell into four categories, based on their respective effect on cell viability and on mechanical function. Cell viability was impaired and cell contractility was decreased by (i) zinc oxide (40-100 nm and less than 44 mm) and copper(II) oxide (less than 50 nm); cell contractility was decreased by (ii) fluorescent polystyrene spheres (40 nm), increased by (iii) welding fumes and unchanged by (iv) diesel exhaust particles, titanium dioxide (25 nm) and copper(II) oxide (less than 5 μm), although in none of these cases was cell viability impaired. Treatment with hydrogen peroxide up to 500 μM did not alter viability or cell mechanics, suggesting that the particle effects are unlikely to be mediated by particle-generated reactive oxygen species. Our results highlight the susceptibility of cellular mechanical function to particulate exposures and suggest that direct exposure of the airway smooth muscle cells to particulates may initiate or aggravate respiratory diseases. © 2010 The Royal Society. Air pollution; Cell mechanics; Environmental health; Mechanobiology; Nanoparticles Air quality; Biomechanics; Cytology; Environmental engineering; Fumes; Health; Hydrogen peroxide; Mechanisms; Muscle; Nanoparticles; Oxygen; Pollution; Polystyrenes; Titanium; Titanium dioxide; Titanium oxides; Zinc; Zinc oxide; Airway smooth muscle cell; Ambient particles; Biomechanical effects; Cell mechanics; Cell stiffness; Cell viability; Copper (ii); Diesel exhaust particles; Environmental health; Human airway smooth muscle cells; Human health effects; Mechanical functions; Mechano-biology; Nano-sized; Nanotechnology industry; Optical magnetic twisting cytometry; Particle effect; Particle materials; Polystyrene spheres; Public health; Reactive oxygen species; Respiratory disease; Welding fumes; Cells; copper oxide; hydrogen peroxide; nanoparticle; polystyrene; reactive oxygen metabolite; titanium dioxide; zinc oxide; copper; cuprous oxide; nanoparticle; oxazine derivative; polystyrene derivative; resazurin; titanium; xanthene derivative; zinc oxide; air pollution; article; breathing muscle; cell function; cell viability; controlled study; environmental health; exhaust gas; human; human cell; particulate matter; smooth muscle fiber; welding fume; analysis of variance; biomechanics; cell line; cell survival; cytology; drug effect; exhaust gas; physiology; respiratory system; Analysis of Variance; Biomechanics; Cell Line; Cell Survival; Copper; Humans; Hydrogen Peroxide; Myocytes, Smooth Muscle; Nanoparticles; Oxazines; Polystyrenes; Respiratory System; Titanium; Vehicle Emissions; Xanthenes; Zinc Oxide;Cell mechanical properties on a whole cell basis have been widely studied, whereas local intracellular variations have been less well characterized and are poorly understood. To fill this gap, here we provide detailed intracellular maps of regional cytoskeleton (CSK) stiffness, loss tangent, and rate of structural rearrangements, as well as their relationships to the underlying regional F-actin density and the local cytoskeletal prestress. In the human airway smooth muscle cell, we used micropatterning to minimize geometric variation. We measured the local cell stiffness and loss tangent with optical magnetic twisting cytometry and the local rate of CSK remodeling with spontaneous displacements of a CSK-bound bead. We also measured traction distributions with traction microscopy and cell geometry with atomic force microscopy. On the basis of these experimental observations, we used finite element methods to map for the first time the regional distribution of intracellular prestress. Compared with the cell center or edges, cell corners were systematically stiffer and more fluidlike and supported higher traction forces, and at the same time had slower remodeling dynamics. Local remodeling dynamics had a close inverse relationship with local cell stiffness. The principal finding, however, is that systematic regional variations of CSK stiffness correlated only poorly with regional F-actin density but strongly and linearly with the regional prestress. Taken together, these findings in the intact cell comprise the most comprehensive characterization to date of regional variations of cytoskeletal mechanical properties and their determinants. Copyright © 2010 the American Physiological Society. Cell mechanics; Heterogeneity; Remodeling; Stiffness F actin; article; atomic force microscopy; cell stress; cell structure; controlled study; cytoskeleton stiffness; flow cytometry; geometry; human; human cell; priority journal; rigidity; Actins; Biomechanics; Cell Adhesion; Cells, Cultured; Cytoskeleton; Humans; Myocytes, Smooth Muscle; Trachea;Cell shape, signaling, and integrity depend on cytoskeletal organization. In this study we describe the cytoskeleton as a simple network of filamentary proteins (links) anchored by complex protein structures (nodes). The structure of this network is regulated by a distance-dependent probability of link formation as P = p / ds, where p regulates the network density and s controls how fast the probability for link formation decays with node distance (d). It was previously shown that the regulation of the link lengths is crucial for the mechanical behavior of the cells. Here we examined the ability of the two-dimensional network to percolate (i.e. to have end-to-end connectivity), and found that the percolation threshold depends strongly on s. The system undergoes a transition around s = 2. The percolation threshold of networks with s &lt; 2 decreases with increasing system size L, while the percolation threshold for networks with s &gt; 2 converges to a finite value. We speculate that s &lt; 2 may represent a condition in which cells can accommodate deformation while still preserving their mechanical integrity. Additionally, we measured the length distribution of F-actin filaments from publicly available images of a variety of cell types. In agreement with model predictions, cells originating from more deformable tissues show longer F-actin cytoskeletal filaments. © 2008 Elsevier B.V. All rights reserved. Cell connectivity; F-actin; Mechanics; Model; Simulation Deformation; Filaments (lamp); Mechanics; Percolation (computer storage); Percolation (solid state); Solvents; Cell connectivity; Cell shapes; Cell types; Cytoskeletal filaments; Cytoskeletal organizations; Cytoskeletal structures; Cytoskeleton; End-to-end connectivities; F-actin; F-actin filaments; Length distributions; Link lengths; Long-range connections; Mechanical behaviors; Mechanical integrities; Model; Model predictions; Network densities; Percolation thresholds; Protein structures; Simple networks; Simulation; System sizes; Two-dimensional networks; Percolation (fluids);We present Monte Carlo simulations for a molecular motor system found in virtually all eukaryotic cells, the acto-myosin motor system, composed of a group of organic macromolecules. Cell motors were mapped to an Ising-like model, where the interaction field is transmitted through a tropomyosin polymer chain. The presence of Ca2+ induces tropomyosin to block or unblock binding sites of the myosin motor leading to its activation or deactivation. We used the Metropolis algorithm to find the transient and the equilibrium states of the acto-myosin system composed of solvent, actin, tropomyosin, troponin, Ca2+, and myosin-S1 at a given temperature, including the spatial configuration of tropomyosin on the actin filament surface. Our model describes the short- and long-range cooperativity during actin-myosin binding which emerges from the bending stiffness of the tropomyosin complex. We found all transition rates between the states only using the interaction energy of the constituents. The agreement between our model and experimental data also supports the recent theory of flexible tropomyosin. © 2009 The American Physical Society.  Actin filaments; Acto-myosin interactions; Bending stiffness; Cooperativity; Equilibrium state; Eukaryotic cells; Experimental datum; Interaction energies; Interaction fields; Metropolis algorithms; Molecular motors; Monte carlo simulations; Myosin motors; Organic macromolecules; Polymer chains; Short-range interactions; Spatial configurations; Transition rates; Tropomyosin; Troponin; Binding sites; Monte Carlo methods; Motors; Partial discharges; Stiffness; Supramolecular chemistry; Binding energy;Rheological properties of adherent cells are essential for their physiological functions, and microrheological measurements on living cells have shown that their viscoelastic responses follow a weak power law over a wide range of time scales. This power law is also influenced by mechanical prestress borne by the cytoskeleton, suggesting that cytoskeletal prestress determines the cell's viscoelasticity, but the biophysical origins of this behavior are largely unknown. We have recently developed a stochastic two-dimensional model of an elastically joined chain that links the power-law rheology to the prestress. Here we use a similar approach to study the creep response of a prestressed three-dimensional elastically jointed chain as a viscoelastic model of semiflexible polymers that comprise the prestressed cytoskeletal lattice. Using a Monte Carlo based algorithm, we show that numerical simulations of the chain's creep behavior closely correspond to the behavior observed experimentally in living cells. The power-law creep behavior results from a finite-speed propagation of free energy from the chain's end points toward the center of the chain in response to an externally applied stretching force. The property that links the power law to the prestress is the chain's stiffening with increasing prestress, which originates from entropic and enthalpic contributions. These results indicate that the essential features of cellular rheology can be explained by the viscoelastic behaviors of individual semiflexible polymers of the cytoskeleton. © 2008 The American Physical Society.  ABS resins; Cells; Colloids; Creep; Cytology; Elasticity; Plasticity; Polymers; Pressure effects; Rheology; Stochastic models; Three dimensional; Viscoelasticity; Viscosity; Adherent cells; Creep behaviors; Creep responses; Cytoskeletal; Cytoskeleton; Dimensional models; End points; Essential features; Living cells; Monte carlo; Numerical simulations; Physiological functions; Power laws; Pre stresses; Prestressed; Rheological properties; Semiflexible chains; Semiflexible polymers; Stretching forces; Time scales; Viscoelastic behaviors; Viscoelastic models; Viscoelastic responses; Physiological models;We report on a model of a prestressed nonlinear semiflexible polymer chain that links thermally driven dynamics to the creep behavior of living cells. Numerical simulations show that the chain's creep follows a power law with an exponent that decreases with increasing prestress. This is related to the propagation of free energy through the chain in response to stretching, where the propagation speed is regulated by the prestress via the chain's nonlinear elasticity. These results indicate that the main aspects of cell rheology are consistent with the dynamics of single polymer chains under tension. © 2006 The American Physical Society.  Cells; Computer simulation; Elasticity; Free energy; Rheology; Surface tension; Cell rheology; Living cells; Semiflexible polymer chains; Thermally driven dynamics; Polymers; actin; polymer; article; biological model; cell function; chemical model; chemistry; conformation; cytoskeleton; flow kinetics; methodology; Monte Carlo method; physiology; thermodynamics; Actins; Cell Physiology; Cytoskeleton; Models, Biological; Models, Chemical; Molecular Conformation; Monte Carlo Method; Polymers; Rheology; Thermodynamics;In the course of certain lung diseases, the surface properties and the amount of fluids coating the airways changes and liquid bridges may form in the small airways blocking the flow of air, impairing gas exchange. During inhalation, these liquid bridges may rupture due to mechanical instability and emit a discrete sound event called pulmonary crackle, which can be heard using a simple stethoscope. We hypothesize that this sound is a result of the acoustical release of energy that had been stored in the surface of liquid bridges prior to its rupture. We develop a lattice gas model capable of describing these phenomena. As a step toward modeling this process, we address a simpler but related problem, that of a liquid bridge between two planar surfaces. This problem has been analytically solved and we use this solution as a validation of the lattice gas model of the liquid bridge rupture. Specifically, we determine the surface free energy and critical stability conditions in a system containing a liquid bridge of volume Ω formed between two parallel planes, separated by a distance 2h, with a contact angle Θ using both Monte Carlo simulation of a lattice gas model and variational calculus based on minimization of the surface area with the volume and the contact angle constraints. In order to simulate systems with different contact angles, we vary the parameters between the constitutive elements of the lattice gas. We numerically and analytically determine the phase diagram of the system as a function of the dimensionless parameters h Ω-1/3 and Θ. The regions of this phase diagram correspond to the mechanical stability and thermodynamical stability of the liquid bridge. We also determine the conditions for the symmetrical versus asymmetrical rupture of the bridge. We numerically and analytically compute the release of free energy during rupture. The simulation results are in agreement with the analytical solution. Furthermore, we discuss the results in connection to the rupture of similar bridges that exist in diseased lungs. © 2006 The American Physical Society.  Biomedical equipment; Computer simulation; Mathematical models; Monte Carlo methods; Problem solving; Pulmonary diseases; Lattice gas models; Liquid bridges; Mechanical stability; Biomedical engineering;[No abstract available]  algorithm; computer aided design; computer analysis; image processing; letter; lung alveolus; lung emphysema; lung gas exchange; lung parenchyma; priority journal; stereology; tissue section;The mean linear intercept (Lm) can be used to estimate the surface area for gas exchange in the lung. However, in recent years, it is most commonly used as an index for characterizing the enlargement of airspaces in emphysema and the associated severity of structural destruction in the lung. Specifically, an increase in Lm is thought to result from an increase in airspace sizes. In this paper, we examined how accurately Lm measures the linear dimensions of airspaces from histological sections and a variety of computer-generated test images. To this end, we developed an automated method for measuring linear intercepts from digitized images of tissue sections and calculate Lm as their mean. We examined how the shape of airspaces and the variability of their sizes influence Lm as well as the distribution of linear intercepts. We found that, for a relatively homogeneous enlargement of airspaces, Lm was a reliable index for detecting emphysema. However, in the presence of spatial heterogeneities with a large variability of airspace sizes, Lm did not significantly increase and sometimes even decreased compared with its value in normal tissue. We also developed an automated method for measuring the area and computed an equivalent diameter of each individual airspace that is independent of shape. Finally, we introduced new indexes based on the moments of diameter that we found to be more reliable than Lm to characterize airspace enlargement in the presence of heterogeneities. Copyright © 2006 the American Physiological Society. Gas exchange animal model; animal tissue; article; computer analysis; controlled study; digital imaging; emphysema; histology; linear system; mouse; nonhuman; priority journal; quantitative analysis; respiratory airflow; algorithm; anatomy; animal; artificial intelligence; automated pattern recognition; computer assisted diagnosis; evaluation; hospitalization; lung alveolus; lung volume; methodology; microbiology; pathology; radiography; Algorithms; Anatomy, Cross-Sectional; Animals; Artificial Intelligence; Emphysema; Image Interpretation, Computer-Assisted; Lung Volume Measurements; Mice; Pattern Recognition, Automated; Pulmonary Alveoli; Severity of Illness Index;In a variety of physico-chemical reactions, the actual process takes place in a reactive zone, called the ""active surface"". We define the active surface of the lung as the set of airway segments that are closed but connected to the trachea through an open pathway, which is the interface between closed and open regions in a collapsed lung. To study the active surface and the time interval between consecutive openings, we measured the sound pressure of crackles, associated with the opening of collapsed airway segments in isolated dog lungs, inflating from the collapsed state in 120 s. We analyzed the sequence of crackle amplitudes, inter-crackle intervals, and low frequency energy from acoustic data. The series of spike amplitudes spans two orders of magnitude and the inter-crackle intervals spans over five orders of magnitude. The distribution of spike amplitudes follows a power law for nearly two decades, while the distribution of time intervals between consecutive crackles shows two regimes of power law behavior, where the first region represents crackles coming from avalanches of openings whereas the second region is due to the time intervals between separate avalanches. Using the time interval between measured crackles, we estimated the time evolution of the active surface during lung inflation. In addition, we show that recruitment and instabilities along the pressure-volume curve are associated with airway opening and recruitment. We find a good agreement between the theory of the dynamics of lung inflation and the experimental data which combined with numerical results may prove useful in the clinical diagnosis of lung diseases. © 2005 Elsevier B.V. All rights reserved. Active surface; Avalanche; Cayley; Invasion percolation; Recruitment; Tree Curve fitting; Data reduction; Pressure effects; Reaction kinetics; Surfaces; Airway segments; Lung inflation; Physico-chemical reactions; Spike amplitudes; Pulmonary diseases;We study the distribution Πn(D) of airway diameters D as a function of generation N in asymmetric airway trees of mammalian lungs. We find that the airway bifurcations are self-similar in four species studied. Specifically, the ratios of diameters of the major and minor daughters to their parent are constants independent of N until a cutoff diameter is reached. We derive closed form expressions for ΠN(D) and examine the flow resistance of the tree based on an asymmetric flow division model. Our findings suggest that the observed diameter heterogeneity is consistent with an underlying regular branching asymmetry. © 2005 The American Physical Society.  Airway bifurcations; Airway diameter; Mammalian lungs; Air; Bifurcation (mathematics); Probability distributions; Trees (mathematics); Respiratory system; animal; audiovisual equipment; biological model; biophysics; breathing mechanics; chemical model; devices; human; lung; lung circulation; pathology; procedures; respiratory function; statistical model; Animals; Biophysics; Humans; Lung; Models, Anatomic; Models, Biological; Models, Chemical; Models, Statistical; Pulmonary Circulation; Respiratory Mechanics; Respiratory Physiological Phenomena;Collagen and elastin are thought to dominate the elasticity of the connective tissue including lung parenchyma. The glycosaminoglycans on the proteoglycans may also play a role because osmolarity of interstitial fluid can alter the repulsive forces on the negatively charged glycosaminoglycans, allowing them to collapse or inflate, which can affect the stretching and folding pattern of the fibers. Hence, we hypothesized that the elasticity of lung tissue arises primarily from 1) the topology of the collagen-elastin network and 2) the mechanical interaction between proteoglycans and fibers. We measured the quasi-static, uniaxial stress-strain curves of lung tissue sheets in hypotonic, normal, and hypertonic solutions. We found that the stress-strain curve was sensitive to osmolarity, but this sensitivity decreased after proteoglycan digestion. Images of immunofluorescently labeled collagen networks showed that the fibers follow the alveolar walls that form a hexagonal-like structure. Despite the large heterogeneity, the aspect ratio of the hexagons at 30% uniaxial strain increased linearly with osmolarity. We developed a two-dimensional hexagonal network model of the alveolar structure incorporating the mechanical properties of the collagen-elastin fibers and their interaction with proteoglycans. The model accounted for the stress-strain curves observed under all experimental conditions. The model also predicted how aspect ratio changed with osmolarity and strain, which allowed us to estimate the Young's modulus of a single alveolar wall and a colla"
Maria Cecília Salvadori,Universidade de São Paulo - Instituto de Física,"Zirconium based metal pretreatments have become widely used in recent years as a substitute for phosphate deposition on steel alloys and for chromate on aluminum alloys in industrial applications. The choice of a zirconium based intermediate layer follows from its ecologic sustainability − decreased water and energy consumption, vehicle weight reduction, and low byproduct generation during processing. Here we describe our investigations of a characterization method of converted metal oxide thin films deposited by a plasma method. The thin film composition was characterized by Rutherford Backscattering Spectroscopy (RBS) and Energy Dispersive Spectroscopy (EDS) before and after conversion by a zirconium-based pretreatment, revealing the formation of zirconia after treatment. The corrosion mechanism of the deposited metal oxide films was investigated using electrochemical analysis, confirming the susceptibility of the film to corrosion and the applicability of corrosion investigations. The results pointed to a better performance of the RBS in comparison to EDS. © 2016 Trans Tech Publications, Switzerland. Corrosion; Pretreatment; RBS; Zirconium Alloy steel; Aluminum alloys; Aluminum coatings; Characterization; Corrosion; Energy dispersive spectroscopy; Energy utilization; Metallic compounds; Metals; Oxide films; Rubidium; Rutherford backscattering spectroscopy; Zirconia; Zirconium; Characterization methods; Ecologic sustainability; Electrochemical analysis; Energy dispersive spectroscopies (EDS); Metal oxide thin films; Pre-Treatment; Thin film composition; Vehicle weight reductions; Thin films;There is special interest in the incorporation of metallic nanoparticles in a surrounding dielectric matrix for obtaining composites with desirable characteristics such as for surface plasmon resonance, which can be used in photonics and sensing, and controlled surface electrical conductivity. We have investigated nanocomposites produced by metal ion implantation into insulating substrates, where the implanted metal self-assembles into nanoparticles. The nanoparticles nucleate near the maximum of the implantation depth profile (projected range), which can be estimated by computer simulation using the TRIDYN code. TRIDYN is a Monte Carlo simulation program based on the TRIM (Transport and Range of Ions in Matter) code that takes into account compositional changes in the substrate due to two factors: previously implanted dopant atoms, and sputtering of the substrate surface. Our study show that the nanoparticles form a bidimentional array buried a few nanometers below the substrate surface. We have studied Au/PMMA (polymethylmethacrylate), Pt/PMMA, Ti/alumina and Au/alumina systems. Transmission electron microscopy of the implanted samples show that metallic nanoparticles form in the insulating matrix. These nanocomposites have been characterized by measuring the resistivity of the composite layer as a function of the implantation dose. The experimental results are compared with a model based on percolation theory, in which electron transport through the composite is explained by conduction through a random resistor network formed by the metallic nanoparticles. Excellent agreement is found between the experimental results and the predictions of the theory. We conclude in that the conductivity process is due only to percolation (when the conducting elements are in geometric contact) and that the contribution from tunneling conduction is negligible. © 2014 Elsevier B.V. Ion implantation; Nanocomposites; Nanoparticles; Surface modification Computer simulation; Insulation; Metal ions; Metal nanoparticles; Metallic matrix composites; Monte Carlo methods; Nanocomposites; Nanoparticles; Solvents; Substrates; Surface treatment; Transmission electron microscopy; Compositional changes; Conductivity process; Insulating substrates; Metallic nanoparticles; Monte- carlo simulations; Random resistor networks; Surface electrical conductivity; Transport and range of ions in matters; Ion implantation;In this study, we show and discuss the results of the interaction of living CHO (Chinese Hamster Ovary) cells, in terms of adhesion and growth on glass, SU-8 (epoxi photoresist), PDMS (polydimethylsiloxane), and DLC (hydrogen free diamond-like carbon) surfaces. Glass, SU-8, and DLC but not PDMS showed to be good surfaces for cell growth. DLC surfaces were treated by oxygen plasma (DLC-O) and sulfur hexafluoride plasma (DLC-F). After 24 h of cell culture, the number of cells on DLC-O was higher than on DLC-F surface. SU-8 with silver implanted, creating nanoparticles 12 nm below the surface, increased significantly the number of cells per unit area. © 2014 AIP Publishing LLC.  Cell adhesion; Cell culture; Glass; Ion implantation; Microchannels; Photoresists; Plasmas; Silver; Chinese hamster ovary; Diamond-like carbon; Oxygen plasmas; Per unit; Sulfur hexafluoride plasmas; Silicones;We describe an approach to ion implantation in which the plasma and its electronics are held at ground potential and the ion beam is injected into a space held at high negative potential, allowing considerable savings both economically and technologically. We used an ""inverted ion implanter"" of this kind to carry out implantation of gold into alumina, with Au ion energy 40 keV and dose (3-9) × 1016 cm-2. Resistivity was measured in situ as a function of dose and compared with predictions of a model based on percolation theory, in which electron transport in the composite is explained by conduction through a random resistor network formed by Au nanoparticles. Excellent agreement is found between the experimental results and the theory. © 2013 AIP Publishing LLC.  Alumina; Electron transport properties; Gold; Ion beams; Ion sources; Solvents; Au nanoparticle; Electron transport; Ground potential; Ion implanters; Model-based OPC; Negative potential; Percolation theory; Random resistor networks; Ion implantation;[No abstract available]  ;Composites of titanium nanoparticles in alumina were formed by ion implantation of titanium into alumina, and the surface electrical conductivity measured in situ as the implantation proceeded, thus generating curves of sheet conductivity as a function of dose. The implanted titanium self-conglomerates into nanoparticles, and the spatial dimensions of the buried nanocomposite layer can thus be estimated from the implantation depth profile. Rutherford backscattering spectrometry was performed to measure the implantation depth profile, and was in good agreement with the calculated profile. Transmission electron microscopy of the titanium-implanted alumina was used for direct visualization of the nanoparticles formed. The measured conductivity of the buried layer is explained by percolation theory. We determine that the saturation dose, φ0, the maximum implantation dose for which the nanocomposite material still remains a composite, is φ0-=-2.2-×-1016-cm-2, and the corresponding saturation conductivity is σ0-=-480-S/m. The percolation dose φc, below which the nanocomposite still has basically the conductivity of the alumina matrix, was found to be φc-=-0.84-×-1016-cm-2. The experimental results are discussed and compared with a percolation theory model. © 2014 AIP Publishing LLC.  Alumina; Nanocomposites; Nanoparticles; Percolation (solid state); Rutherford backscattering spectroscopy; Solvents; Titanium; Transmission electron microscopy; Direct visualization; Implantation depth; Nano-composite layers; Percolation theory; Rutherford back-scattering spectrometry; Surface electrical conductivity; Titanium ion implantation; Titanium nanoparticles; Ion implantation;New techniques for tissue engineering (TE) are rapidly emerging. The basic concept of autologous TE is to isolate cells from small biopsy specimens, and to expand these cells in culture for subsequent seeding onto biodegradable scaffolds. Nanocrystalline diamond films have attracted the attention of researchers from a variety of different areas in recent years, due to their unique and exceptional properties. In this approach, human dental stem cells (hDSCs) were characterized by flow cytometry and grown on diamond films with hydrogen (H)-terminated and oxygen (O)-terminated surfaces for 28 days, and then removed by lysis and washing with distilled water. Energy dispersive spectroscopy analysis was performed, showing that the regions with O-terminated surfaces contained much higher levels of deposited calcium, oxygen, and phosphorus. These results suggest that the extracellular matrix was considerably more developed in the O-terminated regions, as compared with the H-terminated regions. In addition, optical microscopy of hDSCs cultured on the diamond substrate with H-and O-terminated surfaces, before washing with distilled water, showed preferential directions of the cells arrangement, where orthogonal lines suggest that the cells appeared to be following the O-terminated regions or hydrophilic surface. These findings suggest that O-terminated diamond surfaces prepared on biodegradable scaffolds can be useful for mineralized dental tissue formation. © 2013 Mary Ann Liebert, Inc.  Cytology; Diamond films; Energy dispersive spectroscopy; Hydrophilicity; Oxygen; Stem cells; Washing; Basic concepts; Biodegradable scaffold; Diamond substrates; Diamond surfaces; Distilled water; Extracellular matrices; Hydrophilic surfaces; Nanocrystalline diamond films; Scaffolds (biology); calcium; diamond; hydrogen; mannitol; nanocrystal; oxygen; phosphorus; article; cell growth; cell population; cell structure; dental stem cell; extracellular matrix; flow cytometry; human; human cell; human tissue; hydrophilicity; molar tooth; priority journal; stem cell; surface property; tissue engineering; tooth; tooth development; Cells, Cultured; Humans; Hydrophobic and Hydrophilic Interactions; Nanodiamonds; Stem Cells; Tissue Engineering; Tissue Scaffolds; Tooth;Superhydrophobic surfaces formed of microcavities can be designed with specific desired advancing and receding contact angles using a new model described by us in prior work. Here, we discuss the limits of validity of the model, and explore the application of the model to surfaces fabricated with small cavities of radius 250 nm and with large cavities of radius 40 μm. The Wenzel model is discussed and used to calculate the advancing and receding contact angles for samples for which our model cannot be applied. We also consider the case of immersion of a sample containing microcavities in pressurized water. A consideration that then arises is that the air inside the cavities can be dissolved in the water, leading to complete water invasion into the cavities and compromising the superhydrophobic character of the surface. Here, we show that this effect does not destroy the surface hydrophobia when the surface is subsequently removed from the water. © 2013 AIP Publishing LLC.  Advancing and receding contact angles; Large cavities; Pressurized water; Small cavities; Super-hydrophobic surfaces; Superhydrophobic; Water invasion; Wenzel models; Contact angle; Hydrophobicity; Surface properties; Microcavities;We have carried out ion implantation of gold into alumina ceramic substrates and measured the surface resistivity as a function of implantation dose. The Au ion energy was 40 keV and the dose spanned the range 2.7-8.9 × 1016 cm-2. Imaging of the implanted material by transmission electron microscopy revealed that the implanted gold self-assembles into nanoparticles, thus forming a gold-alumina nano-composite. The surface resistivity measurements were compared with the predictions of a model based on percolation theory, in which electron transport through the composite is explained by conduction through a random resistor network formed by the Au nanoparticles. The electrical conductivity of a composite, near the critical conductor-insulator transition, is given by σ ≈ σ0(x- xc)t, where σ0 is the saturation conductivity for which the material still remains a composite, x is the normalized metal atom concentration of the conducting phase, xc is the critical concentration, or percolation threshold and t is the critical exponent. Excellent agreement was found between the experimental results and the predictions of the theory, and the results are consistent with prior related (but more limited) work. The percolation dose was 4.4 × 1016 cm-2, and the critical exponent obtained was t = 1.4 ± 0.1. We conclude that the conductivity process is due to percolation and that the contribution from tunneling conduction is negligible. © 2013 Elsevier B.V. All rights reserved. Conducting ceramic; Metal ion implantation; Percolation; Surface conductivity Alumina ceramic substrates; Conducting ceramic; Conductor-insulator transition; Critical concentration; Electrical conductivity; Percolation thresholds; Random resistor networks; Surface conductivity; Alumina; Ceramic materials; Electric conductivity; Forecasting; Ion implantation; Metal ions; Nanocomposites; Nanoparticles; Percolation (solid state); Solvents; Transmission electron microscopy; Gold;We describe experimental results about the spontaneous wrinkling of diamond-like carbon films over the thickness range 2 nm-58 nm, grown on polydimethylsiloxane (PDMS) substrates with a 5 nm gold film deposited as adhesion layer. Using Atomic Force Microscopy data with suitable processing, we explore both isotropic and anisotropic wrinkling, the latter done by creating trench structures on PDMS substrates. We show new non-predictable results based on the known literature. © 2013 AIP Publishing LLC.  Adhesion layer; Gold film; PDMS substrate; Polydimethylsiloxane (PDMS) substrates; Trench structures; Anisotropy; Atomic force microscopy; Carbon films; Data handling; Microchannels; Silicones;Whereas energetic ion beams are conventionally produced by extracting ions (say, positive ions) from a plasma that is held at high (positive) potential, with ion energy determined by the potential drop through which the ions fall in the beam formation electrode system, in the device described here the plasma and its electronics are held at ground potential and the ion beam is formed and injected energetically into a space maintained at high (negative) potential. We refer to this configuration as an ""inverted ion source."" This approach allows considerable savings both technologically and economically, rendering feasible some ion beam applications, in particular small-scale ion implantation, that might otherwise not be possible for many researchers and laboratories. We have developed a device of this kind utilizing a metal vapor vacuum arc plasma source, and explored its operation and beam characteristics over a range of parameter variation. The downstream beam current has been measured as a function of extraction voltage (5-35 kV), arc current (50-230 A), metal ion species (Ti, Nb, Au), and extractor grid spacing and beamlet aperture size (3, 4, and 5 mm). The downstream ion beam current as measured by a magnetically-suppressed Faraday cup was up to as high as 600 mA, and with parametric variation quite similar to that found for the more conventional metal vapor vacuum arc ion source. © 2013 American Institute of Physics.  Beam characteristics; Conventional metals; Energetic ion beams; Extraction voltage; Ion beam currents; Ion-beam applications; Metal vapor vacuum arcs; Parametric variation; Ion implantation; Ion sources; Metal ions; Vacuum applications; Vapors; Ion beams;We describe an approach to ion implantation in which the plasma and its electronics are held at ground potential and the ion beam is formed and injected energetically into a space held at high negative potential. The technique allows considerable savings both economically and technologically, rendering feasible ion implantation applications that might otherwise not be possible for many researchers and laboratories. Here, we describe the device and the results of tests demonstrating Nb implantation at 90 keV ion energy and dose about 2 × 1016 cm-2. © 2012 American Institute of Physics.  Ground potential; Implantation technique; Ion energies; Low costs; Negative potential; Physical properties; Physics; Ion implantation;We describe work in which gold nanoparticles were formed in diamond-like carbon (DLC), thereby generating a Au-DLC nanocomposite. A high-quality, hydrogen-free DLC thin film was formed by filtered vacuum arc plasma deposition, into which gold nanoparticles were introduced using two different methods. The first method was gold ion implantation into the DLC film at a number of decreasing ion energies, distributing the gold over a controllable depth range within the DLC. The second method was co-deposition of gold and carbon, using two separate vacuum arc plasma guns with suitably interleaved repetitive pulsing. Transmission electron microscope images show that the size of the gold nanoparticles obtained by ion implantation is 3-5 nm. For the Au-DLC composite obtained by co-deposition, there were two different nanoparticle sizes, most about 2 nm with some 6-7 nm. Raman spectroscopy indicates that the implanted sample contains a smaller fraction of sp 3 bonding for the DLC, demonstrating that some sp 3 bonds are destroyed by the gold implantation. © 2012 American Institute of Physics.  Codeposition; Depth range; Diamond-like carbon; DLC film; Gold nanoparticle formation; Gold Nanoparticles; High quality; Hydrogen-free DLC; Implanted samples; Ion energies; Nanoparticle sizes; Repetitive pulsing; Transmission electron microscope; Vacuum arc plasma; Carbon; Hydrogen; Ion implantation; Metal nanoparticles; Raman spectroscopy; Transmission electron microscopy; Vacuum applications; Vacuum technology; Gold;We have explored the effects of atmospheric environment on Kelvin force microscopy (KFM) measurements of potential difference between different regions of test polycrystalline diamond surfaces. The diamond films were deposited by microwave plasma-assisted chemical vapor deposition, which naturally produces hydrogen terminations on the surface of the films formed. Selected regions were patterned by electron-beam lithography and chemical terminations of oxygen or fluorine were created by exposure to an oxygen or fluorine plasma source. For KFM imaging, the samples were mounted in a hood with a constant flow of helium gas. Successive images were taken over a 5-h period showing the effect of the environment on KFM imaging. We conclude that the helium flow removes water molecules adsorbed on the surface of the samples, resulting in differences in surface potential between adjacent regions. The degree of water removal is different for surfaces with different terminations. The results highlight the importance of taking into account the atmospheric environment when carrying out KFM analysis. © 2012 Wiley Periodicals, Inc. Diamond film; Kelvin force microscopy; Surface characterization; Surface electronic properties; Surface microscopy ;Composites formed of a polymer-embedded layer of sub-10 nm gold nanoclusters were fabricated by very low energy (49 eV) gold ion implantation into polymethylmethacrylate. We used small angle x-ray scattering to investigate the structural properties of these metal-polymer composite layers that were fabricated at three different ion doses, both in their original form (as-implanted) and after annealing for 6 h well above the polymer glass transition temperature (150 °C). We show that annealing provides a simple means for modification of the structure of the composite by coarsening mechanisms, and thereby changes its properties. © 2012 American Institute of Physics.  Annealing effects; Coarsening mechanisms; Gold nanocluster; Ion dose; Low energies; Metal-polymer composites; Nano-structured; Polymer glass transition; Small angle X-ray scattering; Ion implantation; Polyesters; Polymers; X ray scattering;We have explored the suitability and characteristics of interface tailoring as a tool for enhancing the adhesion of hydrogen-free diamond-like carbon (DLC) thin films to silicon substrates. DLC films were deposited on silicon with and without application of an initial high energy carbon ion bombardment phase that formed a broad Si-C interface of gradually changing Si:C composition. The interface depth profile was calculated using the TRIDYN simulation program, revealing a gradient of carbon concentration including a region with the stoichiometry of silicon carbide. DLC films on silicon, with and without interface tailoring, were characterized using Raman spectroscopy, scanning electron microscopy, atomic force microscopy and scratch tests. The Raman spectroscopy results indicated sp3-type carbon bonding content of up to 80%. Formation of a broadened Si:C interface as formed here significantly enhances the adhesion of DLC films to the underlying silicon substrate. © 2012 Elsevier B.V. All rights reserved. Diamond-like carbon; Enhanced adhesion; Interface tailoring; Ion bombardment Adhesion enhancement; Carbon bonding; Carbon concentrations; Depth profile; Diamond-like carbon; Diamond-like carbon thin films; DLC film; High energy; Scratch test; Silicon substrates; Simulation program; Adhesion; Atomic force microscopy; Carbon films; Diamonds; Hydrogen; Ion bombardment; Raman scattering; Raman spectroscopy; Scanning electron microscopy; Silicon; Silicon carbide; Stoichiometry; Thin films; Interfaces (materials);Ion implantation of metal species into insulators provides a tool for the formation of thin, electrically conducting, surface layers with experimenter-controlled resistivity. High energy implantation of Pt and Ti into alumina accelerator components has been successfully employed to control high voltage surface breakdown in a number of cases. In the work described here we have carried out some basic investigations related to the origin of this phenomenon. By comparison of the results of alumina implanted with Ti at 75 keV with the results of prior investigations of polymers implanted with Pt at 49 eV and Au at 67 eV, we describe a physical model of the effect based on percolation theory and estimate the percolation parameters for the Ti-alumina composite. We estimate that the percolation dose threshold is about 4 × 10 16 cm -2 and the maximum dose for which the system remains an insulator-conductor composite is about 10 × 10 16 cm -2. The saturation electrical conductivity is estimated to be about 50 Sm. We conclude that the observed electrical conductivity properties of Ti-implanted alumina can be satisfactorily described by percolation theory. © 2012 American Institute of Physics.  Electrical conductivity; High energy implantation; High voltage; Metal species; Percolation theory; Physical model; Surface breakdown; Surface layers; Electric conductivity; Ion implantation; Percolation (solid state); Platinum; Solvents; Alumina;This manuscript reports on the fabrication of plasmonic substrates using cathodic arc plasma ion implantation, in addition to their performance as SERS substrates. The technique allows for the incorporation of a wide layer of metallic nanoparticles into a polymer matrix, such as PMMA. The ability to pattern different structures using the PMMA matrix is one of the main advantages of the fabrication method. This opens up new possibilities for obtaining tailored substrates with enhanced performance for SERS and other surface-enhanced spectroscopies, as well as for exploring the basic physics of patterned metal nanostructures. The architecture of the SERS-active substrate was varied using three adsorption strategies for incorporating a laser dye (rhodamine): alongside the nanoparticles into the polymer matrix, during the polymer cure and within nanoholes lithographed on the polymer. As a proof-of-concept, we obtained the SERS spectra of rhodamine for the three types of substrates. The hypothesis of incorporation of rhodamine molecules into the polymer matrix during the cathodic arc plasma ion implantation was supported by FDTD (Finite-Difference Time-Domain) simulations. In the case of arrays of nanoholes, rhodamine molecules could be adsorbed directly on the gold surface, then yielding a well-resolved SERS spectrum for a small amount of analyte owing to the short-range interactions and the large longitudinal field component inside the nanoholes. The results shown here demonstrate that the approach based on ion implantation can be adapted to produce reproducible tailored substrates for SERS and other surface-enhanced spectroscopies. © 2012 the Owner Societies.  gold; metal nanoparticle; poly(methyl methacrylate); rhodamine; article; chemistry; electrode; Raman spectrometry; Electrodes; Gold; Metal Nanoparticles; Polymethyl Methacrylate; Rhodamines; Spectrum Analysis, Raman;Platinumpolymethylmethacrylate (PtPMMA) nanocomposite material was formed by low energy ion implantation of Pt into PMMA, and the transition from insulating to conducting phase was explored. In situ resistivity measurements were performed as the implantation proceeded, and transmission electron microscopy was used for direct visualization of Pt nanoparticles. Numerical simulation was carried out using the TRIDYN computer code to calculate the expected depth profiles of the implanted platinum. The maximum dose for which the PtPMMA system remains an insulatorconductor composite was found to be 0 1.6 10 16 cm -2, the percolation dose was 0.5 10 16 cm -2, and the critical exponent was t 1.46, indicating that the conductivity is due only to percolation. The results are compared with previously reported results for a AuPMMA composite. © 2011 American Institute of Physics.  Computer codes; Conducting phase; Critical exponent; Depth profile; Direct visualization; Electrical conductivity; In-situ; Low energy ion implantation; Pt nanoparticles; Electric conductivity; Ion implantation; Nanocomposites; Solvents; Transmission electron microscopy; Visualization; Platinum;Objectives: The aim of this study was to describe roughness and gloss alterations of enamel after treatment with 38% hydrogen peroxide (HP) and after polishing with 2% neutral sodium fluoride (SF) or a dental tooth paste containing nanohydroxiapatite particles (nHA) using power spectral density (PSD) description, roughness parameters (Ra, RMS, and Z range) and gloss analysis. Methods: An atomic force microscope (AFM) and a spectrophotometer were used to analyze eighteen specimens of upper incisors. After initial analyses, all specimens were bleached with 38% HP for 135 min. The specimens were analyzed after bleaching. Nine specimens were polished with SF (Group Fluor) and the other nine specimens were polished with nHA (Group nHA), then all specimens were analyzed after polishing. Roughness and gloss were analyzed with ANOVA and Tukey's t-test. Results: No statistical difference was found for Ra and RMS among initial, after bleaching and after polishing in both groups. For Z range, Group nHA showed a significant decrease after polishing. Bleaching with 38% HP did not increase the PSD in the spatial frequency of the visible light spectrum range in both groups. After polishing, nHA group showed a decrease in PSD for all morphological wavelengths. Gloss did not show statistical difference after bleaching in both groups. Gloss showed significant increase after polishing with nHA. Significance: bleaching treatment with 38% HP didn't alter enamel surface roughness or gloss. PSD analyses were very suitable to identifying the morphological changes on the surfaces. © 2011 Wiley Periodicals, Inc. Dental bleaching; Gloss; Power spectral density; Surface analyses fluoride; hydrogen peroxide; hydroxyapatite; tooth bleaching agent; article; atomic force microscopy; drug effect; human; metabolism; physiology; spectrophotometry; surface property; tooth; Durapatite; Fluorides; Humans; Hydrogen Peroxide; Microscopy, Atomic Force; Spectrophotometry; Surface Properties; Tooth; Tooth Bleaching Agents;A vacuum arc ion source provides high current beams of metal ions that have been used both for accelerator injection and for ion implantation, and in both of these applications the degree of space charge neutralization of the beam is important. In accelerator injection application, the beam from the ion source may be accelerated further (post-acceleration), redirected by a bending magnet(s), or focused with magnetic or electrostatic lenses, and knowledge of the beam space charge is needed for optimal design of the optical elements. In ion implantation application, any build-up of positive charge in the insulating targets must be compensated by a simultaneous flux of cold electrons so as to provide overall charge neutrality of the target. We show that in line-of-sight ion implantation using a vacuum arc ion source, the high current ion beam carries along its own background sea of cold electrons, and this copious source of electrons provides a ""self-neutralizing"" feature to the beam. Here we describe experiments carried out in order to demonstrate this effect, and we provide an analysis showing that the beam is space-charge-neutralized to a very high degree © 2011 American Institute of Physics.  Beam space charge; Bending magnets; Charge neutrality; Cold electrons; High current beams; High-current ion beams; Insulating targets; Line-of-sight; Optimal design; Positive charges; Source of electrons; Space-charge neutralization; Vacuum arc ion source; Electron optics; Electrostatic lenses; Ion implantation; Ion sources; Lenses; Metal ions; Optics; Vacuum; Vacuum applications; Vacuum technology; Ion beams;Objective: The aim of this study was to assess by atomic force microscopy (AFM) the effect of Er,Cr:YSGG laser application on the surface microtopography of radicular dentin. Background: Lasers have been used for various purposes in dentistry, where they are clinically effective when used in an appropriate manner. The Er,Cr:YSGG laser can be used for caries prevention when settings are below the ablation threshold. Materials and Methods: Four specimens of bovine dentin were irradiated using an Er,Cr:YSGG laser (λ=2.78μm), at a repetition rate of 20Hz, with a 750-μm-diameter sapphire tip and energy density of 2.8J/cm2 (12.5mJ/pulse). After irradiation, surface topography was analyzed by AFM using a Si probe in tapping mode. Quantitative and qualitative information concerning the arithmetic average roughness (Ra) and power spectral density analyses were obtained from central, intermediate, and peripheral areas of laser pulses and compared with data from nonirradiated samples. Results: Dentin Ra for different areas were as follows: central, 261.26 (±21.65) nm; intermediate, 83.48 (±6.34) nm; peripheral, 45.8 (±13.47) nm; and nonirradiated, 35.18 (±2.9) nm. The central region of laser pulses presented higher ablation of intertubular dentin, with about 340-760nm height, while intermediate, peripheral, and nonirradiated regions presented no difference in height of peritubular and interperitubular dentin. Conclusion: According to these results, we can assume that even when used at a low-energy density parameter, Er,Cr:YSGG laser can significantly alter the microtopography of radicular dentin, which is an important characteristic to be considered when laser is used for clinical applications. © Copyright 2011, Mary Ann Liebert, Inc. 2011.  Ablation thresholds; AFM; Bovine dentin; Caries prevention; Clinical application; Energy density; Energy density distributions; Er ,Cr:YSGG laser; Low energies; Micro topography; Power spectral density analysis; Qualitative information; Repetition rate; Surface microtopography; Tapping modes; Ablation; Atomic force microscopy; Erbium; Laser pulses; Power spectral density; Surface topography; Surfaces; Chromium; analysis of variance; animal; article; atomic force microscopy; cattle; dental caries; dentin; in vitro study; laser; radiation exposure; scanning electron microscopy; surface property; tooth root; ultrastructure; Analysis of Variance; Animals; Cattle; Dental Caries; Dentin; Lasers; Microscopy, Atomic Force; Microscopy, Electron, Scanning; Surface Properties; Tooth Root;We describe the design and implementation of a high voltage pulse power supply (pulser) that supports the operation of a repetitively pulsed filtered vacuum arc plasma deposition facility in plasma immersion ion implantation and deposition (Mepiiid) mode. Negative pulses (micropulses) of up to 20 kV in magnitude and 20 A peak current are provided in gated pulse packets (macropulses) over a broad range of possible pulse width and duty cycle. Application of the system consisting of filtered vacuum arc and high voltage pulser is demonstrated by forming diamond-like carbon (DLC) thin films with and without substrate bias provided by the pulser. Significantly enhanced filmsubstrate adhesion is observed when the pulser is used to induce interface mixing between the DLC film and the underlying Si substrate. © 2010 American Institute of Physics.  Diamond-like carbon; DLC film; Duty cycles; Film-substrate adhesion; High voltage pulsers; High-voltage pulse power supply; Interface mixing; Macropulses; Metal plasma immersion ion implantation and deposition; Micropulses; Negative pulse; Peak currents; Plasma immersion ion implantation and deposition; Pulse width; Si substrates; Substrate bias; Vacuum arc plasma; Vacuum arcs; Carbon films; Diamond like carbon films; Electric potential; Ion implantation; Plasma deposition; Plasma enhanced chemical vapor deposition; Vacuum; Vacuum applications; Ion beam assisted deposition;We have investigated the structure of disordered gold-polymer thin films using small angle x-ray scattering and compared the results with the predictions of a theoretical model based on two approaches-a structure form factor approach and the generalized Porod law. The films are formed of polymer-embedded gold nanoclusters and were fabricated by very low energy gold ion implantation into polymethylmethacrylate (PMMA). The composite films span (with dose variation) the transition from electrically insulating to electrically conducting regimes, a range of interest fundamentally and technologically. We find excellent agreement with theory and show that the PMMA-Au films have monodispersive or polydispersive characteristics depending on the implanted ion dose. © 2010 American Institute of Physics.  Au film; Dose variations; Form factors; Gold ions; Gold nanocluster; Ion dose; Low energies; Polymer thin films; Porod law; Small angle X-ray scattering; Theoretical models; Approximation theory; Conductive films; Gold; Ion implantation; Plastic films; Polydispersity; Polymer films; Polymeric films; Polymers; Scattering; Thin films; X ray scattering; Composite films;We have modeled, fabricated, and characterized superhydrophobic surfaces with a morphology formed of periodic microstructures which are cavities. This surface morphology is the inverse of that generally reported in the literature when the surface is formed of pillars or protrusions, and has the advantage that when immersed in water the confined air inside the cavities tends to expel the invading water. This differs from the case of a surface morphology formed of pillars or protrusions, for which water can penetrate irreversibly among the microstructures, necessitating complete drying of the surface in order to again recover its superhydrophobic character. We have developed a theoretical model that allows calculation of the microcavity dimensions needed to obtain superhydrophobic surfaces composed of patterns of such microcavities, and that provides estimates of the advancing and receding contact angle as a function of microcavity parameters. The model predicts that the cavity aspect ratio (depth-to-diameter ratio) can be much less than unity, indicating that the microcavities do not need to be deep in order to obtain a surface with enhanced superhydrophobic character. Specific microcavity patterns have been fabricated in polydimethylsiloxane and characterized by scanning electron microscopy, atomic force microscopy, and contact angle measurements. The measured advancing and receding contact angles are in good agreement with the predictions of the model. © 2010 American Institute of Physics.  Cavity aspect ratio; Diameter ratio; Periodic microstructure; Receding contact; Super-hydrophobic surfaces; Superhydrophobic; Theoretical models; Angle measurement; Aspect ratio; Atomic force microscopy; Contact angle; Fabrication; Hydrophobicity; Microcavities; Microstructure; Morphology; Scanning electron microscopy; Silicones; Surface properties; Surface morphology;The authors present here a summary of their investigations of ultrathin films formed by gold nanoclusters embedded in polymethylmethacrylate polymer. The clusters are formed from the self-organization of subplantated gold ions in the polymer. The source of the low energy ion stream used for the subplantation is a unidirectionally drifting gold plasma created by a magnetically filtered vacuum arc plasma gun. The material properties change according to subplantation dose, including nanocluster sizes and agglomeration state and, consequently also the material electrical behavior and optical activity. They have investigated the composite experimentally and by computer simulation in order to better understand the self-organization and the properties of the material. They present here the results of conductivity measurements and percolation behavior, dynamic TRIM simulations, surface plasmon resonance activity, transmission electron microscopy, small angle x-ray scattering, atomic force microscopy, and scanning tunneling microscopy. © 2010 American Vacuum Society.  Conductivity measurements; Dynamic TRIM; Electrical behaviors; Gold ions; Gold nanocluster; Ion streams; Low energies; Low energy ion implantation; Material property; Optical activity; Percolation behavior; Self-organizations; Shallow buried; Small angle X-ray scattering; Structural studies; Subplantation; Vacuum arc plasma; Agglomeration; Atomic force microscopy; Computer crime; Computer simulation; Electric properties; Gold; Gold compounds; Ion implantation; Nanoclusters; Optical properties; Plasma diagnostics; Polyesters; Polymer films; Scanning electron microscopy; Scanning tunneling microscopy; Solvents; Surface plasmon resonance; Transmission electron microscopy; Ultrathin films; Vacuum applications; Composite films;In this work, we have studied the influence of the substrate surface condition on the roughness and the structure of the nanostructured DLC films deposited by high-density plasma chemical vapor deposition. Four methods were used to modify the silicon wafers surface before starting the deposition processes of the nanostructured DLC films: micro-diamond powder dispersion, micro-graphite powder dispersion, and roughness generation by wet chemical etching and roughness generation by plasma etching. The reference wafer was only submitted to a chemical cleaning. It was possible to see that the final roughness and the sp3 hybridization degree (that is related with the structure and chemical composition) strongly depend on the substrate surface conditions. The surface roughness was observed by AFM and SEM and the hybridization degree of the DLC films was analyzed by Raman Spectroscopy. Thus, the effects of the substrate surface on the DLC film structure were confirmed. These phenomena can be explained by the fact that the locally higher surface energy and the sharp edges may induce local defects promoting the nanostructured characteristics in the DLC films. © 2009 Elsevier B.V. All rights reserved. Carbon films; DLC; Nanostructured films; Plasma process; Thin films AFM; Chemical compositions; Deposition process; Diamond powders; DLC film; Graphite powder; High density plasma chemical vapor deposition; Hybridization degree; Local defects; Nano-structured; Nanostructured diamond; Nanostructured Films; Plasma process; Reference wafers; SEM; Sharp edges; Substrate surface; Substrate surface conditions; Surface energies; Wet-chemical etching; Chemical cleaning; Diamond films; Diamond like carbon films; Dispersions; Nanostructured materials; Plasma deposition; Plasma etching; Plasmas; Raman spectroscopy; Semiconducting silicon compounds; Silicon wafers; Surface chemistry; Surface defects; Surface roughness; Surface tension; Thin films; Wet etching; Carbon films;We have formed and characterized polycrystalline diamond films with surfaces having hydrogen terminations, oxygen terminations, or fluorine terminations, using a small, simple and novel plasma gun to bombard the diamond surface, formed by plasma assisted CVD in a prior step, with ions of the wanted terminating species. The potential differences between surface regions with different terminations were measured by Kelvin Force Microscopy (KFM). The highest potential occurred for oxygen termination regions and the lowest for fluorine. The potential difference between regions with oxygen terminations and hydrogen terminations was about 80 mV, and between regions with hydrogen terminations and fluorine terminations about 150 mV. Regions with different terminations were identified and imaged using the secondary electron signal provided by scanning electron microscopy (SEM), since this signal presents contrast for surfaces with different electrical properties. The wettability of the surfaces with different terminations was evaluated, measuring contact angles. The sample with oxygen termination was the most hydrophilic, with a contact angle of 75°; hydrogen-terminated regions with 83°, and fluorine regions 93°, the most hydrophobic sample. © 2010 Elsevier B.V. CVD; Diamond; Ion bombardment; Plasma bombardment; Surface terminations CVD; CVD Diamond; Diamond surfaces; Electrical property; Hydrogen termination; Kelvin force microscopy; Oxygen termination; Plasma bombardment; Polycrystalline diamond films; Potential difference; Secondary electrons; SEM; Surface region; Surface termination; Chemical vapor deposition; Contact angle; Diamond films; Diamonds; Electric properties; Fluorine; Hydrogen; Ions; Oxygen; Plasma guns; Plasmas; Scanning electron microscopy; Ion bombardment;We have developed a theoretical model for superhydrophobic surfaces that are formed from an extended array of microcavities, and have fabricated specific microcavity patterns to form superhydrophobic surfaces of the kind modeled. The model shows that the cavity aspect ratio can be significantly less than unity, indicating that the microcavities do not need to be deep in order to enhance the superhydrophobic character of the surface. We have fabricated surfaces of this kind and measured advancing contact angle as high as 153°, in agreement with predictions of the model. © 2010 American Institute of Physics.  Advancing contact angle; Cavity aspect ratio; Super-hydrophobic surfaces; Superhydrophobic; Theoretical models; Aspect ratio; Contact angle; Fabrication; Microcavities; Surface properties; Hydrophobicity;The purpose of this in vitro study was to test a new methodology to evaluate the effects of 35% hydrogen peroxide agent on the microtopography of sound enamel using an atomic force microscope (AFM). The buccal sound surfaces of three extracted human lower incisors were used, without polishing the surfaces to maintain them with natural morphology. These unpolished surfaces were subjected to bleaching procedure with 35% hydrogen peroxide that consisted of 4 applications of the bleaching agent on enamel surfaces for 10 min each application. Surface images were obtained in a 15 μm × 15 μm area using an AFM. The roughness (Ra and RMS) and the power spectral density (PSD) were obtained before and after the bleaching treatment. As results we could inquire that the PSD analyses were very suitable to identifying the morphological changes on the surfaces, while the Ra and RMS parameters were insufficient to represent the morphological alterations promoted by bleaching procedure on enamel. The morphological wavelength in the range of visible light spectrum (380-750 nm) was analyzed, showing a considerable increase of the PSD with the bleaching treatment. © 2009 Elsevier B.V. All rights reserved. Atomic force microscopy; Dental enamel; Power spectrum density; Roughness parameters; Surface morphology analysis; Tooth bleaching Dental enamels; Power spectrum density; Roughness parameters; Surface morphology analysis; Tooth bleaching; Atomic force microscopy; Atomic spectroscopy; Atoms; Bleaching; Cleaning; Dentistry; Enamels; Hydrogen peroxide; Instrument scales; Morphology; Oxidation; Power spectrum; Pulse shaping circuits; Spectral density; Tooth enamel; Surface morphology;Shallow subsurface layers of gold nanoclusters were formed in polymethylmethacrylate (PMMA) polymer by very low energy (49 eV) gold ion implantation. The ion implantation process was modeled by computer simulation and accurately predicted the layer depth and width. Transmission electron microscopy (TEM) was used to image the buried layer and individual nanoclusters; the layer width was ∼6-8 nm and the cluster diameter was ∼5-6 nm. Surface plasmon resonance (SPR) absorption effects were observed by UV-visible spectroscopy. The TEM and SPR results were related to prior measurements of electrical conductivity of Au-doped PMMA, and excellent consistency was found with a model of electrical conductivity in which either at low implantation dose the individual nanoclusters are separated and do not physically touch each other, or at higher implantation dose the nanoclusters touch each other to form a random resistor network (percolation model). © 2009 American Vacuum Society.  Absorption effects; Buried layer; Cathodic arc plasma; Cluster diameter; Electrical conductivity; Gold ions; Gold nanocluster; Gold Nanoparticles; Implantation dose; Layer width; Low energies; Percolation models; Prior measurement; Random resistor networks; Shallow subsurface; TEM; UV visible spectroscopy; Absorption spectroscopy; Computer simulation; Electric conductivity; Gold; Ion bombardment; Ion implantation; Nanoclusters; Plasmons; Resistors; Solvents; Transmission electron microscopy; Ultraviolet spectroscopy; Surface plasmon resonance;We have investigated the fundamental structural properties of conducting thin films formed by implanting gold ions into polymethylmethacrylate (PMMA) polymer at 49 eV using a repetitively pulsed cathodic arc plasma gun. Transmission electron microscopy images of these composites show that the implanted ions form gold clusters of diameter ∼2-12 nm distributed throughout a shallow, buried layer of average thickness 7 nm, and small angle x-ray scattering (SAXS) reveals the structural properties of the PMMA-gold buried layer. The SAXS data have been interpreted using a theoretical model that accounts for peculiarities of disordered systems. © 2009 American Institute of Physics.  Buried layer; Conducting layers; Disordered system; Gold clusters; Gold ions; Implanted ions; Low energy ion implantation; Pulsed cathodic arc plasmas; Small angle X-ray scattering; Theoretical models; Transmission electron microscopy images; Conducting polymers; Conductive films; Gold; Ion bombardment; Ion implantation; Plasmas; Polymer films; Structural properties; Transmission electron microscopy; Gold compounds;PMMA (polymethylmethacrylate) was ion implanted with gold at very low energy and over a range of different doses using a filtered cathodic arc metal plasma system. A nanometer scale conducting layer was formed, fully buried below the polymer surface at low implantation dose, and evolving to include a gold surface layer as the dose was increased. Depth profiles of the implanted material were calculated using the Dynamic TRIM computer simulation program. The electrical conductivity of the gold-implanted PMMA was measured in situ as a function of dose. Samples formed at a number of different doses were subsequently characterized by Rutherford backscattering spectrometry, and test patterns were formed on the polymer by electron beam lithography. Lithographic patterns were imaged by atomic force microscopy and demonstrated that the contrast properties of the lithography were well maintained in the surface-modified PMMA. © 2009 American Institute of Physics.  Atomic forces; Cathodic arcs; Conducting layers; Depth profiles; Dynamic trims; Electrical conductivities; Gold surfaces; Implantation dose; Implanted materials; In-situ; Ion implanted; Lithographic patterns; Low energies; Metal plasmas; Nano-meter scale; Polymer surfaces; Polymethylmethacrylate; Rutherford back-scattering spectrometries; Surface-modified; Test patterns; Computer simulation; Electric conductivity; Electron beams; Gold; Polyesters; Rutherford backscattering spectroscopy; Surfaces; Conducting polymers;In this work, we have studied the influence of the substrate surface condition on the roughness and the structure of the nanostructured DLC films deposited by High Density Plasma Chemical Vapor Deposition. Four methods were used to modify the silicon wafers surface before starting the deposition processes of the nanostructured DLC films: micro-diamond powder dispersion, micro-graphite powder dispersion, and roughness generation by wet chemical etching and roughness generation by plasma etching. The reference wafer was only submitted to a chemical cleaning. It was possible to see that the final roughness and the sp3 hybridization degree strongly depend on the substrate surface conditions. The surface roughness was observed by AFM and SEM and the hybridization degree of the DLC films was analyzed by Raman Spectroscopy. In these samples, the final roughness and the sp3 hybridization quantity depend strongly on the substrate surface condition. Thus, the effects of the substrate surface on the DLC film structure were confirmed. These phenomena can be explained by the fact that the locally higher surface energy and the sharp edges may induce local defects promoting the nanostructured characteristics in the DLC films. © 2008 Elsevier B.V. All rights reserved. CVD processes; DLC films; Nanostructured films; Thin films Chemical cleaning; Diamond films; Diamond like carbon films; Diamonds; Etching; Graphite; Low pressure chemical vapor deposition; Nanostructured materials; Permittivity; Plasma deposition; Plasma etching; Plasmas; Powders; Semiconducting silicon compounds; Silicon wafers; Substrates; Surface chemistry; Surface defects; Surface phenomena; Surface roughness; Surface tension; Surface topography; Theorem proving; Thin films; Vapors; CVD processes; Deposition processes; Diamond powders; DLC films; Graphite powders; High densities; Hybridization degrees; Local defects; Nanostructured; Nanostructured films; Reference wafers; Sharp edges; Substrate surface conditions; Substrate surfaces; Surface energies; Wet chemicals; Carbon films;Gas flow through a conical micronozzle integrated with a piezoelectric actuator has been numerically studied using continuum methods. Experimental data of air flow through a converging-diverging nozzle of throat diameter 245 pm with and without actuator at its outlet side were used for reference. The inlet pressure was kept constant at 266 Pa in all measurements. The actuator at the nozzle outlet was fabricated from poli(vinylidene fluoride) (PVDF), a piezoelectric polymer, and has a 3 mm × 6 mm rectangular shape. A voltage of +300 V DC was used to open the device, and -200 V to close it. The micronozzle with actuator is a microvalve, since the gas flow rate can be controlled by the actuator. We used Ansys/CFX software to solve the Navier Stokes equations for this valve system. The Mach number was obtained for both cases, micronozzle with and without actuator at the outlet. © 2008 IEEE. Computational fluid dynamics; Mach number; Microfluidics; Micronozzle; Microvalve Actuators; Aerodynamics; Flow of gases; Gas dynamics; Nozzles; Numerical analysis; Optical design; Piezoelectric actuators; Piezoelectricity; Valves (mechanical); Air flows; Computational fluid dynamics; Experimental data; Gas flowing; Gas-flow rates; Inlet pressures; Mach number; Mach numbers; Micro nozzles; Micro valves; Microfluidics; Micronozzle; Microvalve; Numerical studies; Piezoelectric polymers; Rectangular shapes; Vinylidene fluoride; Navier Stokes equations;A buried conducting layer of metal/polymer nanocomposite was formed by very low energy gold ion implantation into polymethylmethacrylate. The conducting layer is ∼3 nm deep and of width ∼1 nm. In situ resistivity measurements were performed as the implantation proceeded, and the conductivity thus obtained as a function of buried gold concentration. The measured conductivity obeys the behavior well established for composites in the percolation regime. The critical concentration, below which the polymer remains an insulator, is attained at a dose ∼1.0× 1016 atoms/ cm2 of implanted gold ions. © 2008 American Institute of Physics.  Concentration (process); Conducting polymers; Ion bombardment; Ion implantation; Ions; Organic conductors; Polymers; Critical concentrations; Gold concentration; Gold ion; Gold ions; In-situ; Low energies; Nano composites; Poly-methyl methacrylate; Resistivity measurements; Gold;Micronozzles with piezoelectric actuator were fabricated and investigated. The micronozzles were fabricated in glass substrates using a powder-blasting technique, and the actuator is a bimorph structure made from a piezoelectric polymer. The actuator was located at the nozzle outlet, and was driven in an oscillating mode by applying an alternating voltage across the actuator electrodes. With a pressure difference between inlet and outlet, the gas flow rate through the device was increased. This effect was quantified, and compared to a similar micronozzle with no actuator. The increase in the flow rate was defined as the gas flow through the micronozzle with actuator oscillating minus the gas flow without actuator, was found to depend on the inlet pressure, the pressure ratio, and the nozzle throat diameter. © 2008 Elsevier B.V. All rights reserved. Glass micromachining; Microfluidics; Micronozzle; Piezoelectric microactuator; Poly(vinylidene fluoride) Flow rate; Microfluidics; Micromachining; Nozzles; Piezoelectric actuators; Voltage measurement; Glass micromachining; Micronozzles; Piezoelectric microactuators; Polyvinylidene fluoride; Powder blasting; Flow of gases;The authors describe a novel approach to the measurement of nanofriction, and demonstrate the application of the method by measurement of the coefficient of friction for diamondlike carbon (DLC) on DLC, Si on DLC, and Si on Si surfaces. The technique employs an atomic force microscope in a mode in which the tip moves only in the z (vertical) direction and the sample surface is sloped. As the tip moves vertically on the sloped surface, lateral tip slipping occurs, allowing the cantilever vertical deflection and the frictional (lateral) force to be monitored as a function of tip vertical deflection. The advantage of the approach is that cantilever calibration to obtain its spring constants is not necessary. Using this method, the authors have measured friction coefficients, for load range 0<L<6 μN, of 0.047±0.002 for Si on Si, 0.0173±0.0009 for Si on DLC, and 0.0080±0.0005 for DLC on DLC. For load range 9<L<13 μN, the DLC on DLC coefficient of friction increased to 0.051±0.003. © 2008 American Vacuum Society.  Friction coefficients; Nanofriction; Sloped surface; Atomic force microscopy; Diamond like carbon films; Silicon; Friction;We compare an ultrasound bur with a conventional one and an Er:YAG laser for cavity preparations. Human molars were embedded in resin and sliced for this study. The surface abrasion was performed by a high-speed instrument and ultrasound. The cavity preparation was initially performed with a high-speed diamond bur. After this, a 2.94-μm laser with 400 mJ/pulse at 4 Hz, and a pulse width from 250-500 μs was applied to the tooth surface for 30 s in a sweeping motion. The samples were analyzed by SEM. The abrasion surface with a conventional bur showed structure removal with different grooves, a smear-layer presence, and occluded dentinal tubules. The abraded surface with the CVD bur suggested a removal process in layers. The laser-irradiated surface showed a rough aspect with opened tubules and the absence of a smear layer. The results of this study suggest that a high-speed diamond bur, ultrasound, and laser were able to perform cavity preparation. However, the CVD bur presented a higher surface quality. © 2008 MAIK Nauka.  Abrasion; Chemical vapor deposition; Electronic structure; Semiconductor lasers; Surface analysis; Cavity preparation; Surface abrasion; Surface-cutting efficiency; Sweeping motion; Ultrasonic devices;This work describes a fabrication and test sequence of microvalves installed on micronozzles. The technique used to fabricate the micronozzles was powder blasting. The microvalves are actuators made from PVDF (polivinylidene fluoride), that is a piezoelectric polymer. The micronozzles have convergent-divergent shape with external diameter of 1mm and throat around 230μm. The polymer have low piezoelectric coefficient, for this reason a bimorph structure with dimensions of 2mm width and 4mm of length was build (two piezoelectric sheets were glued together with opposite polarization). Both sheets are recovered with a conductor thin film used as electrodes. Applying a voltage between the electrodes one sheet expands while the other contracts and this generate a vertical movement to the entire actuator. Appling +300V DC between the electrodes the volume flux rate, for a pressure ratio of 0.5, was 0.36 sccm. Applying -200V DC between the electrodes (that means it closed) the volume flux rate was 0.32 sccm, defining a possible range of flow between 0.32 and 0.36 sccm. The third measurement was performed using AC voltage (200V AC with frequency of 1Hz), where the actuator was oscillating. For pressure ratio of 0.5, the flow rate was 0.62 sccm. © 2008 IOP Publishing Ltd.  Ac voltage; Gas flow control; Micro nozzle; Micro valves; Piezoelectric coefficient; Piezoelectric polymers; Piezoelectric sheets; Powder blastings; Pressure ratio; Test sequence; Vertical movement; Volume flux; Electrodes; Piezoelectricity; Polymers; Pumps; Valves (mechanical); Piezoelectric actuators;In this article, the authors measure throughput of sonic diamond microtubes and micronozzles that can work as passive gas flow controllers and flow meters under choking conditions. The behavior of the outlet pressure through the microdevices using an experimental setup with constant volume and constant temperature was determined in order to obtain the critical throughput, the critical mass flow rate, and the discharge coefficients of the diamond sonic microdevices. © 2007 American Vacuum Society.  Diamonds; Flow rate; Flowmeters; Throughput; Discharge coefficients; Microdevices; Micronozzles; Microtubes; Sonic flow; Flow control;A microactuator made from poly(vinylidene fluoride) (PVDF), a piezoelectric polymer, was fabricated to control the gas flow rate through a glass micronozzle. The actuator was formed by gluing together two PVDF sheets with opposite polarization directions. The sheets were covered with thin conducting films on one side, that were then used as electrodes to apply an electric field to move the valve. The actuator has a rectangular shape, 3mm × 6mm. The device was incorporated with a micronozzle fabricated by a powder blasting technique. Upon applying a DC voltage across the actuator electrodes, one sheet expands while the other contracts, generating an opening motion. A voltage of +300V DC was used to open the device by moving the actuator 30νm, and a voltage of -200V DC was used to close the device by moving the actuator 20νm lower than the relaxed position. Flow measurements were performed in a low-pressure vacuum system, maintaining the microvalve inlet pressure constant at 266Pa. Tests carried out with the actuator in the open position and with a pressure ratio (inlet pressure divided by outlet pressure) of 0.5, indicated a flow rate of 0.36sccm. In the closed position, and with a pressure ratio of 0.2, a flow rate of 0.32sccm was measured. © IOP Publishing Ltd.  Electric potential; Flow control; Flow of gases; Flow rate; Microactuators; Piezoelectric actuators; Polarization; Glass micronozzles; Low pressure vacuum systems; Piezoelectric polymers; Powder blasting techniques; Vinyl resins;In this work PMMA was deposited on glass substrate, nanolithographed using Atomic Force Microscopy (AFM) and doped with gold using very low energy ion implantation. Samples with different geometries were prepared in order to identify anisotropic resistivity due to an anisotropic surface morphology. One of the samples was fabricated with parallel lines along the substrate length and the second one with parallel lines perpendicular to it. The PMMA with gold implanted presented resistivities in the range 7.7 × 10-8 Ω.m to 0.23 Ω.m. For any PMMA surface morphology the resistivity decreased when the gold dose increased. The anisotropic factor for both morphologies described above presented a prominent peak for a dose of 1.15 × 1016 atoms/cm2. Finally, a new sample of PMMA with gold implanted dose of 1.15 × 1016 atoms/cm2 was prepared and an I-V curve was obtained, showing clearly that the material is a semiconductor. © The Electrochemical Society.  Anisotropic resistivity; Anisotropic surfaces; Atomic force microscopy (AFM); glass substrates; In order; Low energy ion implantation; Microelectronics technology; parallel lines; Anisotropy; Atomic force microscopy; Computer networks; Electronics industry; Gold; Interfaces (materials); Ion bombardment; Ion implantation; Microelectronics; Microscopic examination; Neodymium; Scanning probe microscopy; Technology; Imaging techniques;We describe a small hollow-cathode plasma source suitable for small-scale materials synthesis and modification application. The supporting electrical system is minimal. The gaseous plasma source delivers a plasma ion current of up to about 1 mA. Here we outline the source construction and operation, and present some of its basic performance characteristics. © 2007 American Institute of Physics.  Cathodes; Plasma sources; Surface treatment; Synthesis (chemical); Small-scale materials; Source construction; Materials science; article; chemistry; electronics; equipment; equipment design; gas; heating; instrumentation; materials testing; methodology; microfluidics; reproducibility; sensitivity and specificity; Equipment Design; Equipment Failure Analysis; Gases; Heating; Materials Testing; Microfluidics; Miniaturization; Reproducibility of Results; Sensitivity and Specificity;Applications of Cu-HyBrID laser (copper laser with Hydrogen Bromide In Discharge) in Dentistry and AFM (atomic force microscopy) evaluations of dental tissues irradiated by laser are seldom reported in the literature. This work presents an AFM investigation of the cross-section of a cavity generated in human dental enamel by laser thermal evaporation using the Cu-HyBrID laser. The results exposed the structural and morphological differences between the fused and non-fused dental enamel, provide qualitative information about the susceptibility of these tissues to abrasive polishing, and revealed the extension of the thermal damage. Quantitative information concerning the wall thickness and the dimensions of the cross-section of non-fused enamel rod were also obtained. © 2007 Springer Science+Business Media, LLC.  Abrasive coatings; Dental composites; Morphology; Polishing; Thickness measurement; Tissue; Abrasive polishing; Dental tissues; Hydrogen Bromide; Thermal damage; Enamels; copper; article; atomic force microscopy; copper hybrid laser; enamel; evaporation; human; human tissue; information; laser; morphology; priority journal; qualitative analysis; quantitative analysis; temperature; thermal injury; thickness; Dental Cavity Preparation; Dental Enamel; Humans; Laser Surgery; Light; Molar, Third;We describe a nanolithography process for a polymethylmethacrylate (PMMA) surface using scanning contact atomic force microscopy. Parallel furrows were scribed with a pyramidal silicon tip using the same scan mechanism as used to image samples. The PMMA was first electron beam irradiated using a scanning electron microscope and developed. The topography formed is reproducible and predictable. Material from the region where the tip scribes is moved to nearby regions, and aligned, elongated PMMA fragments are seen to decorate the valleys between furrows. © 2007 American Institute of Physics.  Atomic force microscopy; Electron beams; Nanolithography; Topography; Electron beam irradiation; Scanning contact atomic force microscopy; Silicon tips; Polymethyl methacrylates; Electron Beams; Lithography; Microscopy; Polymethyl Methacrylate; Topography; nanomaterial; poly(methyl methacrylate); article; atomic force microscopy; chemistry; evaluation; materials testing; methodology; nanotechnology; surface property; ultrastructure; Materials Testing; Microscopy, Atomic Force; Nanostructures; Nanotechnology; Polymethyl Methacrylate; Surface Properties;A quantum mechanical approach is developed to calculate the surface-induced electrical conductivities of very thin metallic films with"
